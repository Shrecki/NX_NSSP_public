{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775a8a63-df82-4cae-b9d1-f28fcec4f71b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "\n",
    "\n",
    "Welcome to the laboratory computers for the course \"Neural signals and signal processing\". Today, you will see how to preprocess fMRI data as well as play around with diffusion weighted imaging to compute a simple tractography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dad7b7-f011-49d8-8dd3-e3b77ab9b178",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/Data\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "!pip install antspyx\n",
    "import ants\n",
    "\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976ed4b-1575-4964-af78-9fa9b1a06fcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Start FSLeyes (very neat tool to visualize MRI data of all sorts) within Python\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f70707-1260-4204-b479-3fa45862cd92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Careful ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Some of the fMRI preprocessing you will see below leverage the anatomical MRI being preprocessed, which you know how to do from week 2. In case of doubt, go back to the previous week to see what you might need.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061e352-d8c2-4b61-891e-738af1819c39",
   "metadata": {},
   "source": [
    "This notebook is structured into two distinct parts:\n",
    "- **Part1: fMRI preprocessing**\n",
    "- **Part2: diffusion data and tractography generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726e64d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 1: fMRI preprocessing\n",
    "\n",
    "You are now familiar with the few steps of preprocessing revolving around the T1 anatomical file. The main preprocessing starts now, with the functional data. \n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Do not forget QC! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    As always, in all your steps visualize the effect of what you're doing. This is the easiest way to check that what you're doing is actually having an effect and better yet: a <b>correct</b> effect!</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "Note that one assumption with fMRI preprocessing is that you've already conducted the anatomical preprocessing. In particular, the two main steps you will need today (repeat it from lab 2) before launching the lab:\n",
    "- T1 skull-stripping (use BET)\n",
    "- T1 segmentation (use FAST)\n",
    "Do it on sub-001, dataset with ID ds004226.\n",
    "\n",
    "\n",
    "## 1.0 Problematic volumes removal\n",
    "\n",
    "A problem can arise in fMRI. To showcase this, please execute the cell below (we'll show you something from another dataset just to drive our point home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6bb15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "dataset_demo = 'ds000114'\n",
    "subject_demo = '01'\n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "\n",
    "# Download one subject's data from each dataset\n",
    "bids_root_demo = op.join(os.path.abspath(\"\"),sample_path, dataset_demo)\n",
    "preproc_root_demo = op.join(bids_root_demo, 'derivatives')\n",
    "\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_demo, \"--target-dir\", bids_root_demo, \"--include\", 'sub-{}'.format(subject_demo)], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f6baa",
   "metadata": {},
   "source": [
    "We've downloaded one functional volume from another dataset, because the phenomenon is really visible in this dataset. \n",
    "Before going any further in this tutorial, let's open up our data and have a look at them. <u>You should always look at your data before conducting any sort of analysis</u>. See if you find anything at all that looks strange. You should look for\n",
    "\n",
    "- [ ] Volumes moving in space (ie: head motion)\n",
    "- [ ] Non homogeneities that do not seem to be coming from brain activity\n",
    "- [ ] Changes in *overall* contrast\n",
    "\n",
    "To open the volume of interest in FSL eyes, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a365c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root_demo, 'sub-01', 'ses-test', 'func', 'sub-01_ses-test_task-fingerfootlips_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f425046",
   "metadata": {},
   "source": [
    "Did you find anything?\n",
    "If so, what volumes would you remove, approximately?\n",
    "\n",
    "### 1.0.1 Field stabilization\n",
    "\n",
    "The scanner's field takes some time to settle. You probably noticed that the initial volume had a high contrast that quickly decayed to some baseline? It is precisely caused by the scanner's field settling.\n",
    "\n",
    "These scans are called *non-stationary volumes*, because they are acquired while the B0 field is not yet not stable.\n",
    "\n",
    "There's little to be done in this regard; we can only throw away the volumes that are contaminated in this specific case, to ensure this change in global signal does not drive our analysis.\n",
    "\n",
    "If we're being really formal, a higher overall contrast can be detected by looking at the mean voxel value in each volume, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd15bb-604f-44d7-a964-da3cb7819279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "plt.plot(nib.load(op.join(bids_root_demo, 'sub-01', 'ses-test', 'func', 'sub-01_ses-test_task-fingerfootlips_bold.nii.gz')).get_fdata().mean(axis=(0,1,2)))\n",
    "plt.xlabel('Time (volume)')\n",
    "plt.ylabel('Mean voxel intensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5e0f8-9b6c-42e6-bf08-6795ec1fe4e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Clearly, while we could throw away only the first volume and the second one for good measure, we'll **choose** to discard the first 10 volumes, to be absolutely sure the scanner is settled.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> Modern techs to the rescue</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Most scanners nowadays actually acquire few scans to help the B0 field settle. These are called dummy scans; the scanner acquires them but throws them away, meaning that you end up with a result that is settled. You should pay attention however if you ever analyze older datasets, as they will not benefit from these new techs obviously!</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a4231",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_to_trim = glob.glob(op.join(bids_root_demo, 'sub-01', 'ses-test', 'func', 'sub-01_ses-test_task-fingerfootlips_bold.nii.gz'))[0]\n",
    "mkdir_no_exist(preproc_root_demo)\n",
    "mkdir_no_exist(op.join(preproc_root_demo, 'sub-01'))\n",
    "mkdir_no_exist(op.join(preproc_root_demo, 'sub-01', 'ses-test'))\n",
    "mkdir_no_exist(op.join(preproc_root_demo, 'sub-01', 'ses-test','func'))\n",
    "output_target = op.join(preproc_root_demo, 'sub-01', 'ses-test', 'func', 'sub-01_ses-test_task-fingerfootlips_bold_settled.nii.gz')\n",
    "\n",
    "############################\n",
    "# Solution\n",
    "# Because we start at 0, we begin at the 11th volume, excluding 10 volumes: we should start at number 10.\n",
    "# There are 184 volumes. We remove 11 volumes, so we're left with 174 volumes.\n",
    "###########################\n",
    "\n",
    "# For this, knowing that there are originally 184 volumes and that you want to throw away the first 10, please fill in\n",
    "# the following variables\n",
    "start_vol = 10 # Where should we start? (First volume is 0, not 1 !)\n",
    "number_of_volumes = 174 # How many volumes should we keep?\n",
    "\n",
    "# Function creating new nifti with trimmed number of volumes\n",
    "fslroi(file_to_trim, output_target, str(start_vol), str(number_of_volumes))\n",
    "print(f\"You kept {nib.load(output_target).shape[-1]} volumes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b61d3",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üè† Take-home message üè†</b></p>\n",
    "<p>Always look at your data!</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "Let's go back to our original dataset now. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524c3e5",
   "metadata": {},
   "source": [
    "## 1.2 Motion correction\n",
    "\n",
    "Motion correction here specifically means trying to make it such that a given voxel describes the same brain position in all volumes.\n",
    "\n",
    "To illustrate why it might be a good idea, let's have a look at the functional data of our participant. Watch the movie. Do you notice anything strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62c38c",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/motion.gif\"/>\n",
    "    <p style=\"text-align:center;\"><i>You might want to pay attention to the axial view (right)</i></p></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb949fa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The volumes tend to move a bit around, don't they?\n",
    "<an example moment of motion>\n",
    "    \n",
    "This is a problem. Indeed, when we talk of a given voxel, our hope for analysis is that it represents a specific coordinate of anatomy. Imagine if you were trying to find your way with Google Maps, but every now and then the houses would suddenly all move by one kilometer! Would not be so easy to get to the right address, would it? Well, here it's the same. We want that a given (X, Y, Z) position describes always the same portion of the brain, otherwise our analysis will simply not work.\n",
    "\n",
    "But because of motion, this is not the case.\n",
    "    \n",
    "This is one of the core issues of fMRI: the participant simply moved, ever so slightly, during the acquisition. As a consequence, well, we have a recording of a moving participant. This is not a rare phenomenon: imagine having to keep your head perfectly still for several minutes and you'll quickly understand that it is **hard**!\n",
    "    \n",
    "Still, we would like to do something about it. This is where motion correction steps in. There are two sides to motion correction. The first, which we'll cover here, attempts to put all volumes back in alignment, so that a given position is indeed consistently describing the same anatomical part for all volumes. The second, which you'll see next week, attempts to correct for the consequences of motion on the magnetic field.\n",
    "<br><br>\n",
    "Do you remember Ducky? Well, imagine now that our dear duck has a rare shaking disease.\n",
    "    <br><img src=\"imgs/ducky/shakyducky.png\" style=\"width:auto;height:500px;\"/>\n",
    "<br>If we take several consecutive pictures of Ducky, it won't be as aligned as it should be\n",
    "    <br><img src=\"imgs/ducky/duckies_before_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "<br>To correct this, I can apply the idea we used in normalization. Let's pick one Ducky image as reference. Then, all other images of Ducky will be registered to this Ducky\n",
    "    <br><img src=\"imgs/ducky/duckies_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "    <br><img src=\"imgs/ducky/duckies_after_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "<br>Now, what if I want to remember how much Ducky had moved ? Well, I can remember the parameters of the transformation I had to apply to align the volumes. This fully encapsulates the motion information.\n",
    "\n",
    "This is precisely what motion correction sets out to achieve. For this, we need first to define a reference, if possible in fMRI space and that would not require too much transformations. Which option(s) seem reasonable to you?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eee3d4-076e-4c8b-871b-1fe3e9ed3c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80533f9-3ebe-4e83-9751-0b3aa5430035",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It turns out the first two options are usually equivalent. Because it saves us one pass of average computation, we will choose the first option: picking a volume and using it as reference! Which one do you think would be good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e42ea-2c8b-433a-90b4-e72bb506aaf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ca44f-2985-4fcd-b83b-e9eaba9e913a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, let us perform this step, on our **first** dataset (the one without fieldmaps). In FSL, we use <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\">MCFLIRT</a> to perform this correction.\n",
    "<br>\n",
    "    \n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    By default, MCFLIRT selects the middle volume of the EPI serie as reference to which other volumes are realigned.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bbc5f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "dataset_id = 'ds004226'\n",
    "subject = '001' \n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, \"--target-dir\", bids_root, \"--include\", 'sub-{}'.format(subject)], check=True)\n",
    "\n",
    "#mkdir_no_exist(bids_root)\n",
    "mkdir_no_exist(deriv_root)\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'anat'))\n",
    "\n",
    "path_original_data = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "path_moco_data = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "mcflirt(infile=path_original_data,o=path_moco_data, plots=True, report=True, dof=6, mats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76df2e",
   "metadata": {},
   "source": [
    "Okay! So, what do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764de111",
   "metadata": {},
   "source": [
    "In the functional folder, notice that we have two new files:\n",
    "\n",
    "**sub-001_task-sitrep_run-01_bold_moco.nii.gz**:\n",
    "- This is your motion-corrected EPI time series (4D NIfTI).\n",
    "- Each volume in the initial run has been spatially realigned to the same reference volume (the middle volume).\n",
    "- This is the dataset you‚Äôll use for further preprocessing and analysis.\n",
    "\n",
    "**sub-001_task-sitrep_run-01_bold_moco.par**:\n",
    "- This file contains the motion parameters, one line per volume.\n",
    "- Typically 6 numbers per line: 3 rotations (pitch, yaw, roll ‚Äî around x, y, z axes) + 3 translations (in mm along x, y, z axes)\n",
    "- These numbers describe the head movement at each timepoint with respect to the reference volume.\n",
    "\n",
    "Notice as well a new directory!\n",
    "\n",
    "**sub-001_task-sitrep_run-01_bold_moco.mat/**\n",
    "- This directory contains one .mat file per volume (each containing 4√ó4 matrices).\n",
    "- Each file is a transformation matrix describing the exact affine transformation used to map the indexed volume to the reference volume.\n",
    "- While the .par file show head movement, the matrices enact the transformation for realignment.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    The motion parameters and the transformation matrices are related, but they are not exactly the same thing. While you can recover one from the other, it is not trivial. Applying the transformation matrix to a volume will put it 'in alignment' as you've done with FLIRT. However the motion parameters cannot be applied directly. Loosely, the motion parameters describe how you would move if you first applied a rotation along x, then along y, then along y, followed by transition along x, then transition along y, then transition along z. This ordering of transformations is **not** really what happens with the transformation matrices. It is a convention adopted by FSL to make it easier to decouple transformations and rotations in the motion parameter analysis; it is therefore a <b>convenience</b>.\n",
    "    <u>Do not confuse transformation matrices and motion parameters</u>!</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea54013",
   "metadata": {},
   "source": [
    "Before going <u>any further</u>, go and have a look at the corrected timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e78d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(path_original_data)\n",
    "fsleyesDisplay.load(path_moco_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa868cba",
   "metadata": {},
   "source": [
    "Did mcflirt help correct motion? Are you convinced it did somewhat a proper job?\n",
    "<br>\n",
    "It's actually not too easy to tell right? Well, let's see if we can figure something out to ease our quality control!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd66b00",
   "metadata": {},
   "source": [
    "#### 1.2.1 Motion parameters and degrees of freedom\n",
    "\n",
    "We told you earlier that motion parameters can be used to estimate the motion along every axis.\n",
    "\n",
    "In our invocation of mcflirt, notice the following:\n",
    "```python\n",
    "mcflirt(..., dof=6)\n",
    "```\n",
    "dof stands for <i><b>d</b>egrees <b>o</b>f <b>f</b>reedom</i>, it really means what kind of transformation we wish to apply. In a 3D transformation, we have 3 axis:<br>\n",
    "<img src=\"imgs/3d_axis.png\"/><br>\n",
    "Along each axis, we can apply one transformation. Because we apply here only **affine** transformations, we can choose any transformation from:\n",
    "- Translation along the axis\n",
    "- Rotation along the axis\n",
    "- Shear along the axis\n",
    "- Scale along the axis\n",
    "\n",
    "Together, you can see this gives in total **12** DOF.\n",
    "We've chosen 6 DOFs, which is the standard choice: we want only to translate and rotate around the volumes, since they've been displaced by motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4679f33",
   "metadata": {},
   "source": [
    "#### 1.2.2 Looking at the resulting correction parameters\n",
    "Recall the motion parameters are stored in the .par file produced by MCFLIRT. Notice that since each volume moved differently, we have one transformation per volume, thus one set of motion parameters per volume as well. We provide you with a way to load these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mot_params_fsl_6_dof(path):\n",
    "    return pd.read_csv(path, sep='  ', header=None, \n",
    "            engine='python', names=['Rotation x', 'Rotation y', 'Rotation z','Translation x', 'Translation y', 'Translation z'])\n",
    "\n",
    "mot_params = load_mot_params_fsl_6_dof(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.par'))\n",
    "mot_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932aaae0",
   "metadata": {},
   "source": [
    "Based on **translation on X alone**, can you find perhaps a volume which exceeds with respect to the **preceding volume** a 0.2 mm displacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here to inspect quickly the translation on X :) # Hint: look at the documentation for np.diff this may come in handy!\n",
    "#%matplotlib inline\n",
    "\n",
    "# Getting the translation is easy\n",
    "trans_x = mot_params[\"Translation x\"]\n",
    "# Now, we want a 0.2mm with respect to previous frame:\n",
    "disp_x = np.diff(trans_x)\n",
    "# Lastly, we can ask for displacements (in absolute value) above 0.2mm and plot it to be clear:\n",
    "threshold=0.2\n",
    "plt.plot(np.abs(disp_x))\n",
    "plt.hlines(threshold, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.xlabel(\"Volumes\")\n",
    "plt.ylabel(\"Framewise translation displacement (mm)\")\n",
    "plt.show()\n",
    "\n",
    "# There are basically (if looking only along X translation) no frame displacement above 0.2mm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b807b",
   "metadata": {},
   "source": [
    "Some metrics have been created, to compute the displacement of a frame compared to the preceding frame: this is the **frame-wise displacement**. While considering the x-translation can be indicative of motion, it omits displacement in other directions. Here the **frame-wise displacement** allows to quantify overall displacements. <br>(see <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3254728/\">Power, Jonathan D., et al. \"Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion.\" Neuroimage 59.3 (2012): 2142-2154.</a> for more details).<br>\n",
    "We can use this one to extract an aggregate measure of motion for all volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ded6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FD_power(mot_params):\n",
    "    framewise_diff = mot_params.diff().iloc[1:]\n",
    "\n",
    "    rot_params = framewise_diff[['Rotation x', 'Rotation y', 'Rotation z']]\n",
    "    # Estimating displacement on a 50mm radius sphere\n",
    "    # To know this one, we can remember the definition of the radian!\n",
    "    # Indeed, let the radian be theta, the arc length be s and the radius be r.\n",
    "    # Then theta = s / r\n",
    "    # We want to determine here s, for a sphere of 50mm radius and knowing theta. Easy enough!\n",
    "    \n",
    "    # Another way to think about it is through the line integral along the circle.\n",
    "    # Integrating from 0 to theta with radius 50 will give you, unsurprisingly, r0 theta.\n",
    "    converted_rots = rot_params*50\n",
    "    trans_params = framewise_diff[['Translation x', 'Translation y', 'Translation z']]\n",
    "    fd = converted_rots.abs().sum(axis=1) + trans_params.abs().sum(axis=1)\n",
    "    return fd\n",
    "\n",
    "fd = compute_FD_power(mot_params).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c15398",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.quantile(fd,0.75) + 1.5*(np.quantile(fd,0.75) - np.quantile(fd,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eba455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.plot(list(range(1, fd.size+1)), fd)\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('FD displacement (mm)')\n",
    "plt.hlines(threshold, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259fc83",
   "metadata": {},
   "source": [
    "Okay great, but what if we want to know which volumes are actually above threshold? Simply run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Indices of frames above threshold: {(np.where(fd > threshold)[0] + 1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c45647",
   "metadata": {},
   "source": [
    "So, you now know which volumes might present motion that is worth checking. Go back to FSLeyes and contrast the uncorrected volumes with the corrected ones. Can you see what sort of motion was problematic and was eliminated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828fa09",
   "metadata": {},
   "source": [
    "## 1.3 Motion-correction: conclusions\n",
    "\n",
    "Motion correction should always be conducted. As you've seen, it is extremely easy to do and has many benefits. However it is not infaillible. High motion tends to cause non linear effects in the signal that simple motion correction above cannot correct since it has no awareness of the magnetic field. <br>\n",
    "<br> Motion parameters can, in this case, come to our rescue. As they represent the effect of motion, including them in our modeling to try and correct the signal can help. One could for example include this information in a General Linear Model to regress out the signal of these volumes (censoring) from overall timeseries. ‚û°Ô∏è More on this next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ee5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.4 Where are we?\n",
    "\n",
    "So, let's see what we have done so far:\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM in two weeks!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "</table>\n",
    "\n",
    "You will see some additional steps next week (slice-timing and field unwarping to be specific).\n",
    "\n",
    "## 1.5 Coregistration of functional to anatomical\n",
    "\n",
    "You have seen coregistration last week, when you were trying to align the T1 to MNI152, both manually and algorithmically. In the specific case of putting a T1 anatomical in a template space (such as MNI), we call it <b>normalization</b>, because we...Normalize it !\n",
    "\n",
    "You've also seen above with motion-correction coregistration of EPI volumes (functional data) to each other to correct motion.\n",
    "\n",
    "But what if you wanted to put the functional data overlayed on the anatomy, to know more precisely which parts of the brain are activated?\n",
    "\n",
    "You will see the last important ingredient to do this: coregistration between functional and anatomical.\n",
    "\n",
    "In this specific case, it is a registration between images of different modalities. We want to register fMRI (EPI) to an anatomical image (T1). There are several reasons for this. The first that comes to mind is that if you overlay your fMRI on the anatomy, you can of course reason much more easily on where you are in the brain, what activations you might be looking at and so forth. Imagine a participant has a brain lesion visible on the anatomy and you want to see how this reflects on the fMRI. Being able to put the two together would make it much easier, would it not?\n",
    "\n",
    "This is the first reason behind coregistration.\n",
    "\n",
    "The second is because of normalization, again. Assume you want to compare all fMRI data of participants. Clearly, putting all of them into a common reference frame is a bit trickier, because of how noisy and low-resolution the data is, right? But you know how to map the anatomical to this common space with excellent accuracy, and you've saved this transformation earlier.\n",
    "If you could figure out how to go from the fMRI space to anatomical, clearly the problem would be solved! You'd only have then to apply the transformation from anatomical to common space and be done with it.\n",
    "\n",
    "\n",
    "Computing the fMRI space to anatomical transformation is precisely the goal of coregistration.\n",
    "<br><br>\n",
    "To do this step, we will use a wonderful command: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/FLIRT(2f)UserGuide.html#epi_reg\">epi_reg</a> ! As the name states, it is a command to register an EPI. Hard to make it clearer huh? \n",
    "\n",
    "### 1.5.1 Using epi_reg to do the EPI registration\n",
    "\n",
    "Notice that we want to compute the transformation to use for coregistration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8867d8-c576-4917-975f-9782ebe46a7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d060ec-2c26-485e-a9cd-cded25d1deca",
   "metadata": {},
   "source": [
    "Your task is simple. You should:\n",
    "- Fill in the name of the EPI source. It should be the **motion-corrected** EPI that you corrected using MCFLIRT. If you want to use a single volume, set the use_first_vol variable to True!\n",
    "- Fill in the path to the whole head T1 image (**before** skull stripping was conducted!)\n",
    "- Fill in the path to the skull-stripped T1 image (**after** skull stripping was conducted!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f8d56-d901-43b6-8e74-aec26de5ac69",
   "metadata": {},
   "source": [
    "Prior to using epi_reg make sure that you have generated both the skull stripped T1 and its tissue segmentation as they will be used in the following cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991403e-d68c-417b-8ad0-3a0f73a2cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Remember to generate skull stripped T1 and perform tissue segmentation if you did not yet !!! Hint: Use skull stripping and segmentation function from Lab 2!\n",
    "\n",
    "# Generate skull stripped T1\n",
    "anatomical_path = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz')\n",
    "betted_brain_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "os.system('bet {} {} -m {}'.format(anatomical_path, betted_brain_path, ''))\n",
    "\n",
    "# Generate segmented tissues\n",
    "[os.remove(f) for f in glob.glob(op.join(preproc_root, 'sub-001', 'anat', '*fast*'))] # Just to clean the directory in between runs of the cell\n",
    "segmentation_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast')\n",
    "fast(imgs=[betted_brain_path], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0d517",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Make sure that the whole head T1 and the skull-stripped T1 have the same orientation.\n",
    "For example, if you ran fsl_anat to extract the brain (which is fine), FSL will change in the headers the orientation of the T1 before skull-stripping. As a consequence, the brain-extracted T1 no longer has the same orientation as the original T1. If you display them on top of each other, they are perfectly matched, but not from the perspective of the <b>headers</b>, which can play nasty tricks on you when performing coregistration.</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "First, we'll show you the documentation of epi_reg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810cf2-7d70-450b-8b7b-fa73834c7130",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run(['epi_reg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5df78a-a54a-43f9-b1cc-bda796809883",
   "metadata": {},
   "source": [
    "epi_reg has one peculiarity. If you launch it on a 4D volume, it will truncate your result to the first volume, because it expects a *single* EPI volume. We should thus first extract a single volume from our EPI, and then call epi_reg on it. We do that for you below.\n",
    "\n",
    "If you want to run with 4D volume to see the result and the warning, set use_single_vol to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f383c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import epi_reg\n",
    "\n",
    "#################\n",
    "# Solution\n",
    "# We use the motion corrected EPI \n",
    "##################\n",
    "\n",
    "# We use the motion corrected EPI\n",
    "epi_source = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "whole_t1 = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "skull_stripped_t1 = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_bbr')\n",
    "ref_vol_name =  op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco_vol_middle')\n",
    "\n",
    "use_single_vol = True\n",
    "\n",
    "if use_single_vol:\n",
    "    # Extract the middle volume with fslroi as we've seen before :)\n",
    "    fslroi(epi_source, ref_vol_name, str(182), str(1))\n",
    "    # Call epi_reg\n",
    "    subprocess.run(['epi_reg','--epi={}'.format(ref_vol_name), '--t1={}'.format(whole_t1), '--t1brain={}'.format(skull_stripped_t1), '--out={}'.format(output_path)])\n",
    "else:\n",
    "    subprocess.run(['epi_reg','--epi={}'.format(epi_source), '--t1={}'.format(whole_t1), '--t1brain={}'.format(skull_stripped_t1), '--out={}'.format(output_path)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b13492",
   "metadata": {},
   "source": [
    "Notice how FAST is ran?\n",
    "This is because the specific coregistration cost (boundary-based registration, BBR) uses the anatomical white-matter tissues from FAST. If no such tissue is provided to the function, it re-runs FAST to obtain it and use it. If you've already done anatomical segmentation, clearly there's no need to redo it right?\n",
    "In particular, imagine if you had to yourself correct the white matter with the help of an expert because somehow FSL did a poor job on your data. Clearly you'd like to have this one used instead of the result from FAST, right?\n",
    "\n",
    "Well- you can! We just need a new option in the epi_reg command:\n",
    "```python\n",
    "epi_reg(...,wmseg=path_to_your_white_matter_segmentation)\n",
    "```\n",
    "Remember from last week, which T1 file corresponds to the white matter, between pve_0, pve_1 and pve_2 ? Put it as your white matter segmentation. (Note: of course, you will need to re-do your T1 segmentation before you can feed it here - a good moment to remember the previous week and revisit FAST and BET ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f657883",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Solution\n",
    "# White matter corresponds to pve_2.\n",
    "##############\n",
    "\n",
    "white_matter_segmentation = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast_pve_2.nii.gz') # We provide the white matter segmentation\n",
    "\n",
    "subprocess.run(['epi_reg','--epi={}'.format(ref_vol_name), '--t1={}'.format(whole_t1), '--t1brain={}'.format(skull_stripped_t1), \n",
    "                '--out={}'.format(output_path),\n",
    "               '--wmseg={}'.format(white_matter_segmentation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665690c",
   "metadata": {},
   "source": [
    "Let's overlay the two (EPI and anatomical) on top of each other to visualize the quality of the coregistration! (Note: You can change the opacity of either volumes to see better whether they overlap!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcaf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(skull_stripped_t1)\n",
    "fsleyesDisplay.load(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff9a10",
   "metadata": {},
   "source": [
    "Now, how do we *know* if the registration is good or bad?\n",
    "Well, there are several things to watch out for, but here are some main leads:\n",
    "- Is the functional in the right orientation?\n",
    "- Are the ventricles correctly aligned?\n",
    "- Are the boundaries of the EPI more or less matching the anatomical?\n",
    "\n",
    "‚û°Ô∏è You can also check how the white matter of the EPI matches your anatomical's white matter provided you have sufficient resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c4389",
   "metadata": {},
   "source": [
    "#### 1.5.2 Some cleanup\n",
    "If you have a look, you might notice that perhaps your directory got filled with many files. These are temporary files, created but uncorrectly not eliminated by epi_reg. The following should help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_epi_reg(path_to_clean):\n",
    "    patterns = ['*_fast_*', '*_fieldmap*']\n",
    "    for p in patterns:\n",
    "        files = glob.glob(op.join(path_to_clean, p))\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_epi_reg(op.join(preproc_root, 'sub-001', 'func'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efca3d",
   "metadata": {},
   "source": [
    "## 1.6 Smoothing\n",
    "All these transforms are not exactly perfect. As you've seen in class, a step of smoothing is typically applied, with the size of the smoothing being dependent on your application, starting resolution etc.\n",
    "The idea of smoothing is really that, as you're averaging, hopefully you increase the signal to noise ratio. <br>\n",
    "A side-effect is that finest patterns of activation will be lost in the averaging (we can't have everything: there's no free lunch).\n",
    "\n",
    "With FSL, smoothing is rather easy to do. However, one thing which is important is the size of your filter.\n",
    "Different softwares might use different conventions. For MRI, it is typical to talk about FWHM (Full-width at half maximum), expressed in mms.\n",
    "\n",
    "FSL, however, takes as input in sigma instead of FWHM. The conversion is easy fortunately:\n",
    "\n",
    "$$ \\sigma = \\frac{FWHM}{2.3548}$$\n",
    "\n",
    "Here for example would be the smoothing command for 6mm FWHM smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'fslmaths {} -s {} {}_smoothed-6mm'.format(output_path, 6/2.3548, output_path)\n",
    "subprocess.run(['fslmaths',output_path, '-s', str(6/2.3548), '{}_smoothed-6mm'.format(output_path)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c06387",
   "metadata": {},
   "source": [
    "Let's observe what we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691354b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(output_path + '_smoothed-6mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb3abe",
   "metadata": {},
   "source": [
    "Do you feel as though the signal-to-noise ratio was improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4224b",
   "metadata": {},
   "source": [
    "## 1.7 MRI + fMRI preprocessing: summary\n",
    "\n",
    "So, these were all the steps you were meant to study this week. Next week, we'll present advanced steps of fMRI, but they are not always conducted unlike those we've shown you above which should always be considered.\n",
    "\n",
    "You should know by now: preprocessing is extremely important and you will likely spend a lot of time on it. Decisions in preprocessing will affect your analysis, so do not take this step lightly, it is <u>critical</u> to do it as well as possible!\n",
    "\n",
    "<u>Always perform quality control to ensure everything is okay!</u>\n",
    "\n",
    "Let's review one last time the different steps you've studied and which FSL tool(s) you used to do it:\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT + FNIRT (from last week), or ANTs</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM next week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration to anatomical</td><td style='text-align:justify;'>Putting the functional volumes in anatomical space</td><td style='text-align:justify;'>FLIRT (epi_reg being a specialized instance)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Smoothing</td><td style='text-align:justify;'>Allowing a bit of lee-way in the voxel's values to account for the imperfection of the registration</td><td style='text-align:justify;'>fslmath with smoothing operation</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136f23d",
   "metadata": {},
   "source": [
    "# Part 2: diffusion data and tractography generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7e7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The preprocessing in DTI involves similar steps to what you saw in fMRI. We will thus tackle specific different steps to not repeat ourselves too much:\n",
    "<center><img src=\"imgs/DTI_preprocessing.png\" width=\"600\"/></center>\n",
    "<p  style=\"text-align: center;\"><i>Image from <a href=\"https://www.researchgate.net/publication/311246309_Imaging_analysis_of_Parkinson's_disease_patients_using_SPECT_and_tractography\">Son, Seong-Jin, Mansu Kim, and Hyunjin Park. \"Imaging analysis of Parkinson‚Äôs disease patients using SPECT and tractography.\" Scientific reports 6.1 (2016): 1-11.</a></i></p>\n",
    "\n",
    "In other words, we will have you look at an example to generate tractogram. Here, a tractogram will be generated using a deterministic algorithm EuDX.  \n",
    "  \n",
    "Diffusion tensor imaging (DTI) is one of the most popular MRI techniques to describe the orientation of white matter fibers in brain research. The process of fiber tracking is called tractography. It allows for a virtual dissection and three-dimensional representation of white matter tracts. \n",
    "While we could still use FSL for the task, we will have you use <a href=\"https://dipy.org/\">DIPY</a>, a Python package for computational neuroanatomy mainly focusing on diffusion MRI analysis.  \n",
    "<br>\n",
    "To generate a tractogram, we need to track the fibers, which is called fiber tracking.\n",
    "<br>\n",
    "Local fiber tracking is used to model white matter fibers by creating streamlines from local directional information. In order to perform local fiber tracking, you will apply the following three steps:\n",
    "<p >\n",
    "    <ol>\n",
    "        <li style=\"font-size: 15px;\">Extract directions from diffusion data</li> \n",
    "        <li style=\"font-size: 15px;\">Identify when the tracking must stop</li>  \n",
    "        <li style=\"font-size: 15px;\">Select a set of locations from which to begin tracking</li>\n",
    "    </ol>\n",
    "</p>\n",
    "Combining them will help you obtain a tractography reconstruction!\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582b7ba",
   "metadata": {},
   "source": [
    "## 2.1. Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti, load_nifti_data\n",
    "\n",
    "hardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')\n",
    "hardi_root_path = op.split(hardi_fname)[0]\n",
    "label_fname = get_fnames('stanford_labels')\n",
    "\n",
    "data, affine, hardi_img = load_nifti(hardi_fname, return_img=True)\n",
    "labels = load_nifti_data(label_fname)\n",
    "bvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data.shape: ',data.shape)\n",
    "print('affine.shape: ',affine.shape)\n",
    "print('hardi_img.shape: ',hardi_img.shape)\n",
    "\n",
    "print('labels.shape: ',labels.shape)\n",
    "print('bvals.shape: ',bvals.shape)\n",
    "print('bvecs.shape: ',bvecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d842124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "print_dir_tree(hardi_root_path, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cc3fe",
   "metadata": {},
   "source": [
    "## 2.2. Get the directions from the diffusion data set\n",
    "\n",
    "### 2.2.1. Defining the white matter region.\n",
    "Before all else, you will need to visualize the labels above. Run the following cell to load the labels on FSLeyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d852ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(label_fname)\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'brain_colours_spectrum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f992f",
   "metadata": {},
   "source": [
    "Based on the values you read within, can you please fill in the cell below with the label corresponding to white matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_matter_value = 1 # Fill with the value you read in FSLeyes for white matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493caf5f",
   "metadata": {},
   "source": [
    "Great, let's now visualize the result of the mask, shall we? For this, let's generate the mask we obtained from above, using our best pal fslmaths ! \n",
    "What you should do is extract from the above file the white matter directly, using fslmaths (you can also use the pythonic approach if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Solution\n",
    "# We show the fslmaths approach. Simply set upper and lower threshold to the white matter value in the parcellation.\n",
    "##################\n",
    "output_name = op.join(hardi_root_path,'extracted_wm')\n",
    "subprocess.run(['fslmaths', label_fname, '-thr', str(white_matter_value), '-uthr', str(white_matter_value), '-bin', output_name])\n",
    "fsleyesDisplay.load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948b411",
   "metadata": {},
   "source": [
    "That's nice, but we are missing something, aren't we? \n",
    "In the coronal slice, look in the middle. There is a region appearing in magenta/purple, but we wanted to include all white matter. Why is that? If you have on top, you'll see it has a different label: 2. This is because this region is white matter, but it is also a sagittal slice of the **corpus callosum**. So we need to do something a bit different. Can you think of a way to modify the above fslmaths command to include both the white matter and the slice of corpus callosum ? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Solution\n",
    "# We show the fslmaths approach.\n",
    "##################\n",
    "\n",
    "lower_threshold = 1 # Select the lower bound to include both white matter and corpus callosum slice\n",
    "upper_threshold = 2 # Select the upper bound to include both white matter and corpus callosum slice\n",
    "output_name = op.join(hardi_root_path,'extracted_wm_complete')\n",
    "\n",
    "subprocess.run(['fslmaths', label_fname, '-thr', str(lower_threshold), '-uthr', str(upper_threshold), '-bin', output_name])\n",
    "fsleyesDisplay.load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7d1f1",
   "metadata": {},
   "source": [
    "Beautiful! So you can see that these labels can be a bit tricky if you're not careful. Based on your above experience above, we will construct a mask in python directly. Please fill in the cell below the two values for:\n",
    "- The white matter regions\n",
    "- The corpus callosum slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b54436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Solution\n",
    "# Remark we create an OR mask between the corpus callosum and white matter values.\n",
    "# The total mask is simply set from this parcellation.\n",
    "#######\n",
    "corpus_callosum_slice_value = 2 # Fill with your value!\n",
    "white_matter_value = 1 # Fill with your value !\n",
    "\n",
    "total_white_matter = (labels == corpus_callosum_slice_value) | (labels == white_matter_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068803fa",
   "metadata": {},
   "source": [
    "### 2.2.2. Actually extracting fiber orientations: the orientation distribution function\n",
    "Okay, now we have a mask to define our fibers. The next cell will be used to estimate the orientation distribution function at each voxel. Before going any further, let's ask why this is necessary. In your opinion, in a single *voxel* how many orientations can we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dfa37-c120-4de2-a29c-58569960ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a549a23",
   "metadata": {},
   "source": [
    "The issue can be summarized as resolving **intravoxel** fiber orientations of MR images.\n",
    "To summarize these, we use an orientation distribution function, coined ODF.\n",
    "\n",
    "The ODF is a function that describes how likely local water diffuses in a given orientation in 3D. \n",
    "\n",
    "We will not bore you with all mathematical details. What you need to know, however, is that this distribution function will rely on a special model, called the constant solid angle ODF (CSA-ODF) model. The idea is the following: considering the distance from origin of the estimated distribution provides useful information:\n",
    "<center><img src=\"imgs/tractography/solid_angle.png\"/></center>\n",
    "<center>Left: odf takes into account solid angle; Right: odf does not take into account solid angle</center>\n",
    "<i><center>Image taken from <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2911516/\">Aganj, Iman, et al. \"Reconstruction of the orientation distribution function in single‚Äêand multiple‚Äêshell q‚Äêball imaging within constant solid angle.\" Magnetic resonance in medicine 64.2 (2010): 554-566.</a></center></i>\n",
    "\n",
    "CSA-ODF with solid angle properly accounts for spherical geometry while CSA-ODF without solid angle is less accurate as it doesn‚Äôt normalize by solid angle.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0241fda",
   "metadata": {},
   "source": [
    "Let's now estimate the orientation distribution function of each voxel, using the CSA-ODF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import auto_response_ssst\n",
    "from dipy.reconst.shm import CsaOdfModel\n",
    "from dipy.data import default_sphere\n",
    "from dipy.direction import peaks_from_model\n",
    "\n",
    "# Single fiber response function: the measured signal of a single fiber\n",
    "# sume: regions where there are single coherent fiber populations\n",
    "# auto_response_ssst: calculate FA for a ROI of radii equal to roi_radii in the center of the volume\n",
    "# and return the response function estimated in that region for the voxels with FA higher than 0.7\n",
    "response, ratio = auto_response_ssst(gtab, data, roi_radii=10, fa_thr=0.7)\n",
    "\n",
    "# Instantiate the Constant Solid Angle model\n",
    "csa_model = CsaOdfModel(gtab, sh_order_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b205286",
   "metadata": {},
   "source": [
    "Now that we have our model, the orientation of tract segments can be extracted, looking at the peaks in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks = peaks_from_model(csa_model, data, default_sphere,\n",
    "                             relative_peak_threshold=.8,\n",
    "                             min_separation_angle=45,\n",
    "                             mask=total_white_matter, npeaks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041ed92",
   "metadata": {},
   "source": [
    "Notice in the above cell the following line:\n",
    "```python\n",
    "csa_peaks = peaks_from_model(..., npeaks=5)\n",
    "```\n",
    "\n",
    "This means that really, we extract per voxel five peaks at most. This is an important assumption. Depending on your voxel size, you might want to pay attention to this number!\n",
    "\n",
    "To confirm this, let's have a look at the extracted peak values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks.peak_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef696642",
   "metadata": {},
   "source": [
    "Knowing the MR dimensions, you can see that we indeed have five peaks per voxel. Great! Let's visualize it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fury import actor, window, ui\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = window.Scene()\n",
    "slice_actor = actor.peak_slicer(csa_peaks.peak_dirs,\n",
    "                            peaks_values=csa_peaks.peak_values,\n",
    "                            affine=affine,mask=total_white_matter,)\n",
    "scene.add(slice_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e1250",
   "metadata": {},
   "source": [
    "So as you can see, the orientations do map out to the expected directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8dd87",
   "metadata": {},
   "source": [
    "## 2.3. Set the stop criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77906254",
   "metadata": {},
   "source": [
    "Now, we need to setup our fiber tracking to stop it. What criterion should we use?\n",
    "Well, we'll roughly use the idea that when we don't have enough evidence to know where a fiber could have gone, we stop tracking it.\n",
    "In other words, if there are areas where the diffusion is totally unrestricted (goes in all directions), we have no clue as to where the fiber might continue. For this, we can threshold the tendency of our peaks to depend on a specific direction (anisotropy).<br>\n",
    "More specifically, we will threshold the general fractional anisotropy (scalar measure between 0 and 1 with 0 ‚Üí isotropic [e.g CSF, gray matter] and 1 ‚Üí highly anisotropic [e.g white matter fiber bundles]) of our data to decide when we should stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc23864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "\n",
    "stopping_criterion = ThresholdStoppingCriterion(csa_peaks.gfa, .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b904f4b",
   "metadata": {},
   "source": [
    "Let's visualize a slice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sli = csa_peaks.gfa.shape[2] // 2\n",
    "plt.figure('GFA')\n",
    "plt.subplot(1, 2, 1).set_axis_off()\n",
    "plt.imshow(csa_peaks.gfa[:, :, sli].T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.subplot(1, 2, 2).set_axis_off()\n",
    "plt.imshow((csa_peaks.gfa[:, :, sli] > 0.25).T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.savefig('gfa_tracking_mask.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5117d0",
   "metadata": {},
   "source": [
    "## 2.4. Specify where to begin the fibers tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101ba67",
   "metadata": {},
   "source": [
    "There are different ways to place seeds, ie starting points from which the fiber tracking is started. This depends on the pathways you might like to model! For example, if you only wanted to model the corpus callosum it would not be so interesting to place seeds in other regions of the brain. <br>\n",
    "So that you understand what the output of the cell below will be, we must first explain what you'll extract in the cell below.<br>\n",
    "The orientation of an image is described by its affine transformation, if you remember well. Let's call this affine $A$.\n",
    "A seed point at the center of voxel $[i,j,k]$ will be represented as $[x,y,z, 1]= A \\cdot [i,j,k,1]$<br>\n",
    "In other words, you will get **coordinates in voxel space**. Note that there is one important assumption: the voxels here should be isotropic (the size of a voxel should be same along all directions).\n",
    "\n",
    "In our specific case, we will start from a sagittal slice of the **corpus callosum**, the one with label 2 to be specific.\n",
    "Please, create the mask (based on the labels above) to extract **only the slice of corpus callosum with label 2 as a mask**. You can refer to what we did above to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "\n",
    "seed_mask = labels == 2 # Your code here to extract only the place of interest! \n",
    "seeds = utils.seeds_from_mask(seed_mask, affine, density=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277de50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.5. Bringing it all together and generating the streamlines\n",
    "Let's see our ingredients:\n",
    "- Seeds, generated above and starting from a slice of the corpus callosum\n",
    "- A mask of regions where we should stop our fibers, based on anisotropy\n",
    "- Peaks of ODF, at most five peaks per voxel\n",
    "\n",
    "It remains now to combine all of these to bake so called streamlines (ie: fibers!). To do so, we will use the EuDX algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "# Initialization of LocalTracking. The computation happens in the next step.\n",
    "streamlines_generator = LocalTracking(csa_peaks, stopping_criterion, seeds,\n",
    "                                      affine=affine, step_size=.5)\n",
    "# Generate streamlines object\n",
    "streamlines_t = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184cd0",
   "metadata": {},
   "source": [
    "Beautiful! Let's now visualize our streamlines!\n",
    "Remember that they represent **only lines that start from the corpus callosum**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da572e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import colormap\n",
    "\n",
    "# Prepare the display objects.\n",
    "color = colormap.line_colors(streamlines_t)\n",
    "\n",
    "streamlines_actor = actor.line(streamlines_t,\n",
    "                               colors=colormap.line_colors(streamlines_t))\n",
    "\n",
    "# Create the 3D display.\n",
    "scene = window.Scene()\n",
    "scene.add(streamlines_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())\n",
    "\n",
    "# Save still images for this static example. Or for interactivity use\n",
    "#window.record(scene, out_path='tractogram_EuDX.png', size=(800, 800))\n",
    "#if interactive:\n",
    "#    window.show(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05149fea",
   "metadata": {},
   "source": [
    "## 2.6. Store the streamlines into a trackvis file\n",
    "\n",
    "What if we wanted to save the result as a file? Well, you can! For this, we need to save it to a special format, the TrackVis (.trk) format.\n",
    "\n",
    "Remember: our goal was to generate the streamlines. It is these streamlines that we therefore want to save! :) \n",
    "Let's do it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_tractogram, save_trk\n",
    "\n",
    "# This is for the cc slice tractogram\n",
    "sft = StatefulTractogram(streamlines_t, hardi_img, Space.RASMM)\n",
    "save_trk(sft, \"tractogram_EuDX.trk\", streamlines_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61be9e0",
   "metadata": {},
   "source": [
    "If you want to visualize it all, you can activate <a href=\"https://trackvis.org/\">TrackVis</a> and open the file from within or re-use `load_trk` from the dipy library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cc30f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.7 Conclusions\n",
    "\n",
    "Let's sum up what we've seen. For a successfull tractography generation, we need the following: \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ingredient</th>\n",
    "        <th>Role</th>\n",
    "        <th>How is it created?</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Seeds</td>\n",
    "        <td>Define starting point of tract propagation.</td>\n",
    "        <td>Can be done randomly or according to some mask of interest</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Diffusion directions</td>\n",
    "        <td>Define the local diffusion in a voxel, for all voxels of interest</td>\n",
    "        <td>Can be done with CSA-ODF or other methods such as e.g structure tensor</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Stopping criteria</td>\n",
    "        <td>Defines where the tract continues or stops.</td>\n",
    "        <td>Can be done based on anatomy, information of diffusion direction, combination of both...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A tracking algorithm</td>\n",
    "        <td>Combines all ingredients above to generate streamlines</td>\n",
    "        <td>Line propagation techniques to grow from seed region, or probabilistic with a pdf of fiber orientations.</td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "Each of the ingredients can be changed for a different flavour. You can explore <a href=\"https://docs.dipy.org/stable/examples_built/index\">DIPY's tutorials</a> to get an idea of the changes you can operate. Feel free to play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ffc0a",
   "metadata": {},
   "source": [
    "## 2.8 Remark: Connectivity analysis based on tractography "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0f481",
   "metadata": {},
   "source": [
    "By using the generated streamlines, we could analyze the brain connectivity, for example, which streamlines pass through or not pass through some regions of the brain, how many streamlines are connecting two ROI, etc. To do this, it would be better if we could create a tractography with seeds spaning the entire white matter. Due to RAM concern, we will not do it here. But please feel free to explore it if you are interested! You will find some useful tutorials here:\n",
    "<a href=\"https://docs.dipy.org/stable/examples_built/index#streamlines-analysis-and-connectivity\">https://docs.dipy.org/stable/examples_built/index#streamlines-analysis-and-connectivity</a>\n",
    "\n",
    "This ends this short tractography tutorial! Again, do not hesitate to explore more on DIPY's website if you're interested :)\n",
    "We strongly encourage you to use TrackVis for visualization of tractograms as it's really made for it and is much more intuitive to use for this purpose than FSLeyes.\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac02d6a-d896-48e0-a36c-e280f83b37df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p><b>üéâ You've reached the end of this week's notebook! Congratulations! üéâ </b></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
