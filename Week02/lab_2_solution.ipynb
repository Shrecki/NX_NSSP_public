{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0f5458",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "\n",
    "Welcome to the NX-421 class! In today's week, you will get more familiar with preprocessing steps typically conducted in MRI analysis, specifically for anatomical MRI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961ac90d-68dc-474a-b907-bebb7812a5c6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 08:38:37.632: Failed to load module \"canberra-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/Data\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b48dd0-56f8-4f91-accb-0a447b69c4fd",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): Gtk-CRITICAL **: 08:38:48.938: gtk_window_resize: assertion 'height > 0' failed\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Start FSLeyes (very neat tool to visualize MRI data of all sorts) within Python\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c069589",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def direct_file_download_open_neuro(file_list, file_types, dataset_id, dataset_version, save_dirs):\n",
    "    # https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:sub-001_scans.tsv\n",
    "    for i, n in enumerate(file_list):\n",
    "        subject = n.split('_')[0]\n",
    "        download_link = 'https://openneuro.org/crn/datasets/{}/snapshots/{}/files/{}:{}:{}'.format(dataset_id, dataset_version, subject, file_types[i],n)\n",
    "        print('Attempting download from ', download_link)\n",
    "        download_url(download_link, op.join(save_dirs[i], n))\n",
    "        print('Ok')\n",
    "        \n",
    "def get_json_from_file(fname):\n",
    "    f = open(fname)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bb2b8-73e1-450e-b3cc-c58f9fe3a01d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this lab, we will have you look at some of the essential anatomical preprocessing steps that should be conducted before any analysis. \n",
    "\n",
    "These steps are critical: their aim is to eliminate noise and ensure that our data is put in a form where we can conduct reliable analysis. As an analogy, if you were to examine pictures of the universe and trying to analyze distant stars, you would have to remove the Milky Way's own light to examine the light of stars outside our galaxy. This is a preprocessing step. In MRI, you will find that we need to conduct several corrections on our input signal before we can analyze anything reliably.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#805AD5; color: #90EE90; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>‚ö†Ô∏è Preprocessing warnings ‚ö†Ô∏è</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "The steps of preprocessing you have seen in class are dependent on the analysis you wish to conduct.\n",
    "In particular, there is not yet a clear consensus on the order in which some steps should be applied, although said order is known to impact subsequent analysis.\n",
    "As an example, we will teach you how to put your subjects in a common space, the MNI space through a step of <b>normalization</b> (more on that later!), such that subjects can be compared at the group-level. Assume now that for the purpose of your analysis, you are interested only in a single subject, or that the method of choice should be at an individual level for your application. Clearly the normalization is superfluous in this case.\n",
    "You should also know that most steps will have parameters to set. These parameters can be set at the population level (this is often the case). Such choice means the preprocessing won't be optimal for each subject.\n",
    "<br>\n",
    "</p></span>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #805AD5; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>Preprocessing pipelines</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "We will teach you how to perform each step individually, so that you get a precise understanding of what each step's purpose is. In real-world applications, there exist dedicated softwares to do so, such as <a href=\"https://fmriprep.org/en/stable/\">fMRIPrep</a>. These softwares, however, take a long time to run (fMRIPrep would take you the whole day for one participant).\n",
    "<br>\n",
    "</p></span>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>üí° Quality Control (QC): the 1 - 10 - 100 dollar rule üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "The 1-10-100 rule states that it takes 1 dollar to verify and correct data at the start, 10 dollars to identify and clean data after the fact and 100 dollars to correct a failure due to bad data.\n",
    "\n",
    "In preprocessing this is especially true. It will take you much less effort to first look at your data, detect what might be wrong from the start and deal with it rather than apply everything blindly and notice after the fact that something went wrong. Always, ALWAYS look at your data before <u>anything and any analysis</u>. A surgeon always looks at the patient before operating, you should do the same: you are surgeons to your dataset, so please look at it carefully (it's craving for attention, the poor thing üíî).\n",
    "These intermediary steps of controlling the health of your dataset are called quality control steps. You're doing exactly what you might expect to do: you check the quality of your data before and after a given step, to ensure that nothing went awry for example.\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac62a0e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 0. Getting a dataset\n",
    "\n",
    "Before doing any analysis, we need to have data to analyse. There are many publicly available datasets in the wild, and even more ways to share them. We will show you first how to get data easily.\n",
    "\n",
    "## 0.1 The BIDS standard and baby's first dataset\n",
    "\n",
    "To avoid all different formats and dataset organisations, which would make it a pain to reproduce articles and share results, the MRI community came up with a way to organize systematically MRI datasets: the BIDS standard, a set of guidelines on how files should be named, in which folders should they be placed and so forth.\n",
    "\n",
    "The BIDS standard is, at the core, a set of rules helping you to format a dataset such that other researchers in neuroimaging can reuse your data with the smallest overhead possible. It is a way to unify how files and acquisitions are organized in folders.\n",
    "\n",
    "This unification comes with several advantages including many tools that ease our life.\n",
    "Indeed, if your dataset is in such a format, it is very easy to conduct any analysis: your scripts will expect a specific structure, so you don't need to play a million times with paths, for example. A similar analysis can then be run on many different datasets, provided they follow the BIDS standard (we say they are <b>BIDS compliant</b> when they do).\n",
    "\n",
    "A whole software ecosystem has evolved around this standard, including tools that enable us to load datasets that are BIDs compliant.\n",
    "\n",
    "This is what we show you in the cell below. In the BIDs format, a dataset has an ID to identify it. With libraries such as <a href=\"https://openneuro.org/\">open-neuro</a>, it is very easy to download such a dataset.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa685c",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üëã Hello! This is openneuro-py 2024.2.0. Great to see you! ü§ó\n",
      "\n",
      "   üëâ Please report problems ü§Ø and bugs ü™≤ at\n",
      "      https://github.com/hoechenberger/openneuro-py/issues\n",
      "\n",
      "üåç Preparing to download ds004226 ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÅ Traversing directories for ds004226 : 1042 entities [00:31, 32.62 entities/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Retrieving up to 5 files (5 concurrent downloads). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished downloading ds004226.\n",
      " \n",
      "üß† Please enjoy your brains.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_id = 'ds004226'\n",
    "subject = '001' \n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "mkdir_no_exist(bids_root)\n",
    "\n",
    "subject_dir = 'sub-{}'.format(subject)\n",
    "\n",
    "###################\n",
    "# Openneuro download.\n",
    "###################\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", op.join(subject_dir, 'anat','*'),# We are asking to get all files within the subject_dir/anat folder by using the wildcard *\n",
    "               \"--include\", op.join(subject_dir, 'func','sub-{}_task-sitrep_run-01_bold.*'.format(subject)),\n",
    "               ], check=True)\n",
    "\n",
    "###################\n",
    "# Create folders relevant for preprocessing.\n",
    "# In BIDs, ANYTHING we modify must go in the derivatives folder, to keep original files clean in case we make a mistake.\n",
    "###################\n",
    "mkdir_no_exist(op.join(bids_root, 'derivatives'))\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'anat'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'fmap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944c505",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Be mindful of waiting properly when downloading a dataset: you should avoid opening a file until it is fully downloaded, otherwise you have a high chance of corrupting it!\n",
    "\n",
    "As you can see from the command above, what we have done is:\n",
    "- Asked to download dataset with ID ds004226\n",
    "- Asked to include only the folders of subject sub-001\n",
    "- In these folders, we took all files in the anat/ folder (anatomical files), all files in func/ (functional folder) and in fmap/ (Fieldmaps, you will see in coming weeks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1091c",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Once the download is done, we can have a look at the resulting folder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6ea4c4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ sub-001_scans.tsv\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ fmap/\n",
      "|--------- sub-001_acq-task_dir-AP_epi.json\n",
      "|--------- sub-001_acq-task_dir-AP_epi.nii.gz\n",
      "|--------- sub-001_acq-task_dir-PA_epi.json\n",
      "|--------- sub-001_acq-task_dir-PA_epi.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-01_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-02_bold.json\n",
      "|--------- sub-001_task-sitrep_run-02_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-02_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-03_bold.json\n",
      "|--------- sub-001_task-sitrep_run-03_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-03_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-04_bold.json\n",
      "|--------- sub-001_task-sitrep_run-04_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-04_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-05_bold.json\n",
      "|--------- sub-001_task-sitrep_run-05_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-05_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-06_bold.json\n",
      "|--------- sub-001_task-sitrep_run-06_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-06_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-07_bold.json\n",
      "|--------- sub-001_task-sitrep_run-07_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-07_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-08_bold.json\n",
      "|--------- sub-001_task-sitrep_run-08_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-08_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-09_bold.json\n",
      "|--------- sub-001_task-sitrep_run-09_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-09_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-10_bold.json\n",
      "|--------- sub-001_task-sitrep_run-10_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-10_events.tsv\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62bc37",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This organization is typical of a BIDs dataset. \n",
    "\n",
    "Each subject's file is split between anatomical data and functional data. You are already a bit familiar with the .nii.gz file extension: it is the extension of MRI files, typically, but what might be the .json file? Well, let's open it to figure it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46573233",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Modality': 'MR',\n",
       " 'MagneticFieldStrength': 3,\n",
       " 'Manufacturer': 'Siemens',\n",
       " 'ManufacturersModelName': 'Skyra',\n",
       " 'InstitutionName': 'Princeton_University_-_Neuroscience_Institute',\n",
       " 'InstitutionalDepartmentName': 'Department',\n",
       " 'InstitutionAddress': 'Washington_and_Faculty_Rd._-_Building_25_25_Princeton_NJ_US_085',\n",
       " 'DeviceSerialNumber': '45031',\n",
       " 'StationName': 'AWP45031',\n",
       " 'BodyPartExamined': 'BRAIN',\n",
       " 'PatientPosition': 'HFS',\n",
       " 'ProcedureStepDescription': 'TamirL_Mark',\n",
       " 'SoftwareVersions': 'syngo_MR_E11',\n",
       " 'MRAcquisitionType': '2D',\n",
       " 'SeriesDescription': 'EPI_2.5mm_1.5TR_32TE_SMS4_Siemens',\n",
       " 'ProtocolName': 'EPI_2.5mm_1.5TR_32TE_SMS4_Siemens',\n",
       " 'ScanningSequence': 'EP',\n",
       " 'SequenceVariant': 'SK',\n",
       " 'ScanOptions': 'FS',\n",
       " 'SequenceName': '_epfid2d1_78',\n",
       " 'ImageType': ['ORIGINAL', 'PRIMARY', 'M', 'ND', 'NORM', 'MOSAIC'],\n",
       " 'SeriesNumber': 9,\n",
       " 'AcquisitionTime': '16:17:36.350000',\n",
       " 'AcquisitionNumber': 1,\n",
       " 'SliceThickness': 2.5,\n",
       " 'SpacingBetweenSlices': 2.5,\n",
       " 'SAR': 0.0635751,\n",
       " 'EchoTime': 0.032,\n",
       " 'RepetitionTime': 1.5,\n",
       " 'FlipAngle': 70,\n",
       " 'PartialFourier': 1,\n",
       " 'BaseResolution': 78,\n",
       " 'ShimSetting': [3407, 8775, 9441, 650, 135, 271, -116, 348],\n",
       " 'TxRefAmp': 300.048,\n",
       " 'PhaseResolution': 1,\n",
       " 'ReceiveCoilName': 'HeadNeck_64',\n",
       " 'ReceiveCoilActiveElements': 'HC1-7',\n",
       " 'PulseSequenceDetails': '%SiemensSeq%_ep2d_bold',\n",
       " 'ConsistencyInfo': 'N4_VE11C_LATEST_20160120',\n",
       " 'MultibandAccelerationFactor': 4,\n",
       " 'PercentPhaseFOV': 100,\n",
       " 'EchoTrainLength': 78,\n",
       " 'PhaseEncodingSteps': 78,\n",
       " 'AcquisitionMatrixPE': 78,\n",
       " 'ReconMatrixPE': 78,\n",
       " 'BandwidthPerPixelPhaseEncode': 20.678,\n",
       " 'EffectiveEchoSpacing': 0.000620007,\n",
       " 'DerivedVendorReportedEchoSpacing': 0.000620007,\n",
       " 'TotalReadoutTime': 0.0477406,\n",
       " 'PixelBandwidth': 2005,\n",
       " 'DwellTime': 3.2e-06,\n",
       " 'PhaseEncodingDirection': 'j-',\n",
       " 'SliceTiming': [0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365],\n",
       " 'ImageOrientationPatientDICOM': [0.999593,\n",
       "  0.0224594,\n",
       "  0.0175892,\n",
       "  -0.0207639,\n",
       "  0.99561,\n",
       "  -0.0912658],\n",
       " 'InPlanePhaseEncodingDirectionDICOM': 'COL',\n",
       " 'ConversionSoftware': 'dcm2niix',\n",
       " 'ConversionSoftwareVersion': 'v1.0.20171215 GCC4.8.5'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_json_from_file(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53284c",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This JSON is extremely important: it is what we call a JSON **sidecar**, and it holds precious acquisition informations! Based **only on the text printed above**, are you able to determine any or all of the following?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7c813e-ba6b-4b74-b795-59046ee262a6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd53946340914d2a92c29cc2f5e55c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Select all which apply</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58de03fb166848f4890e66e8ac476591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Checkbox(value=False, description='TR ?', layout=Layout(width='99%')), HTML(valu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_MCQ(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be8d2b-4352-4ae7-b2bb-96473d2f7507",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üèçÔ∏è JSON sidecars üèçÔ∏è</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    A JSON sidecar is critical as you can see in the BIDS standard. It incorporates many relevant informations about your MRI acquisition. As such whenever you download a dataset, you should always make sure you don't include just the Nifti files (.nii.gz or .nii files), but also the .json, as otherwise you will be lacking critical information.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5574168",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 0.2 Loading more datasets: how to\n",
    "\n",
    "Great! Note that we've loaded only one subject and one file of each modality for the subject. You can have a look <a href=\"https://openneuro.org/datasets/ds004226/versions/1.0.0\">here</a> for this dataset. As you can see, it is a big dataset; we've restricted our download to the bare minimum to spare your computer's disk space as much as possible.\n",
    "\n",
    "Notice two things on the web page. The first is the dataset's accession number:\n",
    "<img src=\"imgs/openneuro_access_nbr.png\">\n",
    "This number is the one we've put in our code earlier, to specify what dataset we wanted to load from:\n",
    "```python\n",
    "dataset_fmap = 'ds004226'\n",
    "...\n",
    "openneuro.download(dataset=dataset_fmap, ...)\n",
    "\n",
    "```\n",
    "\n",
    "Should you wish to download another openneuro dataset that piqued your interest, you'll simply need to change the above variable with the accession number of the dataset on the corresponding page!\n",
    "\n",
    "Secondly, you can observe the entire folder structure, size and many other interesting information of this dataset by simply scrolling its page!\n",
    "<img src=\"imgs/openneuro_full_view.png\">\n",
    "You can really decide whether a dataset is what you need this way, before burdening your connection with any heavy download :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abf645",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Anatomical preprocessing\n",
    "\n",
    "Let's have a look at the nice anatomical we downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a7d85f",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): Gdk-WARNING **: 08:45:11.703: gdkdrawable-x11.c:952 drawable is not a pixmap or window\n"
     ]
    }
   ],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12ca9e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Take some time to explore the exquisite anatomy of the brain. Notice around the brain, the human skull. It is full of regions which show up **in this contrast** whiter than others. Based on what you might know from class about the T1 contrast, can you identify the different regions annotated below taken from another T1?\n",
    "\n",
    "As a hint, think about white matter: it is composed of axons and myelin. Contrast with grey matter, which is comprised of soma. Based on your understanding:\n",
    "<img src=\"imgs/annotated_regions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea21ddd4-2e56-4049-9426-29215d5c84cb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074e7065e9bc4008b3c3775caf0363d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Select all which apply</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5489da479b2489bad47730f88702d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose:', layout=Layout(width='100%'), options=('Tissues high in fat are bright in T‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de173dbfb715404887777c91d84db370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9c1570e9a4b13b1d3b18565a48a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1dd0ffa18f492f8e395d6a3f6f8701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose:', layout=Layout(width='100%'), options=('Region 1 is likely high in fibers a‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe08fc9970c4def83f6fb98cffb7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b59b42653942c490f500e379425c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e41d3a20b541b5a186f2193c887330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose:', layout=Layout(width='100%'), options=('Region 2 contains a mix of fat and ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ca5238071d4f4f8c03d78e96483ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c05b2e2666643bf8f295d3229ab5389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa34425ca7e4298bf7459f9b8ad1a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose:', layout=Layout(width='100%'), options=('Region 3 contains air, which is why‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68c4183dcca464c872cb66b071d51ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_MCQ(2,2)\n",
    "interactive_MCQ(2,3)\n",
    "interactive_MCQ(2,4)\n",
    "interactive_MCQ(2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ca769",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.1 Skull stripping\n",
    "\n",
    "### 1.1.1 Preprocessing and BIDs\n",
    "An important part of **anatomical** preprocessing is to remove the skull around the brain.\n",
    "To adhere to the BIDs format, all modified files should be put in a new folder, called derivatives, such that you always have clean data in the source directory. The derivatives folder can be used for different preprocessing and treatments, each needing their own subfolders. In our case, we've created a single folder, preprocessed_data, hence the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c56df69",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ sub-001_scans.tsv\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ fmap/\n",
      "|--------- sub-001_acq-task_dir-AP_epi.json\n",
      "|--------- sub-001_acq-task_dir-AP_epi.nii.gz\n",
      "|--------- sub-001_acq-task_dir-PA_epi.json\n",
      "|--------- sub-001_acq-task_dir-PA_epi.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-01_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-02_bold.json\n",
      "|--------- sub-001_task-sitrep_run-02_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-02_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-03_bold.json\n",
      "|--------- sub-001_task-sitrep_run-03_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-03_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-04_bold.json\n",
      "|--------- sub-001_task-sitrep_run-04_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-04_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-05_bold.json\n",
      "|--------- sub-001_task-sitrep_run-05_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-05_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-06_bold.json\n",
      "|--------- sub-001_task-sitrep_run-06_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-06_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-07_bold.json\n",
      "|--------- sub-001_task-sitrep_run-07_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-07_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-08_bold.json\n",
      "|--------- sub-001_task-sitrep_run-08_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-08_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-09_bold.json\n",
      "|--------- sub-001_task-sitrep_run-09_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-09_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-10_bold.json\n",
      "|--------- sub-001_task-sitrep_run-10_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-10_events.tsv\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b113b67",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1.1.2 Actual skull stripping\n",
    "\n",
    "Perfect! Let's move on to actually extracting the brain! To make it easier for you to detect what was actually extracted, we will let the brain extraction proceed, using FSL's <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/docs/#/structural/bet\">BET</a> (brain extraction tool) and show you the mask of the region determined by FSL as brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ef5c06",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_skull_stripped_anatomical(bids_root, preproc_root, subject_id, robust=False):\n",
    "    \"\"\"\n",
    "    Function to perform skull-stripping (removing the skull around the brain).\n",
    "    This is a simple wrapper around the brain extraction tool (BET) in FSL's suite\n",
    "    It assumes data to be in the BIDS format (which we will cover in the following weeks).\n",
    "    The method also saves the brain mask which was used to extract the brain.\n",
    "\n",
    "    The brain extraction is conducted only on the T1w of the participant.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bids_root: string\n",
    "        The root of the BIDS directory\n",
    "    preproc_root: string\n",
    "        The root of the preprocessed data, where the result of the brain extraction will be saved.\n",
    "    subject_id: string\n",
    "        Subject ID, the subject on which brain extraction should be conducted.\n",
    "    robust: bool\n",
    "        Whether to conduct robust center estimation with BET or not. Default is False.\n",
    "    \"\"\"\n",
    "    # We perform here skull stripping (you'll learn more about it next week!).\n",
    "    # For now all you need to do is that we remove the bones and flesh from the MRI to get the brain!\n",
    "    subject = 'sub-{}'.format(subject_id)\n",
    "    anatomical_path = op.join(bids_root, subject, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id))\n",
    "    betted_brain_path = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w'.format(subject_id))\n",
    "    os.system('bet {} {} -m {}'.format(anatomical_path, betted_brain_path, '-R' if robust else ''))\n",
    "    print(\"Done with BET.\")\n",
    "\n",
    "resulting_mask_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_mask')\n",
    "get_skull_stripped_anatomical(bids_root, preproc_root, \"001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6738408a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c91a6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Is the mask nicely fitting around the brain? What you would like is that the mask is taking all parts of the brain and excluding the rest.\n",
    "To answer this one, play with the mask's opacity in FSL eyes.<br>\n",
    "*Hint: have a look at the frontal regions. Inspect as well the superior parietal regions*<br>\n",
    "<br><br>\n",
    "What you are doing here is simply **Quality Control** (QC). It is a crucial step that you should **NEVER** skip when dealing with data preprocessing. As all steps are dependent on the success of previous steps, always make sure that everything is performing properly before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d4348",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1.1.3 Improving the fit\n",
    "If you look a bit into bet's documentation, you'll quickly find that there are parameters with which you can play; robust brain centre estimation and fractional intensity threshold. To demonstrate the importance and impact of these parameters, let's use a robust brain center estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d66735d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_skull_stripped_anatomical(bids_root, preproc_root, \"001\", robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361984c7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:48.818: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:47:52.305: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n"
     ]
    }
   ],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz'))\n",
    "fsleyesDisplay.load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4141db1",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "How good is the mask now?\n",
    "\n",
    "Is it perfect? (*Hint: have a look at voxels around 94-209-131*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4679f",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1.1.4 Manual corrections\n",
    "If you really want good fit, you might want to resort to **manually correcting the mask**. \n",
    "\n",
    "FSLeyes readily allows you to do such things! While on FSLeyes, press **Alt + E** to open the editing interface.\n",
    "<img src=\"imgs/editing_menu_fsl.png\">\n",
    "<center><i>FSLeyes editing menu</i></center>\n",
    "\n",
    "We will work here on removing some unwanted voxels. Toggle the 'Select mode' first. This way, FSL will show us which voxels we currently have selected, before changing their value\n",
    "<img src=\"imgs/selection_mode_toggle_fsl.png\">\n",
    "<center><i>Make sure to be in Select mode by clicking it</i></center>\n",
    "\n",
    "Then let's pick the pencil tool, to select the voxels we want.\n",
    "<img src=\"imgs/brush_tool.png\">\n",
    "\n",
    "Good, we're set and we can now select voxels. We'll try to select some **unwanted** voxels. Simply paint over them!\n",
    "<br>\n",
    "<center><img src=\"imgs/painted_voxels.png\"></center>\n",
    "<center><i>Selected voxels are shown in purple</i></center>\n",
    "\n",
    "Now, we are dealing with a mask. We thus want to put the value of our selection to **0**, so as to remove it from the mask. To do so, we must change the fill value to 0, and then click to replace our selection with the provided value:\n",
    "<br>\n",
    "<img src=\"imgs/paint_steps.png\">\n",
    "<center><i>The two steps to set selection to a specific fill value.</i></center>\n",
    "<br>\n",
    "<img src=\"imgs/mask_painted.png\">\n",
    "<center><i>Painting with zero a mask means we remove the painted voxels from the mask.</i></center>\n",
    "\n",
    "It remains now to apply the mask to our anatomical data. This is fortunately something that you now know how to do from the previous lab! Fill in the next cell with the appropriate code **and make sure to save the masked brain in the proper directory**.\n",
    "\n",
    "You can then escape back to the non-edit mode by pressing again **Alt + E**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd046f16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Fill me with the code to use your mask\n",
    "# To help you, we provide you with the skeleton of two potential approaches.\n",
    "# You can fill either of them. Do not forget to test them by visualizing the result!\n",
    "# For fslmaths, you can either read the documentation, or execute it without argument by running os.system('fslmaths')\n",
    "##################################\n",
    "\n",
    "# Option 1: Pythonic approach.\n",
    "def apply_python_mask_approach(img_path, mask_path, masked_img_path):\n",
    "    \"\"\"\n",
    "    The first approach, Pythonic way. The goal is, given a mask, to apply it to a T1 where the brain is to be extracted.\n",
    "    \n",
    "    YOU SHOULD COMPLETE THE METHOD AS IT DOES NOT WORK RIGHT NOW!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path: str\n",
    "        Path to the image on which we would like to apply the mask (in your case, the T1 with the skull still on). Should be a .nii.gz file\n",
    "    mask_path: str\n",
    "        Path to the mask you would like to apply to your image. Should be a .nii.gz file, containing only binary values (0 or 1)\n",
    "    masked_img_path: str\n",
    "        Path to which the resulting image will be saved.\n",
    "    \"\"\"\n",
    "    import nibabel as nib\n",
    "\n",
    "    # Load both the T1 and the mask from disk\n",
    "    img = nib.load(img_path)\n",
    "    mask = nib.load(mask_path)\n",
    "    \n",
    "    # Load the data from both above images as numpy arrays\n",
    "    img_data = img.get_fdata()\n",
    "    mask_data = mask.get_fdata()\n",
    "\n",
    "    #######################\n",
    "    # Solution 1\n",
    "    # Create an empty image and select all which falls in the mask (perhaps the most natural way to think about the mask)\n",
    "    #######################\n",
    "    # In all positions within the mask, get the image content\n",
    "    saved_img_data = np.zeros(img_data.shape)\n",
    "    saved_img_data[mask_data > 0] = img_data[mask_data > 0]\n",
    "\n",
    "    # Save the image to disk, by creating a new Nifti image and then writing it out\n",
    "    img_out = nib.Nifti1Image(saved_img_data,img.affine, img.header)\n",
    "    nib.save(img_out, masked_img_path)\n",
    "\n",
    "    #######################\n",
    "    # Solution 2\n",
    "    # Another approach is to remove from img_data all that is outside the mask.\n",
    "    #######################\n",
    "    \n",
    "    # In all positions OUTSIDE the mask (where it is equal to 0), throw away the image\n",
    "    img_data[mask_data == 0] = 0\n",
    "    \n",
    "    # Save the image to disk, by creating a new Nifti image and then writing it out\n",
    "    img_out = nib.Nifti1Image(img_data,img.affine, img.header)\n",
    "    nib.save(img_out, masked_img_path)\n",
    "    \n",
    "def apply_fsl_math_approach(img_path, mask_path, masked_img_path):\n",
    "    ###########################\n",
    "    # Solution\n",
    "    # By reading fslmaths documentation, one can see that the -mas option is exactly what we desire.\n",
    "    ###########################\n",
    "    os.system('fslmaths {} -mas {} {}'.format(img_path, mask_path, masked_img_path))\n",
    "    \n",
    "\n",
    "anatomical_path = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz') # The original brain\n",
    "betted_brain_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz') # The brain without skull is in the derivatives folder\n",
    "resulting_mask_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_mask.nii.gz') # The mask to use\n",
    "\n",
    "########################\n",
    "# CHOOSE ONE OF THE TWO TO IMPLEMENT IT AND LAUNCH IT\n",
    "########################\n",
    "apply_fsl_math_approach(anatomical_path, resulting_mask_path, betted_brain_path)\n",
    "apply_python_mask_approach(anatomical_path, resulting_mask_path, betted_brain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afc300-6f6f-4383-b07e-f7d86941acf9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As always, do not forget to visualize what you have done. If you did a proper job, you should now see the brain without any skull!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "470411d9-b778-4156-bfcd-99d9b57a04ad",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(betted_brain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b107c2-dd19-4dc2-b022-a159e6994913",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note: If you get a RuntimeError: wrapped C/C++ object of type OrthoEditActionToolBar has been deleted, do not worry, this is simply because you forgot to escape back to non-edit mode (Alt+E; option+E for MacOS) before resetting the overlay, but it is a benign error :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548c592",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.2 Tissue segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b2abc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For the purpose of analysis, it can be useful to separate the tissues into tissue classes; in particular extracting the white matter, grey matter and cerebrospinal fluid (abreviated as CSF) is very interesting in fMRI analysis. Consider for example an analysis that you wish to restrict to the somas of your neurons, would it make sense to conduct your analysis on the CSF ?\n",
    "\n",
    "You'll find that the segmentation is not done on fMRI volumes; it is done on the anatomical and the resulting tissue masks are then used on the functional data. Can you imagine why this is the case?\n",
    "\n",
    "Let's perform tissue segmentation. To do so, we'll use FSL's <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST\">FAST</a> (FMRIB's Automated Segmentation Tool).\n",
    "\n",
    "The underlying idea of FAST is to try and model each voxel's intensity as being a mixture between the different tissue types.\n",
    "Pay attention in the documentation to the following line:\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b></b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Before running FAST an image of a head should first be brain-extracted, using BET. The resulting brain-only image can then be fed into FAST.</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "Based on this, **in the cell below choose which image should be used as fast_target, between the anatomical_path and the brain_extracted_path images**.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:orange; color: #112A46; border-left: solid red 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üêû Troubleshooting: FSL stopped responding </b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    It is perfectly possible (even likely) that FSLeyes will stop responding over the course of this lab. This is perfectly normal! Simply wait for whichever function (such as FAST) to finish and it should start responding again, don't worry too quickly, be patient :)</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "Note that FAST will take one or two minutes to run, this is expected, do not panic :)\n",
    "\n",
    "P.S: Recently, deep learning tools started generating good enough tissue segmentations, such as the FSL-based <a href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/SynthSeg\">SynthSeg</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4be9e751",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anatomical_path = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz')\n",
    "bet_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "\n",
    "#########################\n",
    "# Solution\n",
    "# By reading above: we must apply FAST to the brain-extracted image. Thus we must use the BET path brain.\n",
    "##########################\n",
    "fast_target = bet_path # Replace with either anatomical_path or bet_path (note: you can try both and decide which is more reasonable!)\n",
    "\n",
    "[os.remove(f) for f in glob.glob(op.join(preproc_root, 'sub-001', 'anat', '*fast*'))] # Just to clean the directory in between runs of the cell\n",
    "segmentation_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast')\n",
    "fast(imgs=[fast_target], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525450b2",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's check the quality of the segmentation, shall we?\n",
    "We want to extract 3 tissue types here: the white matter, the grey matter and the csf. How well did fast perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd7eb6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If you look at the directories now, we have new files in our hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa0fd6ab",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|--------------- sub-001_T1w.nii.gz\n",
      "|--------------- sub-001_T1w_fast_mixeltype.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_0.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_1.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_2.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pveseg.nii.gz\n",
      "|--------------- sub-001_T1w_fast_seg.nii.gz\n",
      "|--------------- sub-001_T1w_mask.nii.gz\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ sub-001_scans.tsv\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ fmap/\n",
      "|--------- sub-001_acq-task_dir-AP_epi.json\n",
      "|--------- sub-001_acq-task_dir-AP_epi.nii.gz\n",
      "|--------- sub-001_acq-task_dir-PA_epi.json\n",
      "|--------- sub-001_acq-task_dir-PA_epi.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-01_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-02_bold.json\n",
      "|--------- sub-001_task-sitrep_run-02_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-02_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-03_bold.json\n",
      "|--------- sub-001_task-sitrep_run-03_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-03_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-04_bold.json\n",
      "|--------- sub-001_task-sitrep_run-04_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-04_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-05_bold.json\n",
      "|--------- sub-001_task-sitrep_run-05_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-05_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-06_bold.json\n",
      "|--------- sub-001_task-sitrep_run-06_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-06_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-07_bold.json\n",
      "|--------- sub-001_task-sitrep_run-07_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-07_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-08_bold.json\n",
      "|--------- sub-001_task-sitrep_run-08_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-08_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-09_bold.json\n",
      "|--------- sub-001_task-sitrep_run-09_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-09_events.tsv\n",
      "|--------- sub-001_task-sitrep_run-10_bold.json\n",
      "|--------- sub-001_task-sitrep_run-10_bold.nii.gz\n",
      "|--------- sub-001_task-sitrep_run-10_events.tsv\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e1ac9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The pve files correspond to our segmented tissues. We have exactly three files, because we set n_classes to 3 above:\n",
    "```python\n",
    "fast(..., n_classes=3)\n",
    "```\n",
    "\n",
    "Let's try to identify which segmentation is which tissue type in the brain. To do this, you'll have to visualize the tissues and decide for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd5ad0-b05f-4199-aded-39280d33dc16",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To make it easier on you, we will display:\n",
    "\n",
    "- pve_0 in <span style=\"color:red;\">red</span>\n",
    "- pve_1 in <span style=\"color:green;\">green</span>\n",
    "- pve_2 in <span style=\"color:blue;\">blue</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29c1afa4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.427: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.427: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.427: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.427: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.428: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 08:55:34.429: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n"
     ]
    }
   ],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(bet_path)\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_0*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_1*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_2*'))[0])\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[1]).cmap = 'Red'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[2]).cmap = 'Green'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[3]).cmap = 'Blue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "626e0575-431b-4828-90be-61e0240e7a17",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .widget-radio-box label {\n",
       "        white-space: normal !important;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5384b5934ec248709b3a31d737de2a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0234a0beaf49ac95d14ab45d763d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose:', layout=Layout(width='100%'), options=('pve_0 is white matter, pve_1 is gre‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd58c4103e44deca296ed62e0cb15fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_MCQ(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd536883",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(2,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbcd4d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üè† Tissues and contrast: Take home message üè†</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Tissues in T1-w or T2-w images will show up in different colour, dependent on their content. This is because their content affects their relaxation time and, in turn, the intensity captured during the acquisition. Here are different tissue types for both modalities (from <a href=\"https://www.researchgate.net/publication/324396120_Basic_MRI_for_the_liver_oncologists_and_surgeons\">Vu, Lan N., John N. Morelli, and Janio Szklaruk. \"Basic MRI for the liver oncologists and surgeons.\" Journal of hepatocellular carcinoma 5 (2017): 37.</a>):\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>MR</th>\n",
    "            <th>High signal (bright)</th>\n",
    "            <th>Low signal (dark)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>T1-w</th>\n",
    "            <th>Fat, melanin</th>\n",
    "            <th>Iron</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Blood</th>\n",
    "            <th>Water</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Proteinaceous fluid</th>\n",
    "            <th>Air, bone</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Paramagnetic substances</th>\n",
    "            <th>Collagen</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Chelated gadolinium contrast</th>\n",
    "            <th>Most tumors</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>T2-w</th>\n",
    "            <th>Water</th>\n",
    "            <th>Air</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Edema</th>\n",
    "            <th>Bone</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Blood</th>\n",
    "            <th>Hemosiderin, deoxyhemoglobin, methemoglobin</th>\n",
    "        </tr>\n",
    "    </table>\n",
    "    Typical preprocessing steps of anatomical data starts by extracting the brain by removing skull tissues. This step can be conducted mostly automatically, but it is perfectly possible to manually correct the extracted brain, either to include more or less voxels when tweaking the parameters does not yield satisfactory results.\n",
    "    The extracted brain can be segmented into different tissues. Using the difference in brightness due to contrast, we can separate the grey matter, the white matter and the CSF, which is useful for later analysis.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb39193",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 2: Coregistration of images, a critical preprocessing step\n",
    "\n",
    "Now that you're familiar with how fMRI works, we will have you conduct what is called coregistration.\n",
    "As you will see in class, one of the key issues is to ensure MRI images are well aligned to each other. There are different reasons: comparison between participants during analysis, overlaying of anatomical MRI with functional MRI (to use for example an atlas based on anatomy), correction of motion...\n",
    "\n",
    "\n",
    "We'll show you the following steps:\n",
    "- Download a participant's dataset\n",
    "- Visualize data together\n",
    "- Remove the skull\n",
    "- Coregister a brain to another\n",
    "\n",
    "For our purpose, we will align an anatomical MRI with the MNI anatomical template you displayed before.\n",
    "This process is typically called **normalization**: we put the MRI into a \"standard\" space of reference. However the principle we'll show you can be applied to just any pair of images.\n",
    "\n",
    "<h2>2.1 Aligning images manually</h2>\n",
    "\n",
    "We will exploit the dataset we've been using up until now. You could, if you want, download another dataset to make sure you understood everything up until now and work on it of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07bc09",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3>2.1.1 Visualizing the data together</h3>\n",
    "\n",
    "The volume of interest will be the anatomical file of subj-001, located under /home/jovyan/Data/dataset/ds004226/derivatives/preprocessed_data/sub-001/anat/sub-001_T1w.nii.gz .\n",
    "Remember: thanks to the BIDs standard, you can tell it is an anatomical by its folder (anat/) and its name (T1w).\n",
    "\n",
    "We will use freeview to visualise the data, both to show you another tool besides FSLeyes but also because moving images manually is handier there :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8be1ef80",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def launch_freeview(img_list):\n",
    "    \"\"\"\n",
    "    Wrapper around freeview to launch it with several images.\n",
    "    This wrapper is necessary to launch freeview in a separate thread, ensuring the notebook is free to do something else.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_list: list of string\n",
    "        List of images (files) to load. Assumed by default to be volume files.\n",
    "    \"\"\"\n",
    "    args = []\n",
    "    \n",
    "    for i in range(len(img_list)):\n",
    "        args.append(\"-v\")\n",
    "        args.append(img_list[i])\n",
    "    # Run the command\n",
    "    subprocess.run([\"freeview\"] + args)\n",
    "\n",
    "imgList = [op.expandvars('$FSLDIR/data/standard/MNI152_T1_1mm.nii.gz'), \n",
    "           op.join(preproc_root, \"sub-001\", \"anat\", \"sub-001_T1w.nii.gz:colormap=greyscale\")] # Modify here this list to add any image you want, in .nii.gz format\n",
    "freeview_thread = threading.Thread(target=launch_freeview, args=(imgList,)) # Remark the (imgList,) when passing to args. This is very important to make Thread work properly\n",
    "\n",
    "# Start the thread\n",
    "freeview_thread.start()\n",
    "\n",
    "print(\"Freeview is running in a separate thread.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cc008",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wait, we can't see anything apart from the top brain.\n",
    "No worries! Simply set the background to be transparent by ticking the Clear Background option, as in the picture below:\n",
    "\n",
    "<img src=\"imgs/freeview_tick.png\" style=\"width: 1000px; height:auto; border: black 6px groove;\"/>    \n",
    "\n",
    "You should then notice the brains are not too aligned to each other. For example, in the coronal view:\n",
    "\n",
    "<img src=\"imgs/bad_alignment.png\" style=\"width: 1000px; height:auto; border: black 6px groove;\"/>    \n",
    "\n",
    "This is because there is no absolute system of reference. We need to align MRIs to one another ourselves, through coregistration.\n",
    "\n",
    "<h3>2.1.3 Aligning images together</h3>\n",
    "\n",
    "Let's start with a straightforward approach: you will align the images manually. In Freeview, click in Tools > Transform Volume.\n",
    "You should get the following panel:\n",
    "\n",
    "<img src=\"imgs/freeview_panel.png\" style=\"width: 1000px; height:auto; border: black 6px groove;\"/>    \n",
    "\n",
    "<p style=\"font-size:25px;\"><b>Now, play with the sliders of translation and rotation to align the anatomical to the reference. </b> Try to align the two brains as best you can.</p>\n",
    "\n",
    "Congratulations, you have done your first brain coregistration! \n",
    "\n",
    "What features did you use to visually align the brains to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdae67",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>üí° Image coregistration üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "What you have just done manually is a coregistration of a subject MRI (sub-001) to a reference image (the MNI152 template image). This step is critical when handling a group of participants.\n",
    "In this way, you make sure that all brains are sitting in the same space and can thus be analyzed in equal manner.\n",
    "\n",
    "This step is absolutely critical. You should always check how well your images are aligned to one another, even after automated approaches. You might sometimes need to align the reference yourself, which you now know how to do.\n",
    "You will now learn how to do it automatically.</p></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0998e80",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2>2.2 A more principled way</h2>\n",
    "The MRI and the template are not very well aligned, but we can try to make them more aligned. Specifically, we would like to find a transformation such that we can align our anatomical to the MNI template. This is the so-called normalization step.\n",
    "So we need two ingredients to do this:\n",
    "- A way to compute the transformation from anatomical to the MNI template (this step is called registration)\n",
    "- A reference image in the space of the MNI template (here, actually, this is the MNI template)\n",
    "\n",
    "#### 2.2.1 What transformation?\n",
    "\n",
    "What should this transformation be?\n",
    "It is a combination of translation, rotation, scaling and other possible modifications, applied on our anatomical, so that it ends up matched to the target (the MNI image). In essence, the transformation fully describes the process to align the two images!\n",
    "\n",
    "\n",
    "#### 2.2.2 Why the reference ?\n",
    "<p>\n",
    "Let's pause for a moment. If the transformation already encapsulates all that there is to know about how to transform the volume, why do we need a reference from the target space?\n",
    "\n",
    "To answer this, let's think about what happens when moving the anatomical image, shall we?\n",
    "We rotate it, translate it, maybe shear it, cool, we have an anatomical well registered. But there's still an issue.\n",
    "<br><b>What is the resolution of our anatomical?</b><br>\n",
    "If you remember, the anatomical has an exquisite spatial resolution, but it might not be exactly the same as the MNI template: what if you used the MNI with 0.5mm voxel size from last week for example? In this case, we have a mismatch in resolutions. Let's get concrete with an example with Ducky!\n",
    "</p>\n",
    "\n",
    "#### 2.2.3 Ducky's sunglasses\n",
    "<p>    \n",
    "Imagine the following example: we have an image of a duck (Ducky!), and we want to align sunglasses to it. \n",
    "Notice that this is hard, algorithmically, right? Unlike the brain, there  are no landmarks to use such as white-matter to try and optimize some cost. Sunglasses could be worn on the beak, the head, even the neck: there are no rules to fashion!\n",
    "Fortunately, we are told Ducky should wear the glasses on its beak like the cool kids and are even provided the transformation to do so, a combination of translation, rotation and scaling. Applying this transform to the sunglasses, we get the following:\n",
    "    \n",
    "<div>\n",
    "<center>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/ducky/ducky_alone.png\" style=\"width: 200px; height:auto; border: blue 6px groove;\"/>\n",
    "    <p style=\"text-align:center;\">Ducky (our reference!)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/ducky/sunglasses_alone.png\"/ style=\"width: 150px; height:auto;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">The sunglasses (in their own space)</p>\n",
    "</div>   \n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/ducky/ducky_summary.png\" style=\"width: 200px; height:auto; border: blue 6px groove;\"/>\n",
    "    <p style=\"text-align:center;\">What the transformation will do</p>\n",
    "</div>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "Okay, we do all this. We obtain this:\n",
    "<br>\n",
    "<center><img src=\"imgs/ducky/ducky_total_uncropped.png\" style=\"width: 200px; height:auto; border: black 6px groove;\"/></center>\n",
    "\n",
    "Now, why is Ducky's image so big?\n",
    "The answer is simple: the sunglasses and Ducky did not have the same:\n",
    "- Resolution (meaning the glasses can get blurry, think of putting a 144px resolution screenshot on a 4K resolution image!)\n",
    "- Field of view (canvas size if you're thinking of Photoshop layers for example)\n",
    "\n",
    "Here are the field of view of the two images, kept as is :<br>\n",
    "<center><img src=\"imgs/ducky/ducky_uncropped.png\" style=\"width: 200px; height:auto; border: black 6px groove;\"/></center>\n",
    " \n",
    "But we would like to put the sunglasses on ducky without increasing the size around ducky. <b>So let's use Ducky's picture's resolution and field of view to correct our transformed sunglasses' own resolution and field of view</b>.\n",
    "In other words, we will interpolate the sunglasses to match our duck's resolution and we will crop its field of view (or pad it if necessary) to match perfectly our duck. Final result:\n",
    "    <center><img src=\"imgs/ducky/ducky_complete.png\" style=\"width: 200px; height:auto; border: black 6px groove;\"/>    <p style=\"text-align:center;\">Much better.</p>\n",
    "</center>\n",
    "\n",
    "</p>\n",
    "\n",
    "#### 2.2.4 Back to neurological data\n",
    "\n",
    "We use exactly the same idea when applying transformations. It is for this reason that when applying a transformation in FSL, you will always need to pass a reference image of the space in which you want to end up. This way, FSL will adapt the field of view but also the resolution by interpolation. This interpolation parameter can be done through nearest neighbour, trilinear or sinc interpolation.\n",
    "\n",
    "\n",
    "### 2.2.5 Types of normalization\n",
    "\n",
    "So, you now know that you need a transformation and a reference. Great. Now, the transformation you allow can be of two types: it can be linear, meaning whatever you apply will be the same across the entire image, or non linear, where each voxel gets a separate treatment\n",
    "\n",
    "(ducky linear and non linear)\n",
    "\n",
    "\n",
    "\n",
    "## 2.3 Actually doing it: Linear normalization\n",
    "\n",
    "To perform linear normalization, the idea is simple. The transformation we want should be linear - ie, affine.\n",
    "Such a matching is usually called in image processing <a href=\"https://en.wikipedia.org/wiki/Image_registration\">image registration</a>. Here, we're dealing with 3D data, so the problem is a bit more complicated. Fortunately all of this has been coded by very smart people, and to our rescue comes a tool specifically to register volumes to each other: <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/flirt/index\">FLIRT</a>!\n",
    "<br>\n",
    "This tool can allow many registrations and is extremely powerful. In its most basic form, it expects:\n",
    "- An input volume, the volume you want to register (Ducky's sunglasses)\n",
    "- A reference volume, to which the input is registered (Ducky's body)\n",
    "- An output volume, the result of the transformation (Ducky's sunglasses once they are on Ducky's beak)\n",
    "\n",
    "Here is how you can call it to register the patient's anatomical to some reference sitting in another space (here the MNI152 template):\n",
    "```python\n",
    "flirt()\n",
    "\n",
    "```\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Pay attention! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    FLIRT expects the anatomical to be skull stripped to maximize normalization. Luckily, you already did it before with BET.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f14c2",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we can compute the transformation, using flirt.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>üí° Remember üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "To conduct coregistration, we need to remember our two key ingredients (besides the algorithm itself), namely the images:\n",
    "<ul>\n",
    "    <li>Reference image: the one which is fixed</li>\n",
    "    <li>Source image: the one which will be moved</li>\n",
    "</ul>\n",
    "<b>Our goal</b>: to align the source (moving image) to the reference (fixed image) by moving the source.\n",
    "</p></span>\n",
    "</div>\n",
    "\n",
    "In the following cell, we provide you with two images. One is the MNI152 template, the other is the anatomical T1 of the subject. On both images, the skull was removed.\n",
    "Choose which one should be a reference and which one should be a source, and run flirt accordingly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e0818a8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final result: \n",
      "0.960608 0.005215 0.007118 3.047266 \n",
      "-0.028609 1.031014 0.237479 -45.851817 \n",
      "-0.011549 -0.197684 1.137878 -36.700647 \n",
      "0.000000 0.000000 0.000000 1.000000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:03:05.646: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:03:05.646: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:03:05.646: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:03:05.646: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n"
     ]
    }
   ],
   "source": [
    "from fsl.wrappers import flirt\n",
    "\n",
    "# The two images\n",
    "subject_id = '001'\n",
    "subject_anatomical = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-001_T1w')\n",
    "mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "\n",
    "###################\n",
    "# Select which image should be source or reference\n",
    "# ANSWER:\n",
    "# The subject anatomical will most often be the source, as the template is usually where we want to map our subjects for \n",
    "# group comparison.\n",
    "# There are cases, however, where we may want the subject anatomical to be the reference. This is the case when we want to map an \n",
    "# atlas to a subject while preserving the subject as much as possible.\n",
    "# We showcase here the case where the subject is chosen as source.\n",
    "#Please note that the values in the transformation matrix of flirt refer to:\n",
    "#Top-left 3√ó3 block: rotation, scaling, shearing.\n",
    "#Last column: translation in x, y, z.\n",
    "#Bottom row: always [0 0 0 1] for affine transforms.\n",
    "##################\n",
    "source = subject_anatomical # Fill me\n",
    "reference = mni_template # Fill me\n",
    "result = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-{}_T1w_mni'.format(subject_id))\n",
    "flirt(source, reference, out=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e59c1",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, **whenever** we do a preprocessing step, we should inspect the result to assess if we did a proper job. This is called **quality control**!\n",
    "\n",
    "Visualize the result of flirt on top of the reference. What do you think of alignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "846c9c1d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(reference) \n",
    "fsleyesDisplay.load(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e3aad",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 2.3.1 Choosing a cost\n",
    "If you have a look at the options, you will notice that there is a cost option to flirt. Indeed, when performing registration, we have a function to measure how well the two images are matching one another. Flirt then attempts transformations to try and improve the fit. This function fit is defined through a cost, among different types.\n",
    "\n",
    "Which cost should we use? If you were in a pure void, there would be no right or wrong answer from the get-go. No choice but to experiment and find out!\n",
    "\n",
    "Hopefully, <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/flirt/user_guide?id=flirt\">the documentation</a> should give you some pointers. What you want here is to register a T1 to a T1: this is a <u>within</u> modality registration, so you should restrict yourself only to costs appropriate to this type of modality! \n",
    "\n",
    "To help you, we've set up a cell that will run the different coregistrations for you. Simply fill in the different costs to consider :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0077fcbe",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final result: \n",
      "1.086353 0.007973 0.011315 -9.045721 \n",
      "-0.013414 1.108506 0.296360 -62.791872 \n",
      "-0.006203 -0.239784 1.354974 -58.293611 \n",
      "0.000000 0.000000 0.000000 1.000000 \n",
      "\n",
      "\n",
      "Final result: \n",
      "0.961249 0.004764 0.006706 3.135296 \n",
      "-0.027534 1.033343 0.236322 -46.152920 \n",
      "-0.010721 -0.197353 1.140527 -37.155305 \n",
      "0.000000 0.000000 0.000000 1.000000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.661: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.661: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.661: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.661: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.662: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.662: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.662: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:04:58.662: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# Solution\n",
    "# We consider only within-modalities costs, as the two images belong to the same modality: least squares and normalized correlation ratio.\n",
    "# Remark correlation ratio also technically works\n",
    "# So FSL FLIRT offers several cost functions for image registration. Mutual information measures statistical dependence between image intensity distributions. Correlation ratio evaluates how well one image can predict another using conditional variance statistics. Normalized correlation measures linear correlation between image intensities using the Pearson correlation coefficient. Normalized mutual information provides a scale-independent version of mutual information. Least squares minimizes squared intensity differences by assuming corresponding voxels should have similar intensities. Label difference minimizes mismatched labels for discrete segmentation images. Each function makes different assumptions about the relationship between corresponding image intensities.\n",
    "#######\n",
    "possible_costs = ['mutualinfo', 'corratio', 'normcorr', 'normmi', 'leastsq', 'labeldiff']\n",
    "costs_to_consider = [ 'leastsq', 'normcorr' ] # fill me with the relevant costs\n",
    "\n",
    "for c in costs_to_consider:\n",
    "    flirt(source, reference, out=result + '_' + c, cost=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369ba8e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And let's perform lastly the QC step for each of these nice costs. We'll leave it to you to decide if you prefer any cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42d5d50e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in costs_to_consider:\n",
    "    fsleyesDisplay.load(result + '_' + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e721b6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.4 Going beyond: Non linear normalization\n",
    "\n",
    "So, you know how to do it linearly. \n",
    "What if we wanted to do it non-linearly?\n",
    "\n",
    "With FLIRT, <i>it's painfully hard</i>. To do it, you can use <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fnirt/index\">FNIRT</a>. You can browse through, the take-home message is that it is complicated, many steps are involved.\n",
    "\n",
    "But, there are other tools available, one of them being <a href=\"https://github.com/ANTsX/ANTs\">ANTs (Advanced Normalization Tools)</a>.\n",
    "For completeness, we will show you now how to use it (very succinctly) so that you know how to do it.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Pay attention! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    FNIRT does NOT expect the input data to be skull-stripped.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79b067d7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:07:23.584: GFileInfo created without standard::is-hidden\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:07:23.584: file ../gio/gfileinfo.c: line 1633 (g_file_info_get_is_hidden): should not be reached\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:07:23.584: GFileInfo created without standard::is-backup\n",
      "\n",
      "(ipykernel_launcher.py:611): GLib-GIO-CRITICAL **: 09:07:23.584: file ../gio/gfileinfo.c: line 1655 (g_file_info_get_is_backup): should not be reached\n"
     ]
    }
   ],
   "source": [
    "moving_image = ants.image_read(source + '.nii.gz')\n",
    "fixed_image = ants.image_read(reference + '.nii.gz')\n",
    "\n",
    "# Compute the transformation (non linear) to put align the moving image to the fixed image\n",
    "transformation = ants.registration(fixed=fixed_image, moving=moving_image, type_of_transform = 'SyN' )\n",
    "\n",
    "# After the transformation has been computed, apply it\n",
    "warpedImage = ants.apply_transforms(fixed=fixed_image, moving=moving_image, transformlist=transformation['fwdtransforms'])\n",
    "\n",
    "# Save the image to disk\n",
    "resultAnts = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-{}_T1w_mni_SyN.nii.gz'.format(subject_id))\n",
    "ants.image_write(warpedImage, resultAnts)\n",
    "\n",
    "# Inspect the results with FSLeyes or freeview, as you prefer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a279e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Look at the results and compare it against the linear coregistration. Which one do you prefer? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41bc79ed",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(reference)\n",
    "fsleyesDisplay.load(result)\n",
    "fsleyesDisplay.load(resultAnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f84987",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Anatomical: conclusions\n",
    "\n",
    "As a final note, all these steps (<u>including</u> non linear normalization!) can be done automatically for you with a single command: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/fsl_anat.html\">fsl_anat</a>. So you might want to use this command, instead of running all of the above when conducting preprocessing.\n",
    "\n",
    "We provide it here for convenience, but beware: it takes <b>several minutes</b> to complete, so you will need some patience!\n",
    "\n",
    "Here are the different steps you've seen today:\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Preprocessing step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues (cerebro-spinal fluid, white matter, grey matter) based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization/Coregistration</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT (if linear), ANTs/FNIRT (if not linear)</td></tr>\n",
    "</table>\n",
    "\n",
    "There are other operations which can be conducted, depending on your analysis, but the ones you've seen are <b>always</b> done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26528282-ea9e-4615-8c64-ecfb07f5a452",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.system('fsl_anat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475523a2",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def fsl_anat_wrapped(anatomical_target, output_path):\n",
    "    os.system('fsl_anat -i {} --clobber --nosubcortseg -o {}'.format(anatomical_target,output_path))\n",
    "    # Now move all files from the output_path.anat folder created by FSL to \n",
    "    # the actual output_path\n",
    "    fsl_anat_path = output_path+'.anat'\n",
    "    files_to_move = glob.glob(op.join(fsl_anat_path, '*'))\n",
    "    for f in files_to_move:\n",
    "        shutil.move(f, op.join(output_path, op.split(f)[1]))\n",
    "    \n",
    "    # Remove the output_path.anat folder\n",
    "    os.rmdir(fsl_anat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1710f",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsl_anat_wrapped(anatomical_path, op.join(preproc_root, 'sub-001', 'anat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02164173",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's inspect the resulting files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd03d4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ff2eb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "That's a lot of files! But let's worry about mostly two of them. <br>\n",
    "Notice the T1_to_MNI_lin and the T1_to_MNI_nonlin ?\n",
    "<br>In the former's case, FLIRT was run to obtain a linear normalization, whereas FNIRT was used for the latter to obtain a non linear normalization. But what difference does it make, in practice? Well, let's inspect the results, shall we?\n",
    "\n",
    "*Hint: consider the brain landmarks, such as the ventricles but also the overall shape of the brain to determine if there was a change and if so which one(s)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71f824",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm')))\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_lin'))\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
