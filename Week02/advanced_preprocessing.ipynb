{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0ae1ae-53ad-4bf8-90e9-39b6fb5f7878",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "\n",
    "Welcome to the **OPTIONAL** notebook on advanced preprocessing.\n",
    "\n",
    "It is *purely optional*. If you wish to understand most other preprocessing steps involved in fMRI pre-processing, you've come to the right place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0259ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose imports to handle paths, files etc\n",
    "import os\n",
    "import os.path as op\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Useful functions to define and import datasets from open neuro\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "\n",
    "\n",
    "def reset_overlays():\n",
    "    \"\"\"\n",
    "    Clears view and completely remove visualization. All files opened in FSLeyes are closed.\n",
    "    The view (along with any color map) is reset to the regular ortho panel.\n",
    "    \"\"\"\n",
    "    l = frame.overlayList\n",
    "    while(len(l)>0):\n",
    "        del l[0]\n",
    "    frame.removeViewPanel(frame.viewPanels[0])\n",
    "    # Put back an ortho panel in our viz for future displays\n",
    "    frame.addViewPanel(OrthoPanel)\n",
    "    \n",
    "def mkdir_no_exist(path):\n",
    "    if not op.isdir(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def direct_file_download_open_neuro(file_list, file_types, dataset_id, dataset_version, save_dirs):\n",
    "    # https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:sub-001_scans.tsv\n",
    "    for i, n in enumerate(file_list):\n",
    "        subject = n.split('_')[0]\n",
    "        download_link = 'https://openneuro.org/crn/datasets/{}/snapshots/{}/files/{}:{}:{}'.format(dataset_id, dataset_version, subject, file_types[i],n)\n",
    "        print('Attempting download from ', download_link)\n",
    "        download_url(download_link, op.join(save_dirs[i], n))\n",
    "        print('Ok')\n",
    "        \n",
    "def get_json_from_file(fname):\n",
    "    f = open(fname)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2caee9",
   "metadata": {},
   "source": [
    "### 2.1 Field map unwarping\n",
    "\n",
    "The field itself it not homogeneous, as you've seen in class. This means, in turn, that there are distortions in the acquisition.\n",
    "We can try to correct for it, through field maps, provided they've been acquired.\n",
    "\n",
    "Fortunately this is the case in our dataset - but you will need to download them as we have avoided loading them for you - on purpose!\n",
    "\n",
    "To make sure you've understood how to load datasets, here is the dataset of interest: https://openneuro.org/datasets/ds004226/versions/1.0.0\n",
    "\n",
    "<img src=\"imgs/dataset_screen.png\">\n",
    "\n",
    "Your first task is to load:\n",
    "- Subject 001 fieldmap files located in the fmap subfolder (WITH the JSON sidecars!)\n",
    "\n",
    "You are free to use either the openneuro cell or resort to the direct download option.\n",
    "Choose whichever is more convenient for you, download-time wise in particular.\n",
    "\n",
    "Out of convenience, we already provide you with the openneuro-py case. Modify the command-line run below to target *all files in the fmap subfolder of sub-01*. As the command is, it would include all files in the *anat subfolder of sub-01*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35362-85d1-42f1-adfc-69a62c8bd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fmap = 'ds004226'\n",
    "subject = '001'\n",
    "sample_path = \"dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_fmap)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "fmap_path = op.join(bids_root, 'sub-001', 'fmap')\n",
    "\n",
    "# Change the below command to include all files of the fmap subdirectory\n",
    "cmd_to_modify = \"\"\"openneuro-py download --dataset {} \n",
    "          --include sub-{}/anat/*\n",
    "          --target_dir {}\"\"\".format(dataset_fmap, subject, bids_root).replace(\"\\n\", \" \")\n",
    "\n",
    "os.system(cmd_to_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25bcf5-efdd-4daa-85ae-00daacba454b",
   "metadata": {},
   "source": [
    "Again, remember this download might not be finished immediately :)\n",
    "Now, assuming it *is*, let's have a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7228ba-fba5-4a95-b493-4ea194e1d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85a3de-2a3e-4fe7-9b4c-a63e18ddbd59",
   "metadata": {},
   "source": [
    "You can see that there are four files, corresponding to two fieldmap acquisitions. One is PA, the other is AP.\n",
    "\n",
    "We need some parameters to be able to exploit these files.\n",
    "In particular, we need to figure out:\n",
    "- The phase encoding direction\n",
    "- The total readout time\n",
    "\n",
    "Your task is to figure out which keys to exploit for this.\n",
    "Have a look at the code below (and feel free to play around a bit of course!) to setup the values properly.\n",
    "To help you, we've loaded one of the two JSON sidecars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d1a9b-0ed8-4c6d-ae2c-daac31b1724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format('AP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219030dd-f854-47e9-9cae-c7d6be72cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_fmap_path = op.join(preproc_root, 'sub-001', 'fmap')\n",
    "mkdir_no_exist(preproc_fmap_path)\n",
    "direction_file = op.join(preproc_fmap_path, 'datain.txt')\n",
    "\n",
    "f = open(direction_file, 'w')\n",
    "\n",
    "for name in ['AP', 'PA']:\n",
    "    data = get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format(name)))\n",
    "    phase_dir = data['PhaseEncodingDirection'] # Extract here the phase encoding direction !\n",
    "    total_readout_time = data['TotalReadoutTime'] # Extract here the total readout time !\n",
    "    \n",
    "    # We expect a specific format, namely x y z total_readout_time, where x,y and z are set to 1/-1 iff they are the phase\n",
    "    # encoding direction, 0 otherwise.\n",
    "    phase = [0, 0, 0, total_readout_time]\n",
    "    is_neg = len(phase_dir) == 2 and phase_dir[1] == '-'\n",
    "    phase_dir = phase_dir[0]\n",
    "    phase[ord(phase_dir)-ord('i')] = -1 if is_neg else 1\n",
    "    for i in range(3):\n",
    "        f.write('{} {} {} {}\\n'.format(phase[0], phase[1], phase[2], phase[3]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db073c57-6575-4c59-94ff-4a1ecd8c318b",
   "metadata": {},
   "source": [
    "#### Creating the field map\n",
    "Now, we will create the field map.\n",
    "This process is tedious, sometimes hard to get right. We've provided you with a function that performs all these steps. Have a look through it and apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13aa48-53c9-4fe8-bede-dc49d1ffccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fmap_AP_PA():\n",
    "    merged_phase_imgs = op.join(preproc_fmap_path, 'sub-001_acq-task_dir-fmap_merged')\n",
    "    # Combine AP and PA as single file\n",
    "    cmd = 'fslmerge -t {} {} {}'.format(merged_phase_imgs,\n",
    "                                       op.join(fmap_path, 'sub-001_acq-task_dir-AP_epi.nii.gz'),\n",
    "                                       op.join(fmap_path, 'sub-001_acq-task_dir-PA_epi.nii.gz'))\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # Now we generate the fieldmap\n",
    "    output_fmap = op.join(preproc_fmap_path, 'fieldmap_ex')\n",
    "    unwarped_img = op.join(preproc_fmap_path, 'se_epi_unwarped')\n",
    "    cmd = 'topup --imain={} --datain={} --config={} --fout={} --iout={} -v'.format(\n",
    "        merged_phase_imgs,\n",
    "        direction_file, \n",
    "        'b02b0.cnf', \n",
    "        output_fmap, \n",
    "        unwarped_img)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # Convert fmap units\n",
    "    fslmaths(output_fmap).mul(6.28).run(output_fmap + '_rads')\n",
    "    # Create magnitude fmap\n",
    "    fslmaths(unwarped_img).Tmean().run(output_fmap + '_mag')\n",
    "    \n",
    "    # Extract fmap brain using bet\n",
    "    bet(output_fmap + '_mag', output_fmap + '_mag_brain', robust=True, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f74b25-d1dd-4230-9624-b508b44a334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fmap_AP_PA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d00452-9140-4482-9796-81e9feb03699",
   "metadata": {},
   "source": [
    "In the step above, go and check all the steps. In particular, you might want to check first that the generated brain extracted fieldmap is correct and if not correct the brain mask and mask again the fieldmap, as you've done in the anatomical section.\n",
    "<br><br>\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Pay attention! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Easy? Well, not always. Field maps for this dataset came in a specific format. But they can come in *many* different ways, meaning you will need to be very careful when recovering them. The steps outlined above in particular are only applicable in the case of having an AP-PA acquisition. Here is the full resource of FSL's FUGUE on field map unwarping: <a href=https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FUGUE/Guide>https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FUGUE/Guide</a> . Don't be afraid to refer to it, should you have a different format in a project!</p>\n",
    "</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8dc6-723b-43c9-823b-3062a648acd0",
   "metadata": {},
   "source": [
    "It now remains to apply the fieldmap! \n",
    "\n",
    "To do so we will apply to the **first volume of our series to show you the result of distortion correction**.\n",
    "Your first task is thus to extract the first volume by modifying the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_trim = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz')\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "extracted_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1')\n",
    "\n",
    "# Select only the FIRST volume!\n",
    "start_vol = 0 # Where should we start? (First volume is 0, not 1 !)\n",
    "number_of_volumes = 1 # How many volumes should we keep?\n",
    "\n",
    "fslroi(file_to_trim, extracted_epi, str(start_vol), str(number_of_volumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(extracted_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093777b",
   "metadata": {},
   "source": [
    "Great! Now, Run the command below to unwarp (correct the distortions) based on the fieldmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38373a9-a9b8-4884-84c8-7d28f1aa40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_path = op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads')\n",
    "result= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1_fmap')\n",
    "\n",
    "unwarpdir='y-'\n",
    "dwell_time= data['DwellTime']\n",
    "os.system('fugue -i {} --loadfmap={} --dwell={} --unwarpdir={} -u {}'.format(extracted_epi, fmap_path, dwell_time, unwarpdir, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ff392-9223-4aac-8ee8-c13a5042fab2",
   "metadata": {},
   "source": [
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98f5f2-a3f1-4757-8bf7-3c0bd5eb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(extracted_epi_path)\n",
    "load(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199cde7-6655-4b49-a845-1524e852cd2b",
   "metadata": {},
   "source": [
    "You should observe the following two volumes:\n",
    "\n",
    "<center>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain.png\" style=\"height: 200px;width:auto;border: blue 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">Before unwarp (sagittal)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img class=\"middle-img\" src=\"imgs/corrected_brain.png\"/ style=\"height: 200px;width:auto;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (sagittal)</p>\n",
    "</div>\n",
    "<br><br>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain_2.png\" style=\"height: 270px;width:250px;border: blue 6px groove;\"/>\n",
    "    <p style=\"text-align:center;\">Before unwarp (axial)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/corrected_brain_2.png\" style=\"height: 270px;width:250px;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (axial)</p>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Do you think the fieldmap made an improvement? To drive your answer, feel free to inspect both volumes in FSLeyes. Observe the frontal and ventral regions. Do you notice anything different? Which one seems to match better what you'd expect from the brain anatomy ? \n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> Assessing quality of functional data üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    When in doubt about what you observe, don't be afraid to go have a look at your T1 to compare against. If a structure in the functional shows up in the T1 but distorted, you'll find out faster this way.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18139c8b-b600-4774-ab8f-c2741779e1fe",
   "metadata": {},
   "source": [
    "### 2.3 Slice time correction\n",
    "\n",
    "The importance of this step is still being investigated in fMRI literature. See <a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2019.00821/full\">here</a> for an in-depth analysis of its impact on the pipeline. One of the take-aways from this reference is that slice-time correction together with motion correction does improve results of fMRI analysis and does not hurt.\n",
    "Doing it before or after MC is not clear, as you can see in the reference above, so we're *choosing* here to showcase it after motion correction, but only time and further investigations will tell if there's a good order :)\n",
    "\n",
    "\n",
    "#### 2.3.1- A toy example\n",
    "\n",
    "To help you understand the underlying theory of slice-time correction, we will start from a rather unreasonable case on synthetic data, which will help you better visualize how slice-time correction affects the patterns.\n",
    "\n",
    "As you'll see, this step can only be performed if you have knowledge of the way in which slices were acquired. Most of the time, fortunately, this is easy to recover. If it is not present in your data, you can ask the scanner operator to find out which sequence was used for your data acquisition.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "First load the file named \"ground_truth.nii\" in FSLeyes to visualize it. The data is 4D, go ahead a play a bit around to see exactly what happens during the sequence! (don't forget in case of flickering to untick the \"Synchronize movie update\" option of FSLeyes).\n",
    "\n",
    "What you see is the ground truth, ie: the real phenomenon as it plays out!\n",
    "\n",
    "We now have an acquisition sequence. It performs in slices, but not in a linear order.\n",
    "Our sequence is even weirder: at a given time, not one but 9 slices are made at the same time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce97298-6217-43de-836a-d3f88b543fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run generate_smileys.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2a4d-b8be-4709-abb8-727c67adb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import InterpolatedUnivariateSpline as Interp\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_array_asnib(array, save_name):\n",
    "    img = nib.Nifti1Image(array.astype(np.uint8), np.eye(4))\n",
    "    nib.save(img, save_name)\n",
    "    \n",
    "def check_dims(axis_len, seq_len):\n",
    "    if axis_len != seq_len:\n",
    "        raise Exception('The number of slices in the sequence is different from the number of slices available in the axis. Are you sure this is the right axis?')\n",
    "\n",
    "def reslice_with_timings(slice_dir, slice_sequence,input_data, original_times):\n",
    "    assert(original_times.size == input_data.shape[3])\n",
    "    \n",
    "    n_acqs_per_tr = slice_seq.shape[0]\n",
    "    n_multibands = slice_seq.shape[1]\n",
    "    n_slices = slice_seq.size\n",
    "    \n",
    "    r= -1\n",
    "    if slice_dir=='x':\n",
    "        # For each slice in x, interpolate!\n",
    "        n = input_data.shape[0]\n",
    "        check_dims(n, n_slices)\n",
    "        r=0\n",
    "    elif slice_dir == 'y':\n",
    "        # For each slice in y, interpolate!\n",
    "        n = input_data.shape[1]\n",
    "        check_dims(n, n_slices)\n",
    "        r=1\n",
    "    elif slice_dir == 'z':\n",
    "        # For each slice in z, interpolate!\n",
    "        n = input_data.shape[2]\n",
    "        check_dims(n, n_slices)\n",
    "        r=2    \n",
    "    else:\n",
    "        # Undefined yo\n",
    "        raise Exception('Invalid dimension! Should be x, y or z')\n",
    "    # Reshape the input data to have r as first dimension!\n",
    "    input_data = np.swapaxes(input_data, 0, r)\n",
    "    \n",
    "    output_data = np.zeros(input_data.shape)\n",
    "    print(output_data.shape)\n",
    "\n",
    "    y_s = input_data.shape[1]\n",
    "    z_s = input_data.shape[2]\n",
    "    \n",
    "    # Now, on the first axis, we will iterate over slices :)\n",
    "    for b in range(0, n_acqs_per_tr):\n",
    "        time_slice = original_times + b*1./n_acqs_per_tr\n",
    "        # For all slices acquired together in the multiband\n",
    "        slices = slice_seq[b]\n",
    "        print('---------')\n",
    "        print(time_slice)\n",
    "        print(slices)\n",
    "        for s in range(0, n_multibands):\n",
    "            sl = slices[s]\n",
    "            for y in range(0, y_s):\n",
    "                for z in range(0, z_s):\n",
    "                    lin_interper = Interp(time_slice, input_data[sl, y, z, :], k=1)\n",
    "                    output_data[sl, y, z, :] = lin_interper(original_times)\n",
    "    # Now that this very inefficient method is done we should remember to swap back the axis :)\n",
    "    input_data =np.swapaxes(input_data, r, 0)\n",
    "    output_data=np.swapaxes(output_data, r, 0)\n",
    "\n",
    "    output_data[output_data < 0] = 0\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d0b5d-66c4-4f7b-ab31-615e67a8d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_seq = np.arange(0, 99).reshape((11, - 1))\n",
    "slice_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75842c17-dd90-418f-91e9-23d07b021d04",
   "metadata": {},
   "source": [
    "As you can see, these slices are acquired in a sequential order that can be called either ascending or descending depending on your convention!\n",
    "It means that the slices are obtained successively.\n",
    "\n",
    "There are different types of slicing, which depends on your sequence. Notice that in our example we acquire several slices simultaneously (for example, slices 0 up to 9 are all acquired together!). This could be some multiband acquisition, for instance, but really it is mostly to help you visualize the effect of slice timing for the exercise. In practice the slices are defined to sample the signal in the most appropriate way, so our toy sequence will likely be too crude :)\n",
    "\n",
    "In any case, we had some ground truth signal, that you can visualize in ground_truth_modulation.nii (don't forget to play the movie with the box unticked!)\n",
    "\n",
    "This signal represents the true signal that we want to acquire.\n",
    "The participant steps in an MRI, and the scanner operator uses the sequence we've defined above:\n",
    "\n",
    "```\n",
    "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "       [ 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
    "       [18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
    "       [27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    "       [36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
    "       [45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
    "       [54, 55, 56, 57, 58, 59, 60, 61, 62],\n",
    "       [63, 64, 65, 66, 67, 68, 69, 70, 71],\n",
    "       [72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
    "       [81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
    "       [90, 91, 92, 93, 94, 95, 96, 97, 98]])\n",
    "```\n",
    "\n",
    "What this really means is that we acquire 9 slices at the same time and then move on:\n",
    "(A gif to showcase what this means)\n",
    "\n",
    "Your task will be two-folds:\n",
    "1. Understand which axis the sequence was applied on, ie along which direction (X, Y or Z) was slicing performed\n",
    "2. With this simple knowledge, apply a slice-timing correction algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d929f-c505-4329-bd11-fe135eac193b",
   "metadata": {},
   "source": [
    "This cute smiley will represent our neurological data! Every time point, the behaviour is simple: the smiley's intensity increases across time!\n",
    "Here is what it looks like in FSLeyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fea7cd-aeec-47a2-ade4-561c0faa4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load('ground_truth_modulation.nii')\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea02c8d-cc35-4c9a-aa6f-af1bde00643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load('ground_truth_upsampled_modulation.nii')\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a4fd0-59ef-4fa7-a934-51d7212186eb",
   "metadata": {},
   "source": [
    "Using the following slice sequence, we have acquired our smiley signal.\n",
    "The resulting timeseries is represented in the acquired_modulation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ffe4c-33ee-40c0-9cc1-83c93a6aeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load('acquired_modulation.nii')\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735339a-c692-4663-a56f-b2f4f361c0ae",
   "metadata": {},
   "source": [
    "Oh no! What went wrong?\n",
    "\n",
    "Well, if you think about it, nothing. It is simply that acquiring every slice takes time. During this time, the signal is evolving, so we're a little late in our acquisition, which causes the drift you're seeing.\n",
    "\n",
    "This is what you observe in the acquired modulation file. The slice that is at the top - which is the last slice - also turns out to be the one with highest value. At timestep 0, it is in fact almost equal to 10 - the value of the ground truth at timestep 1 !\n",
    "\n",
    "#### Correcting the delay\n",
    "\n",
    "The heart of slice-timing correction is an interpolation in time.\n",
    "Because the timing of the slices is wrong, we account for it by interpolating back to some reference time. This is how we obtain the resliced data.\n",
    "For this, we need some informations. The first is the sequence in which slices are acquired, to know the lag of each slice. We also need the axis along which slices are acquired.\n",
    "\n",
    "Based **only on the abnormal smiley visualization in FSLeyes** and the knowledge of the sequence (that is: a bottom-up sequence with 9 simultaneous bands in every slice), can you determine along with direction the acquisition was made?\n",
    "\n",
    "- [ ] The X direction\n",
    "- [ ] The Y direction\n",
    "- [ ] The Z direction\n",
    "\n",
    "Based on your answer, in the following cell, fill in the phase_encode variable, with either 'x', 'y' or 'z' (with the quotation marks!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22149708-1e7a-4129-bdcc-398f3cce6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_encode = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d93ca-9de7-4b49-ba15-f08f3fcf21cc",
   "metadata": {},
   "source": [
    "We will now conduct slice-timing correction: the idea is simply to interpolate back the slices in time along the slice direction. Easy right? Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9035e2-8f90-4ee7-91f6-c687eb3187f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_ts = nib.load('ground_truth_modulation.nii').get_fdata()\n",
    "smile_resampled = nib.load('acquired_modulation.nii').get_fdata()\n",
    "resliced_data = reslice_with_timings(phase_encode, slice_seq, smile_resampled, np.arange(0,9))\n",
    "save_array_asnib(resliced_data.astype(np.uint8), 'resliced_data.nii')\n",
    "\n",
    "reset_overlays()\n",
    "load('resliced_data.nii')\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41757311-9e52-4659-ab2e-67c81acb06e7",
   "metadata": {},
   "source": [
    "Are you convinced on this toy example we did a not-too bad job?\n",
    "\n",
    "#### 2.3.2  Application to real data\n",
    "\n",
    "We have shown you the basic principle, but the application to real data requires some specific informations.\n",
    "You need the following ingredients:\n",
    "- When was each slice acquired in the sequence: **(Slice timing)**\n",
    "- Along which axis were the slices acquired: **Phase direction**\n",
    "- How much time we take to acquire all slices: **TR**\n",
    "\n",
    "Let's go back to our practical dataset to extract these informations. Can you find them, when looking through the JSON sidecar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a4c56-4d26-4213-a197-2edefc620f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_json_from_file(op.join(bids_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188188-bc2c-478d-b747-263a105d04e9",
   "metadata": {},
   "source": [
    "This data is actually a dictionary. We can thus extract the slice timing as an array directly from it. For example, to extract TaskName, we would use:\n",
    "```python\n",
    "data['TaskName']\n",
    "```\n",
    "\n",
    "Go ahead and extract the slice timing array, and store it in the slice_timing variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fbdbb-4565-441e-ac7e-56d21b83c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_timing = data[???] # Replace with the appropriate key (have a look above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713d62a-fdf3-4547-9cb8-fc51cc91c5db",
   "metadata": {},
   "source": [
    "Now, we might want to know where our slices are, ie along which axis, right? Typically it is along the z-direction, but we're better off if we check! Using FSLeyes, determine how many slices each axis has **for the functional data of interest**. You should thus open the relevant functional file in FSLeyes to answer this question.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>Using FSL command line</b></p>\n",
    "<p style='text-indent: 10px;'>To figure out the dimensions of an MRI image, a faster option - if you have FSL installed directly - is to run the command line command:\n",
    "    <blockquote>fslhd [your_volume]</blockquote>\n",
    "This will give you all informations contained within the header of the NIfti file. For example, running the command for our volume will easily allow us to access the slice informations:\n",
    "    <img src=\"imgs/fslhd_capture.png\"></p>\n",
    "</span>\n",
    "</div>\n",
    "Let's compare now with the amount of slices we have in our acquisition. We can consider simply the number of timings for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slice_timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659a402",
   "metadata": {},
   "source": [
    "So we have 42 slices in our slice timings, and you likely found 64 slices on the X axis, 64 axis on the Y axis and 42 slices on the Z axis. As a consequence, Z is the axis where the slices were acquired!\n",
    "Great, so we know which axis we want, we know the slice timings, but we still need to know the TR. This information is also in the JSON sidecar! Extract it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14624586",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data[???] # Extract the TR from the sidecar's appropriate field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c1429",
   "metadata": {},
   "source": [
    "To now perform the correction, we need to apply FSL's slicetimer command. For this, we need to save the timings first to their own separate file! Instead of giving the slice timings, we will provide instead the slice **order** (ie which slice was done in which order) and let FSL figure out how to best correct based on this information.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59910eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_order = np.argsort(slice_timing) + 1\n",
    "\n",
    "# Write to a file the corresponding sorted timings :)\n",
    "timing_path = op.join(preproc_root_demo,  'sub-01', 'func', 'sub-01_task-listening_run-1_slice-timings.txt')\n",
    "file = open(timing_path, mode='w')\n",
    "for t in slice_order:\n",
    "    file.write(str(t) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a80368",
   "metadata": {},
   "source": [
    "Finally we can call slicetimer from a terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d929366",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_realign = op.join(bids_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold')\n",
    "output_target = op.join(preproc_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold_slice-corr')\n",
    "cmd = 'slicetimer -i ' + file_to_realign + ' -o ' + output_target + ' -r ' + str(tr) + ' -d 3 --ocustom=' + timing_path\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188288d",
   "metadata": {},
   "source": [
    "Had we launched it on the unscrubbed data, we would really notice the impact on the first volume. <span style=\"color:red;\">Notice that you should in general **never** do this, as you would introduce lots of noise and garbage in your data.</span> Prefer to first clean all that is weird and then perform steps that might not bring a visible improvement rather than starting with data that is so bad that you can visually see changes when running above steps.\n",
    " <div class=\"row\">\n",
    "    <img src=\"imgs/slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>First volume without slice correction</center>\n",
    "    <img src=\"imgs/slice_corr.png\" style=\"width:100%\">\n",
    "       <center>First volume with slice correction: the staircase has been more or less mitigated but the result is still imperfect...</center>\n",
    "    <center>**And because of the linear interpolation, the garbage of volume 0 was spilled to volume 1!!!**</center>\n",
    "    <img src=\"imgs/vol1_slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>Second volume without slice correction</center>\n",
    "    <img src=\"imgs/vol1_slice_corr.png\" style=\"width:100%\">\n",
    "       <center>Second volume with slice correction: the result is worse than before...</center>\n",
    "</div> \n",
    "\n",
    "\n",
    "The message here is: \n",
    "- **always** perform QC between your steps\n",
    "- no algorithm can turn trash to gold. Remove the faulty volumes or you'll likely have a garbage-in garbage-out scenario. \n",
    "\n",
    "<u>Remember the 1 - 10 -100 dollar rule! It is much easier to avoid errors than compensate for them.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9079c4-8ae7-4c34-b68c-1cfdba7909d7",
   "metadata": {},
   "source": [
    "## Where are we?\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Field unwarping</td><td style='text-align:justify;'>Correction distortions induced by inhomogeneities of the b0 field through maps acquired specifically to measure this field called fieldmaps.</td><td style='text-align:justify;'>FUGUE (but also FLIRT - see below)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Slice-timing correction</td><td style='text-align:justify;'>Accounting for the difference in acquisition between the slices that make up a volume to interpolate back voxels to a fixed time reference.</td><td style='text-align:justify;'>slicetimer</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c201356",
   "metadata": {},
   "source": [
    "#### Including the fieldmap correction\n",
    "\n",
    "If you've looked a bit, you've probably noticed some frontal \"horns\" in the axial view of the EPI.\n",
    "While one could hypothesize perhaps the patient had some imp lineage, we notice that these horns are missing on the anatomical. What could explain these horns, based on all you've seen so far?\n",
    "- [ ] A rare case of astral projection\n",
    "- [ ] A distortion of the magnetic field\n",
    "\n",
    "<br> And this is where fieldmaps come into play! For these to work, we need to modify our command in the following ways:\n",
    "```python\n",
    "epi_reg(..., wmseg=path_to_wm, fmap=path_to_fmap, fmapmag=path_to_fmap_magnitude, fmapmagbrain=path_to_bet_fmap_magnitude, echospacing=effective_echo_spacing, pedir=pahse_encoding_direction)\n",
    "```\n",
    "\n",
    "All these informations have been determined in the field map part of the tutorial, fortunately! Complete the following snippet to fill in all informations and get the coregistration running with fieldmap correction! (Remember: these informations should be extracted from the **EPI**, not the fieldmaps!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_fmap = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_bbr_fmap')\n",
    "dwell_time = ???\n",
    "unwarpdir=???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_reg(ref_vol_name, \n",
    "        whole_t1, \n",
    "        skull_stripped_t1, \n",
    "        output_path_fmap, \n",
    "        wmseg=op.join(preproc_root, 'sub-001', 'anat', 'T1_fast_pve_2'),\n",
    "        fmap=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads'),\n",
    "        fmapmag=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag'),\n",
    "        fmapmagbrain=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag_brain'),\n",
    "        echospacing=dwell_time,\n",
    "        pedir=unwarpdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885ef99",
   "metadata": {},
   "source": [
    "Now let's check whether this brought any improvement at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(output_path_fmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283605d",
   "metadata": {},
   "source": [
    "What do you think? Do you think it is any better? If so why, if so why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec62f69",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "epi_reg is still using FLIRT under the hood! To be more specific, it is using flirt setup with its search cost as BBR. If you look through FLIRT's options, you'll notice that many more options are open to you:\n",
    "<img src=\"imgs/flirt_options.png\"/>\n",
    "\n",
    "Feel free to explore the effect of different search costs :) But remember: not all costs are born equal when registering images **across modalities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eedaaf",
   "metadata": {},
   "source": [
    "## Applying the transformation to all volumes\n",
    "\n",
    "You must have noticed that the coregistration has been done only on a single EPI volume, right?\n",
    "This is, as you've guessed, because all volumes are aligned with respect to each other following motion-correction. As such, it is wasteful to compute more than once the transformation from EPI to anatomical.\n",
    "\n",
    "The order of transformations we would like to have is:\n",
    "\n",
    "<img src=\"imgs/transforms.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dea5f",
   "metadata": {},
   "source": [
    "Notice that motion correction was applied by selecting a reference volume in the EPI. By default, the middle EPI was used. So, let's extract this EPI. It will be on this one that we will work, compute all transformations and reason.\n",
    "**It is critical that you pay attention to which image was used to compute your transformations, otherwise combining them won't make sense!**.\n",
    "For this reason, let's now go over the entire pipeline and transformation steps, sticking to this middle EPI. We extract it again with fslroi, as you've seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_epi = op.join(bids_root_fmap, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "middle_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_middle-vol')\n",
    "fslroi(original_epi, middle_epi, str(182), str(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869a2dd",
   "metadata": {},
   "source": [
    "Now, let's do motion correction. Recall that it is done on the **entire** EPI timeseries with mcflirt. We will explicitly give the middle epi as reference this time around, to force FSL to use this volume and realign everyone to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_moco_data = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "mcflirt(infile=original_epi,o=path_moco_data, plots=True, report=True, dof=6, mats=True, reffile=middle_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34053a6",
   "metadata": {},
   "source": [
    "Fantastic! Now, because **the reference volume did not move at all** (since it is the reference to which everyone is realigned), we can use this volume as starting point to compute our other transforms: we're only missing the coregistration with fieldmap unwarping, as the normalization is obtained through the anatomical data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_t1 = op.join(preproc_root, 'sub-001', 'anat', 'T1_biascorr')\n",
    "skull_stripped_t1 = op.join(preproc_root, 'sub-001', 'anat', 'T1_biascorr_brain')\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space')\n",
    "dwell_time = 0.000620007\n",
    "unwarpdir='y-'\n",
    "\n",
    "epi_reg(middle_epi, \n",
    "        whole_t1, \n",
    "        skull_stripped_t1, \n",
    "        output_path, \n",
    "        wmseg=op.join(preproc_root, 'sub-001', 'anat', 'T1_fast_pve_2'),\n",
    "        fmap=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads'),\n",
    "        fmapmag=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag'),\n",
    "        fmapmagbrain=op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag_brain'),\n",
    "        echospacing=dwell_time,\n",
    "        pedir=unwarpdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb3f1f",
   "metadata": {},
   "source": [
    "Beautiful! \n",
    "‚û°Ô∏è Inspect the two resulting files to ensure that nothing went wrong. In other words:\n",
    "- Check that the MCFLIRT result is okay with respect to motion\n",
    "- Check that the realignment following epi_reg made the EPI well aligned with the anatomical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664d52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0babf6c7",
   "metadata": {},
   "source": [
    "Now, if you inspect the folder you'll see we have several files. Some of them are...Well garbage left-over by FSL, but others are more precious: they are transformation files, which describe the transforms we just did.\n",
    "Namely:\n",
    "\n",
    "--\n",
    "```\n",
    "sub-001_task-sitrep_run-01_bold_bbr_fmap_warp\n",
    "```\n",
    "corresponds to the anatomical + field unwarping transformation.\n",
    "The \\_warp was added by FSL, to signal that this is a nonlinear transformation (otherwise it would be a .mat file, but here this is a NIFTI!). But what sort of transformation is it? Where does it map?\n",
    "It actually goes from EPI (after motion correction) to Anatomical space.\n",
    "To convince yourself of this fact, you can apply the transformation yourself, using the applywarp function, whose signature is as follows:\n",
    "```python\n",
    "applywarp(volume_on_which_transform_is_applied, reference_volume, warp1=transformation_to_apply)\n",
    "```\n",
    "--\n",
    "```\n",
    "sub-001_task-sitrep_run-01_bold_bbr_fmap_warp\n",
    "```\n",
    "corresponds to the motion correction transforms we had (one per each volume!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6726a1f",
   "metadata": {},
   "source": [
    "<u>Our goal</u>: now that we've computed all these transformations **for our reference volume**, we will combine them and apply all of them at every volume of the EPI!\n",
    "This way, we will perform an interpolation of our data exactly once, after the last transform and thus minimize error propagation.\n",
    "\n",
    "However we must tackle one problem: FSL provides us with a tool to apply transformations (even non linear ones such as fieldmap correction) per volume, but not in 4D.\n",
    "We will thus:\n",
    "- Split our 4D volume in individual 3D volumes\n",
    "- Apply the transformation corresponding to each volume separately\n",
    "- Combine back the volumes in a single 4D volume\n",
    "- Remove intermediary volumes\n",
    "\n",
    "The first step, to make everything clear, is to work on the transformations. Let's get started!\n",
    "\n",
    "\n",
    "### Which transformations to combine?\n",
    "    \n",
    "Answer the following question as brain teaser:\n",
    "\n",
    "- [ ] Knowing the MCFLIRT transform uses FLIRT and is done to perform motion-correction with 6 dof, is it a linear transformation represented by a matrix or a non linear transformation represented by a NIFTI volume?\n",
    "- [ ] Assuming we have a total transformation EPI -> Standard space, should our reference be the anatomical volume in standard space or the starting EPI?\n",
    "- [ ] Do we need one individual EPI (**before** moco) > Standard space transform per EPI volume?\n",
    "\n",
    "Okay, now we will combine the three transformations as a single warp. To do so, we use the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_transforms(reference_volume, warp_save_name,  is_linear, epi_2_moco=None, epi_2_anat_warp=None, anat_2_standard_warp=None):\n",
    "    \"\"\"\n",
    "    Combines transformation before motion correction all the way to standard space transformation\n",
    "    The various transformation steps are optional. As such, the final warp to compute is based on \n",
    "    which transforms are provided.\n",
    "    \"\"\"\n",
    "    args_base = {'premat': epi_2_moco, 'warp1': epi_2_anat_warp}\n",
    "    if is_linear:\n",
    "        args_base['postmat'] = anat_2_standard_warp\n",
    "    else:\n",
    "        args_base['warp2'] = anat_2_standard_warp\n",
    "    args_filtered = {k: v for k, v in args_base.items() if v is not None}\n",
    "\n",
    "    #print(args_filtered)\n",
    "    convertwarp(warp_save_name, reference_volume, **args_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1010fa2",
   "metadata": {},
   "source": [
    "Now, to apply the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6740372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(reference_volume, target_volume, output_name, transform):\n",
    "    applywarp(target_volume,reference_volume, output_name, w=transform, rel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8860d23",
   "metadata": {},
   "source": [
    "Using these two functions should not be too hard now. Notice that in combine_all_transforms, setting any transform to None instead of the correct transform will skip the transform step in the total transformation. This way, you should be able to perform quality control. In particular, please ensure that:\n",
    "- [ ] Applying ONLY motion correction transformation to the first volume yields the expected alignement (so it should be aligned with the \\_moco volume.)\n",
    "- [ ] Applying motion correction + epi -> anat should be aligned to anatomical\n",
    "- [ ] Finally, applying motion correction + epi > anat + anat > standard should be aligned to the standard\n",
    "Only once you're convinced these steps are working well should you proceed to standard space. **Remember the 1-10-100 rule! Always perform QC before moving on.**\n",
    "\n",
    "To help you, we provide you below with the template to do such a thing, so that you don't have to worry too much about the nitty gritty details.\n",
    "Focus on:\n",
    "- The reference file to use \n",
    "- The transformations to provide (either a file or None)\n",
    "\n",
    "<b>Notice this is performed only on a single volume. Indeed, if you are debugging you should avoid wasting time applying transformations on entire timeseries to quickly diagnose whether a step is working or failing.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "ref=op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin.nii.gz')\n",
    "anat_2_mni= op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin_coeff.nii.gz')\n",
    "func_2_anat= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_bbr_fmap_warp.nii.gz')\n",
    "\n",
    "# We show this one when selecting the middle EPI (volume 182)\n",
    "middle_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_middle-vol')\n",
    "split_nbr = '0182'\n",
    "\n",
    "# We will name its warp as split0182 to show you how to do this for any volume\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_split' + split_nbr + '_epi_2_std_warp')\n",
    "\n",
    "# Get the transformation matrix of this volume (this one is actually the unit matrix, \n",
    "# since this volume is the reference)\n",
    "epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "\n",
    "s0 = time.time()\n",
    "\n",
    "# -- Step 1: Combine the transformations, that is :\n",
    "#    EPI -> Motion correction -> Coregistration to anatomical -> Normalization to standard\n",
    "#    We obtain a single warp, going from EPI -> Standard space\n",
    "combine_all_transforms(ref, warp_name,  False, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni)\n",
    "s1 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "\n",
    "# -- Step 2: Apply the transformation to our EPI\n",
    "applywarp(middle_epi,ref, out_vol, w=warp_name, rel=True)\n",
    "s2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ae196",
   "metadata": {},
   "source": [
    "Above, we've timed the steps to estimate which one might be more expensive, between combining the transforms and applying them. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transform combination time:', s1 - s0)\n",
    "print('Apply transform time:', s2 - s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac047cf",
   "metadata": {},
   "source": [
    "As you can clearly see, combining the transforms is more than 100 times slower than applying the final transform. As a consequence, we would like to do this step as rarely as we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a00a9",
   "metadata": {},
   "source": [
    "#### Optimizing a bit\n",
    "\n",
    "Okay, so this step is slow. Can we make it faster? Well, yes!\n",
    "\n",
    "Note that computing all these non linear fields <u>will</u> take time. We've seen above in fact that it is <u>the</u> most expensive step.\n",
    "Now, applywarp has a neat option. It allows us to apply a transformation using a pre transformation matrix followed by the warp. Why is it cool?\n",
    "Well, remember this picture?\n",
    "\n",
    "What if we made it this way?\n",
    "\n",
    "Then, we only compute one warp once, and we can call applywarp on the volumes themselves, which will be much quicker and is **strictly** equivalent! To do this, we must combine all transforms except the motion-correction transform, which is easily done:\n",
    "```python\n",
    "combine_all_transforms(..., epi_2_moco=None,...)\n",
    "applywarp(split_vol, ref, out_vol, w=warp_name, rel=True, premat=epi_moco)\n",
    "```\n",
    "\n",
    "Let's compare the two methods, runtime wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- START OF METHOD 1 \n",
    "# In this method, we compute the transform from start to finish and apply it\n",
    "s0 = time.time()\n",
    "combine_all_transforms(ref, warp_name,  False, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni)\n",
    "s1 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1')\n",
    "applywarp(middle_epi,ref, out_vol, w=warp_name, rel=True)\n",
    "s2 = time.time()\n",
    "# ----- START OF METHOD 2\n",
    "# In this method, we compute the transform only post motion correction. We apply the motion correction and then the warp\n",
    "combine_all_transforms(ref, warp_name,  False, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni)\n",
    "s3 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2')\n",
    "applywarp(middle_epi,ref, out_vol, w=warp_name, premat=epi_moco, rel=True)\n",
    "s4 = time.time()\n",
    "\n",
    "print('Method 1 runtime:', s2 - s0, '({} for combination, {} to apply)'.format(s1 - s0, s2 - s1))\n",
    "print('Method 2 runtime:', s4 - s2, '({} for combination, {} to apply)'.format(s3 - s2, s4 - s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9c4ef",
   "metadata": {},
   "source": [
    "To convince you that the two produced images are almost identical (you might notice differences on the order of the $10^{-3}$, but consider the relative error this entails and why such an error might happen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1'))\n",
    "load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67fe1d",
   "metadata": {},
   "source": [
    "Why does it matter? Well, just applying a back-of the envelope calculation, the first method takes 40s per volume, while the second method takes 25 seconds to combine **once** the transforms excluding motion correction, and 0.4 seconds per volume to apply the transforms including motion correction. If we plot the two with an increasing number of volumes, we can see why this quickly becomes relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "x = np.arange(0, 1000, 10)\n",
    "plt.plot(x, x*40, label='Method 1')\n",
    "plt.plot(x, 25 + x*0.3, label='Method 2')\n",
    "plt.xlabel('Number of volumes')\n",
    "plt.ylabel('Runtime (seconds) [LOG SCALE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb01b1f",
   "metadata": {},
   "source": [
    "Hopefully, you're convinced that:\n",
    "- We don't lose anything using method 2 imaging-wise\n",
    "- We have a benefit in using method 2, computation-wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13b398",
   "metadata": {},
   "source": [
    "### Applying it to the entire timeseries\n",
    "\n",
    "With all this in mind, let's now apply our transformation to all our volumes! The steps are:\n",
    "\n",
    "1. Split our EPI into all individual volumes (remember: applywarp only works on a single 3D image but our EPI is 4D).\n",
    "2. Combine all transformations from EPI after motion correction all the way to standard space **once**. \n",
    "3. Use applywarp for every volume, passing the motion correction transform of this volume and the EPI > standard space warp\n",
    "4. Combine back all volumes as a single 4D EPI in standard space\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split our starting EPI volume across time \n",
    "split_target = original_epi\n",
    "split_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_split')\n",
    "cmd = 'fslsplit ' + split_target + ' ' + split_name + ' -t'\n",
    "\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's combine the different transforms EXCEPT motion correction!\n",
    "ref=op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin')\n",
    "anat_2_mni= op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin_coeff')\n",
    "func_2_anat= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space_warp')\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_epi_moco_2_std_warp')\n",
    "\n",
    "combine_all_transforms(ref, warp_name,  False, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni)\n",
    "\n",
    "# Here, we will apply transformation WITH motion correction to all our volumes\n",
    "produced_vols = []\n",
    "\n",
    "# Notice that we are sorting the volumes here! This is important, to make sure we don't get them in random order :)\n",
    "split_vols = sorted(glob.glob(op.join(preproc_root, 'sub-001', 'func', '*_bold_split*')))\n",
    "for split_vol in split_vols:\n",
    "    split_nbr = split_vol.split('_')[-1].split('.')[0].split('split')[1]\n",
    "    epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "    out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "    applywarp(split_vol,ref, out_vol, w=warp_name, premat=epi_moco, rel=True)\n",
    "    produced_vols.append(out_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f162e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std')\n",
    "tr = 1.5\n",
    "cmd = 'fslmerge -tr ' + total_epi + ' ' + ' '.join(produced_vols) + str(tr)\n",
    "os.system(cmd)\n",
    "\n",
    "# Let's not forget to remove all the temporary volumes, shall we?\n",
    "for v in split_vols + produced_vols:\n",
    "    l = glob.glob(v + '*')\n",
    "    for e in l:\n",
    "        os.remove(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73531a",
   "metadata": {},
   "source": [
    "Now, let's check we did a proper job. If we did a proper mapping, we should definitely observe the EPI positioned on the anatomical in MNI space. How well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8abf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(ref)\n",
    "load(total_epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af9b52-458d-409b-9407-80ab90976c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
