{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0005f9d3-e329-4837-918e-9df0b384489e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a8a63-df82-4cae-b9d1-f28fcec4f71b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Welcome to the laboratory computers for the course \"Neural signals and signal processing\". \n",
    "This week, we finish up the preprocessing of fMRI on some advanced points, which you might need when working on your mini-project.\n",
    "We will then look at functional Near Infrared Spectroscopy (fNRIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7844c-199f-423c-9336-eef343b02430",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ,get_json_from_file\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/Data\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe243d82-cc8d-4de7-816e-b2062e71af99",
   "metadata": {},
   "source": [
    "NOTE: Do not worry if you get the message \"Gtk-Message: 09:26:08.207: Failed to load module \"canberra-gtk-module\", the Notebook will still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff04c6-e097-450c-8ae2-033f9cabb72b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Inject custom CSS\n",
    "custom_css =\"\"\"\n",
    "<style>\n",
    "\n",
    "  .fit {\n",
    "    object-fit: cover;\n",
    "  }\n",
    "\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: center; /* Align blocks vertically in the middle */\n",
    "    justify-content: flex-start; /* Align blocks to the left */\n",
    "  }\n",
    "\n",
    "  .imageDiv {\n",
    "      background: #fff;\n",
    "      display: block;height: 150px;width: 150px;padding: 10px;border-radius: 2px;box-shadow: 0 1px 4px rgba(0, 0, 0, 0.3), 0 0 40px rgba(0, 0, 0, 0.1) inset;flex-shrink: 0;\n",
    "  }\n",
    "    .arrow-right {\n",
    "      width: 0; \n",
    "      height: 0; \n",
    "      border-top: 1em solid transparent;\n",
    "      border-bottom: 1em solid transparent;\n",
    "      border-left: 1em solid #000;\n",
    "    }\n",
    "      .col-md-10 {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "      }\n",
    "    \n",
    "    .bby {\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        height: 100vh;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    \n",
    "    .flow-container {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content:center;\n",
    "    }\n",
    "    \n",
    "    .box-text-container {\n",
    "        flex-direction: column;\n",
    "        display: center;\n",
    "        align-items: center;\n",
    "        justify-content:center;\n",
    "    }\n",
    "    \n",
    "    .step-box {\n",
    "        background-color: lightgray;\n",
    "        padding: 0.5em;\n",
    "        margin: 0 0.5em;\n",
    "        text-align: center;\n",
    "        font-weight: bold;\n",
    "        border-radius: 0.25em;\n",
    "        border: 0.15em solid black;\n",
    "        min-width: 6em;\n",
    "    }\n",
    "    \n",
    "    .arrow {\n",
    "        font-size: 1.5em;\n",
    "        color: blue;\n",
    "        font-weight: bold;\n",
    "        margin: 0 0 0.5em;\n",
    "    }\n",
    "    \n",
    "    .text {\n",
    "        font-weight: bold;\n",
    "        font-size: 0.9em;\n",
    "        margin: 0 0.5em;\n",
    "    }\n",
    "    \n",
    "    .box-wrapper {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        padding: 0.5em;\n",
    "        border: 0.15em solid black;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    \n",
    "    /* Style for the text below the box */\n",
    "    .kapt_2 {\n",
    "        margin-top: 0.5em;\n",
    "        font-size: 1em;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\n",
    "    .disp_p {\n",
    "        font-size:2.5em;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(custom_css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45f4ca-3fff-4a7f-871f-2cc778b8a304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Start FSLeyes (very neat tool to visualize MRI data of all sorts) within Python\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de61548-ef10-4f72-93f9-c252fca6aba9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 1: Advanced preprocessing (Optional)\n",
    "\n",
    "This part is a continuation of last week. It is optional, but some parts (such as \"Applying the transformation to all volumes\") might be helpful for your miniprojects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2caee9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.1 Field map unwarping\n",
    "\n",
    "The field itself is not homogeneous, as you've seen in class. This means, in turn, that there are distortions in the acquisition.\n",
    "We can try to correct for it, through field maps, provided they've been acquired.\n",
    "\n",
    "### What are field maps ? \n",
    "\n",
    "Field maps are maps of the magnetic field (hence their name). They are acquired during an experimental session to capture parts where the MRI's magnetic field might present inhomogeneities. These inhomogeneities will, in turn, cause distortions in the signal which are not part of the subject's anatomy, as well as signal drop (places where the contrast becomes very small between tissues). Such artefacts should obviously be removed.\n",
    "This is where fieldmaps are typically coming into play: by knowing how your scanner is distorting your signal, you can hope to correct for it - to some amount.\n",
    "\n",
    "The first step is - naturally - to acquire fieldmaps.\n",
    "\n",
    "Fortunately this is the case in our dataset - but you will need to download them as we have avoided loading them for you - on purpose!\n",
    "\n",
    "To make sure you've understood how to load datasets, here is the dataset of interest: https://openneuro.org/datasets/ds004226/versions/1.0.0\n",
    "\n",
    "<img src=\"imgs/dataset_screen.png\">\n",
    "\n",
    "Your first task is to load:\n",
    "- Subject 001 data files, including the fieldmap files, located in the fmap subfolder (WITH the JSON sidecars!)\n",
    "\n",
    "Out of convenience, we already provide you with the openneuro-py case. Modify the command-line run below to include *all files in the fmap subfolder of sub-01*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35362-85d1-42f1-adfc-69a62c8bd5ce",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "dataset_id = 'ds004226'\n",
    "subject = '001'\n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "fmap_path = op.join(bids_root, 'sub-001', 'fmap')\n",
    "subject_dir = 'sub-{}'.format(subject)\n",
    "\n",
    "##################\n",
    "# Solution\n",
    "# There are two solutions\n",
    "# The easiest is simply to include all data of subject-01\n",
    "# The other is to add one line for the fieldmaps\n",
    "##################\n",
    "# Change the command below to include files in the fmap subdirectory\n",
    "# You should STILL be loading the EPI and anatomical\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", op.join(subject_dir, 'anat','*'),# We are asking to get all files within the subject_dir/anat folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'func','*'),# We are asking to get all files within the subject_dir/func folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'fmap','*'),# We are asking to get all files within the subject_dir/fmap folder by using the wildcard *\n",
    "               ], check=True)\n",
    "\n",
    "# Simple variant to include everything from subject-01\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", subject_dir # Effectively get all data\n",
    "               ], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25bcf5-efdd-4daa-85ae-00daacba454b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Again, remember this download might not be finished immediately :)\n",
    "Now, assuming it *is*, let's have a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7228ba-fba5-4a95-b493-4ea194e1d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85a3de-2a3e-4fe7-9b4c-a63e18ddbd59",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Look in the fmap folder. There are four files, corresponding to two fieldmap acquisitions. One is PA, the other is AP.\n",
    "\n",
    "We need some parameters to be able to exploit these files.\n",
    "In particular, we need to figure out:\n",
    "- The phase encoding direction\n",
    "- The total readout time\n",
    "\n",
    "Your task is to figure out which keys to exploit for this.\n",
    "Have a look at the code below (and feel free to play around a bit of course!) to setup the values properly.\n",
    "To help you, we've loaded one of the two JSON sidecars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d1a9b-0ed8-4c6d-ae2c-daac31b1724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format('AP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fe3b2-614d-4734-b93d-ea85eb757350",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_no_exist(deriv_root)\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'anat'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'fmap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219030dd-f854-47e9-9cae-c7d6be72cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_fmap_path = op.join(preproc_root, 'sub-001', 'fmap')\n",
    "mkdir_no_exist(preproc_fmap_path)\n",
    "direction_file = op.join(preproc_fmap_path, 'datain.txt')\n",
    "\n",
    "f = open(direction_file, 'w')\n",
    "\n",
    "for name in ['AP', 'PA']:\n",
    "    data = get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format(name)))\n",
    "    phase_dir = data['PhaseEncodingDirection'] # Extract here the phase encoding direction !\n",
    "    total_readout_time = data['TotalReadoutTime'] # Extract here the total readout time !\n",
    "    \n",
    "    # We expect a specific format, namely x y z total_readout_time, where x,y and z are set to 1/-1 if and only if they are the phase\n",
    "    # encoding direction, 0 otherwise.\n",
    "    phase = [0, 0, 0, total_readout_time]\n",
    "    is_neg = len(phase_dir) == 2 and phase_dir[1] == '-'\n",
    "    phase_dir = phase_dir[0]\n",
    "    phase[ord(phase_dir)-ord('i')] = -1 if is_neg else 1\n",
    "    for i in range(3):\n",
    "        f.write('{} {} {} {}\\n'.format(phase[0], phase[1], phase[2], phase[3]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db073c57-6575-4c59-94ff-4a1ecd8c318b",
   "metadata": {},
   "source": [
    "### 1.1.1 Creating the field map\n",
    "Now, we will create the field map.\n",
    "This process is tedious, sometimes hard to get right. First, let's look at the two fieldmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c19424-b0fa-4902-8f42-64cd4d15d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-001_acq-task_dir-AP_epi.nii.gz'))\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-001_acq-task_dir-PA_epi.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545c42-1d20-484c-abb8-5c45995bcd74",
   "metadata": {},
   "source": [
    "As you notice, they are quite different with respect to their distortions. This is because they used different phase encoding directions (Anterior -> Posterior and Posterior -> Anterior, hence AP and PA).\n",
    "\n",
    "Looking from these two encoding directions, we (or rather clever algorithm: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/topup.html\">topup</a>) can build a complete map of the distortions on our EPI / fMRI file.\n",
    "\n",
    "Here is the signature of topup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fa0fe-d090-4b23-bf76-c9453b16bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"topup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7427f-7a17-48d6-969e-17446c7e5b48",
   "metadata": {},
   "source": [
    "Not very clear, is it? You need to, in fact, conduct several steps. \n",
    "\n",
    "- First, AP and PA fieldmaps should be made a single file to be fed to topup.\n",
    "- You also need to feed it the direction file we created above (which specifies encoding direction and readout time of our EPI).\n",
    "- From the computed field, we need to convert it to radians, and finally, we obtain both a phase information for the field - in radians - and a magnitude information.\n",
    "\n",
    "You are now ready to apply the fieldmap to correct distortions.\n",
    "\n",
    "The function below conducts all these steps for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13aa48-53c9-4fe8-bede-dc49d1ffccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fmap_AP_PA(direction_file):\n",
    "    \"\"\"\n",
    "    From an AP/PA pair of files, generate the corresponding fieldmap files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    direction_file: str\n",
    "        Path to a direction file (typically called datain.txt) indicating phase encoding direction, total \n",
    "        readout time and other relevant parameters for fmap computation\n",
    "    \"\"\"\n",
    "    merged_phase_imgs = op.join(preproc_fmap_path, 'sub-001_acq-task_dir-fmap_merged')\n",
    "    \n",
    "    # Step 1: Combine AP and PA as single file\n",
    "    subprocess.run(['fslmerge', '-t', merged_phase_imgs, \n",
    "                    op.join(fmap_path, 'sub-001_acq-task_dir-AP_epi.nii.gz'), \n",
    "                    op.join(fmap_path, 'sub-001_acq-task_dir-PA_epi.nii.gz')])\n",
    "    \n",
    "    # Step 2: Compute the fieldmap deformation with topup \n",
    "    # In this particular step, we feed in the direction file (ie the phase encoding direction, phase order etc, which we've saved above as direction file\n",
    "    output_fmap = op.join(preproc_fmap_path, 'fieldmap_ex')\n",
    "    unwarped_img = op.join(preproc_fmap_path, 'se_epi_unwarped')\n",
    "\n",
    "    subprocess.run(['topup', \n",
    "                    '--imain={}'.format(merged_phase_imgs), \n",
    "                    '--datain={}'.format(direction_file),\n",
    "                   '--config={}'.format('b02b0.cnf'),\n",
    "                   '--fout={}'.format(output_fmap),\n",
    "                   '--iout={}'.format(unwarped_img),\n",
    "                   '-v'])\n",
    "    \n",
    "    # Step 3: Convert fmap units to rads\n",
    "    subprocess.run(['fslmaths', output_fmap, '-mul', str(6.28), output_fmap + '_rads'])\n",
    "    #fslmaths(output_fmap).mul(6.28).run(output_fmap + '_rads')\n",
    "    \n",
    "    # Step 4: Create magnitude fmap\n",
    "    subprocess.run(['fslmaths', unwarped_img, '-Tmean', output_fmap + '_mag'])\n",
    "    #fslmaths(unwarped_img).Tmean().run(output_fmap + '_mag')\n",
    "    \n",
    "    # Extract fmap brain using bet\n",
    "    subprocess.run(['bet', output_fmap + '_mag',output_fmap + '_mag_brain', '-m', '-R'])\n",
    "generate_fmap_AP_PA(direction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8dc6-723b-43c9-823b-3062a648acd0",
   "metadata": {},
   "source": [
    "It now remains to apply the fieldmap! \n",
    "\n",
    "To do so we will apply to the **first volume of our series to show you the result of distortion correction**.\n",
    "Your first task is thus to extract the first volume by modifying the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_trim = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz')\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "extracted_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1')\n",
    "\n",
    "###########\n",
    "# Solution\n",
    "# Extracting the first volume means we keep a single volume. \n",
    "# This is exactly as we did last week for the reference EPI used in epi_reg,\n",
    "# but we take the volume number 0 instead of the middle one.\n",
    "###########\n",
    "\n",
    "# Select only the FIRST volume!\n",
    "start_vol = 0 # Where should we start? (First volume is 0, not 1 !)\n",
    "number_of_volumes = 1 # How many volumes should we keep?\n",
    "\n",
    "fslroi(file_to_trim, extracted_epi, str(start_vol), str(number_of_volumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(extracted_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093777b",
   "metadata": {},
   "source": [
    "Great! \n",
    "We are finally able to apply our fieldmap to the EPI. We can do so, using FUGUE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38373a9-a9b8-4884-84c8-7d28f1aa40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_path_rad = op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads')\n",
    "epi_result= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1_fmap')\n",
    "\n",
    "unwarpdir='y-'\n",
    "dwell_time= data['EffectiveEchoSpacing'] # Although FUGUE wants the dwell time, the reported dwell time in the header is wrong: it should be in order of milliseconds (0.3 to 60ms is reasonable), but here it is on the order of microseconds. We use effective echo spacing instead, which is the same thing...And in correct order of magnitude.\n",
    "\n",
    "subprocess.run(['fugue', '-i', extracted_epi, # Fieldmaps are applied to EPI\n",
    "                '--loadfmap={}'.format(fmap_path_rad), # We used the fmap we just created\n",
    "               '--dwell={}'.format(dwell_time), # The dwell time is necessary as input parameter\n",
    "               '--unwarpdir={}'.format(unwarpdir), # The unwarp direction\n",
    "               '-u', epi_result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ff392-9223-4aac-8ee8-c13a5042fab2",
   "metadata": {},
   "source": [
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98f5f2-a3f1-4757-8bf7-3c0bd5eb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(epi_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b876e-a801-4971-8b16-efe7fccc0311",
   "metadata": {},
   "source": [
    "You should observe the following two volumes:\n",
    "\n",
    "<center>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain.png\" style=\"height: 200px;width:auto;border: blue 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">Before unwarp (sagittal)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img class=\"middle-img\" src=\"imgs/corrected_brain.png\"/ style=\"height: 200px;width:auto;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (sagittal)</p>\n",
    "</div>\n",
    "<br><br>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain_2.png\" style=\"height: 270px;width:250px;border: blue 6px groove;\"/>\n",
    "    <p style=\"text-align:center;\">Before unwarp (axial)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/corrected_brain_2.png\" style=\"height: 270px;width:250px;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (axial)</p>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287d605-d9ec-405b-89f1-d064c2a432c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199cde7-6655-4b49-a845-1524e852cd2b",
   "metadata": {},
   "source": [
    "Do you think the fieldmap made an improvement? To drive your answer, feel free to inspect both volumes in FSLeyes. Observe the frontal and ventral regions. Do you notice anything different? Which one seems to match better what you'd expect from the brain anatomy ? \n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> Assessing quality of functional data üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    When in doubt about what you observe, don't be afraid to go have a look at your T1 to compare against. If a structure in the functional shows up in the T1 but distorted, you'll find out faster this way.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d00452-9140-4482-9796-81e9feb03699",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Pay attention! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Easy? Well, not always. Field maps for this dataset came in a specific format. But they can come in <b>many</b> different ways, meaning you will need to be very careful when recovering them. The steps outlined above in particular are only applicable in the case of having an AP-PA acquisition. Here is the full resource of FSL's FUGUE on field map unwarping: <a href=https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue>https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue</a> . Don't be afraid to refer to it, should you have a different format in a project!</p>\n",
    "</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eedaaf",
   "metadata": {},
   "source": [
    "## 1.2 Combining transforms\n",
    "\n",
    "We know how to perform motion-correction, and how to coregister an image to another. That's great!\n",
    "But each transformation usually implies a step of interpolation (because the image is transformed and must be resampled). This interpolation means the resulting data is \"corrupted\" slightly. We would like to minimize the amount of interpolations to only once if possible.\n",
    "\n",
    "Here is a reminder of the different spaces we've considered, as well as the different operations and how they move between spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de1af2-539c-4a81-884f-b445c44790f0",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/epi_transform_steps.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766aaea-a775-4b18-a0a7-ea24ba690581",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50fc54-80c6-4aa6-9605-5fae3fa539b5",
   "metadata": {},
   "source": [
    "The order of transformations we would like to have is:\n",
    "\n",
    "<body>\n",
    "    <div class=\"flow-container\">\n",
    "        <div class=\"text\">EPI</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"step-box\">Motion correction<br>Mcflirt</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"text\">EPI<br> (motion corrected)</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"step-box\">Field unwarping + affine coregistration<br>epi_reg with fieldmap</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"text\">EPI<br> (Anatomical space)</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"step-box\">Normalization<br>Flirt or ANTs</div>\n",
    "        <div class=\"arrow\">‚Üí</div>\n",
    "        <div class=\"text\">EPI<br> (template space)</div>\n",
    "    </div>\n",
    "\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dea5f",
   "metadata": {},
   "source": [
    "### 1.2.1 Combining fieldmap unwarping and EPI registration\n",
    "\n",
    "FSL provides a way to compute the EPI to anatomical while combining it with the fieldmap unwarping. We will show you in this part how to do it.\n",
    "\n",
    "But before applying all transforms, let's worry about doing the required preprocessing, namely:\n",
    "- Motion correction\n",
    "- Field unwarping\n",
    "- EPI to anatomical coregistration\n",
    "- Anatomical to template coregistration (Normalization): we will use the MNI152 1mm template that you know from the previous weeks\n",
    "\n",
    "\n",
    "First, remark that motion correction is done by selecting a reference volume in the EPI to which all others are coregistered. By default, the middle EPI was used. \n",
    "\n",
    "Because we used in our fieldmap computation the first EPI, we need to use this one instead.\n",
    "\n",
    "**It is critical that you pay attention to which image was used to compute your transformations, otherwise combining them won't make sense!**.\n",
    "\n",
    "For this reason, let's now go over the entire pipeline and transformation steps, sticking to the first EPI. We extract it again with fslroi, and we re-run the motion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_epi = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "reference_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869a2dd",
   "metadata": {},
   "source": [
    "Now, let's do motion correction. Recall that it is done on the **entire** EPI timeseries with mcflirt. We will explicitly give the first epi as reference this time around, to force FSL to use this volume and realign everyone to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "path_moco_data = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "mcflirt(infile=original_epi,o=path_moco_data, plots=True, report=True, dof=6, mats=True, reffile=reference_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34053a6",
   "metadata": {},
   "source": [
    "Fantastic! Now, because **the reference volume did not move at all** (since it is the reference to which everyone is realigned), we can use this volume as starting point to compute our other transforms: we're only missing the coregistration with fieldmap unwarping, as the normalization is obtained through the anatomical data :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = '001'\n",
    "subject='sub-001'\n",
    "\n",
    "# Relevant paths for anatomical preprocessing\n",
    "anatomical_path = op.join(bids_root, subject, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id))\n",
    "betted_brain_path = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w'.format(subject_id))\n",
    "segmentation_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast')\n",
    "\n",
    "mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "anat_result = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_mni'.format(subject_id))\n",
    "anat_2_mni_trans = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_2_mni_lin.mat'.format(subject_id))\n",
    "\n",
    "# Relevant variables for epi_reg\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space')\n",
    "dwell_time = 0.000620007\n",
    "unwarpdir='y-'\n",
    "\n",
    "##############\n",
    "# Put here all steps which you might need to conduct before you can do epi_reg\n",
    "# Hint: consider the anatomical preprocessing steps, look at week 2 if you forgot!\n",
    "# For flirt, have a look at the omat argument to save the transform\n",
    "##############\n",
    "\n",
    "##############\n",
    "# Solution\n",
    "# We need to do the brain extraction, using bet as well as the segmentation, using fast, lastly we perform normalization to MNI152 space\n",
    "##############\n",
    "from fsl.wrappers import fast\n",
    "print('running bet')\n",
    "subprocess.run(['bet', anatomical_path, betted_brain_path, '-m', '-R'])\n",
    "\n",
    "print('running fast')\n",
    "fast(imgs=[betted_brain_path], out=segmentation_path, n_classes=3)\n",
    "\n",
    "print('running flirt')\n",
    "flirt(betted_brain_path, mni_template, out=anat_result, omat = anat_2_mni_trans)\n",
    "\n",
    "#############\n",
    "# Launching epi_reg with fieldmap unwarping.\n",
    "# Careful to do it ON THE FIELDMAP CORRECTED VOLUME\n",
    "# Note epi_reg will take a few minutes to compute the transform - feel free to bombard us with questions (or candy)\n",
    "#############\n",
    "reference_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))\n",
    "subprocess.run(['epi_reg','--epi={}'.format(reference_epi), \n",
    "                '--t1={}'.format(anatomical_path), \n",
    "                '--t1brain={}'.format(betted_brain_path), \n",
    "                '--out={}'.format(output_path),\n",
    "                '--fmap={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads')),\n",
    "                '--fmapmagbrain={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag_brain')),\n",
    "                '--fmapmag={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag')),\n",
    "                '--wmseg={}'.format(op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast_pve_2')),\n",
    "                '--echospacing={}'.format(dwell_time),\n",
    "                '--pedir={}'.format(unwarpdir)])\n",
    "\n",
    "print(\"Done with EPI to anatomical registration with fieldmap unwarping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb3f1f",
   "metadata": {},
   "source": [
    "Beautiful! \n",
    "\n",
    "‚û°Ô∏è Inspect the two resulting files to ensure that nothing went wrong. In other words:\n",
    "- Check that the MCFLIRT result is okay with respect to motion\n",
    "- Check that the realignment following epi_reg made the EPI well aligned with the anatomical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa418-1fbe-4956-bfb5-808d3d601a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect here with fsleyes, such as fsleyesDisplay.load(yourEpi) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6197ec6-2059-48cc-9805-1da6a1e81c07",
   "metadata": {},
   "source": [
    "### 1.2.2 Getting the saved transformation\n",
    "\n",
    "Applying the transformation to a single volume is nice, but we should still need to know where the transformation was saved, to apply it to all other volumes of interest.\n",
    "\n",
    "Let's inspect our resulting folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e1820-c0d1-4833-aa90-0fce590f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babf6c7",
   "metadata": {},
   "source": [
    "The file <b>sub-001_task-sitrep_run-01_bold_anat-space_warp</b> corresponds to the <u>transformation</u> from EPI to anatomical file with the fieldmap unwarping applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287459ce-5f1f-4c43-89c4-a838790fb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6726a1f",
   "metadata": {},
   "source": [
    "### 1.2.3 Combining the transforms\n",
    "\n",
    "Let's list our available transforms as is:\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Transform</th>\n",
    "    <th>Step</th>\n",
    "    <th>File(s)</th>\n",
    "    <th>Linear?</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Each EPI to reference EPI</td>\n",
    "    <td>Motion correction (mcflirt) </td>\n",
    "    <td>matrices in func/sub-001_task-sitrep_run-01_bold_moco.mat/</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>EPI to anat (with fieldmap)</td>\n",
    "    <td>Functional to anat coregistration (epi_reg) </td>\n",
    "    <td>func/sub-001_task-sitrep_run-01_bold_anat-space_warp.nii.gz</td>\n",
    "    <td>No</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Anat to template</td>\n",
    "    <td>Normalization (FLIRT or FNIRT or ANTs)</td>\n",
    "    <td>anat/.mat</td>\n",
    "    <td>Yes if FLIRT or ANTs with linear transform, no otherwise</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "We will now combine all these transformations, so that we apply one transformation and exactly one interpolation at the end to minimize errors.\n",
    "\n",
    "We provide you with a function to do so just below, along with a function to apply the resulting combined transformation. Beware, all these functions operate on single volumes, not on 4D data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_transforms(reference_volume, warp_save_name,  is_linear, epi_2_moco=None, epi_2_anat_warp=None, anat_2_standard_warp=None):\n",
    "    \"\"\"\n",
    "    Combines transformation BEFORE motion correction all the way to standard space transformation\n",
    "    The various transformation steps are optional. As such, the final warp to compute is based on \n",
    "    which transforms are provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        Reference volume. The end volume after all transformations have been applied, relevant for final resolution and field of view.\n",
    "    warp_save_name: str\n",
    "        Under which name to save the total warp\n",
    "    is_linear: bool\n",
    "        Whether the transformation is linear or non linear.\n",
    "    epi_2_moco: str\n",
    "        Transformation of the EPI volume to motion-correct it (located in the .mat/ folder of the EPI\n",
    "    epi_2_anat_warp: str\n",
    "        Transformation of the EPI volume to the anatomical space, typically obtained by epi_reg. Assumed to include fieldmap correction and thus be non-linear.\n",
    "    anat_2_standard_warp: str\n",
    "        Transformation of the anatomical volume to standard space, such as the MNI152 space. Might be linear or non linear, which affects is_linear value accordingly.\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import convertwarp\n",
    "    args_base = {'premat': epi_2_moco, 'warp1': epi_2_anat_warp}\n",
    "    if is_linear:\n",
    "        args_base['postmat'] = anat_2_standard_warp\n",
    "    else:\n",
    "        args_base['warp2'] = anat_2_standard_warp\n",
    "    args_filtered = {k: v for k, v in args_base.items() if v is not None}\n",
    "\n",
    "    convertwarp(warp_save_name, reference_volume, **args_filtered)\n",
    "    print(\"Done with warp conversion\")\n",
    "\n",
    "def apply_transform(reference_volume, target_volume, output_name, transform):\n",
    "    \"\"\"\n",
    "    Applies a warp field to a target volume and resamples to the space of the reference volume.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        The reference volume for the final interpolation, resampling and POV setting\n",
    "    target_volume: str\n",
    "        The target volume to which the warp should be applied\n",
    "    output_name: str\n",
    "        The filename under which to save the new transformed image\n",
    "    transform: str\n",
    "        The filename of the warp (assumed to be a .nii.gz file)\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    combine_all_transforms to see how to build a warp field\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import applywarp\n",
    "    applywarp(target_volume,reference_volume, output_name, w=transform, rel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8860d23",
   "metadata": {},
   "source": [
    "Using these two functions should not be too hard now. Notice that in combine_all_transforms, setting any transform to None instead of the correct transform will skip the transform step in the total transformation. This way, you should be able to perform quality control. In particular, please ensure that:\n",
    "- [ ] Applying ONLY motion correction transformation to the first volume yields the expected alignement (so it should be aligned with the \\_moco volume.)\n",
    "- [ ] Applying motion correction + epi -> anat should be aligned to anatomical\n",
    "- [ ] Finally, applying motion correction + epi > anat + anat > standard should be aligned to the standard\n",
    "Only once you're convinced these steps are working well should you proceed to standard space. **Remember the 1-10-100 rule! Always perform QC before moving on.**\n",
    "\n",
    "To help you, we provide you below with the template to do such a thing, so that you don't have to worry too much about the nitty gritty details.\n",
    "Focus on:\n",
    "- The reference file to use \n",
    "- The transformations to provide (either a file or None)\n",
    "\n",
    "<b>Notice this is performed only on a single volume. Indeed, if you are debugging you should avoid wasting time applying transformations on entire timeseries to quickly diagnose whether a step is working or failing.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fsl.wrappers import applywarp\n",
    "ref=mni_template\n",
    "\n",
    "# We show this one when selecting the first EPI (volume 0000)\n",
    "target_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "split_nbr = '0000'\n",
    "\n",
    "# We will name its warp as split0000\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_split' + split_nbr + '_epi_2_std_warp')\n",
    "\n",
    "# Get the transformation matrix of this volume (this one is actually the unit matrix, \n",
    "# since this volume is the reference)\n",
    "\n",
    "# -- Step 1: Combine the transformations, that is :\n",
    "#    EPI -> Motion correction -> Coregistration to anatomical -> Normalization to standard\n",
    "#    EPI -> Motion correction is given by the matrix in sub-001_task-sitrep_run-01_bold_moco.mat/MAT_{vol_nbr}, where {vol_nbr} is the volume number of the volume of interest\n",
    "#    EPI -> Coreg to anatomical, this is the _warp.nii.gz file in func/ folder\n",
    "#    Anatomical > Template is saved by flirt when doing the anatomical to template coregistration, in anat/ folder\n",
    "func_2_anat= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space_warp.nii.gz')\n",
    "epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "\n",
    "s0 = time.time()\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s1 = time.time()\n",
    "\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "\n",
    "# -- Step 2: Apply the transformation to our EPI\n",
    "applywarp(target_epi,ref, out_vol, w=warp_name, rel=False)\n",
    "s2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ae196",
   "metadata": {},
   "source": [
    "Above, we've timed the steps to estimate which one might be more expensive, between combining the transforms and applying them. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transform combination time:', s1 - s0)\n",
    "print('Apply transform time:', s2 - s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac047cf",
   "metadata": {},
   "source": [
    "As you can clearly see, combining the transforms is more than 6 times slower than applying the final transform. As a consequence, we would like to do this step as rarely as we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a00a9",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 Optimizing a bit\n",
    "\n",
    "Okay, so this step is slow. Can we make it faster? Well, yes!\n",
    "\n",
    "Note that computing all these non linear fields <u>will</u> take time. We've seen above in fact that it is <u>the</u> most expensive step.\n",
    "Now, applywarp has a neat option. It allows us to apply a transformation using a pre transformation matrix followed by the warp. Why is it cool?\n",
    "Well, consider the following steps:\n",
    "\n",
    "<center><img src=\"imgs/space_steps.png\"></center>\n",
    "\n",
    "<br>\n",
    "Let's group transforms in two potential ways:\n",
    "\n",
    "<center><img src=\"imgs/two_ways_grouping.png\" style=\"max-width:1200px;\"></center>\n",
    "In other words, we consider <b>grouping up the transformations to apply them</b>. As you will see, computing transformations can take time when they are non linear, whereas applying them is comparatively fast. We will investigate both grouping all transforms together or grouping all transforms which follow motion correction together.\n",
    "\n",
    "Let's compare the two methods, runtime wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- START OF METHOD 1 \n",
    "# In this method, we compute the transform from start to finish and apply it\n",
    "s0 = time.time()\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s1 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1')\n",
    "#applywarp(target_epi,ref, out_vol, w=warp_name, rel=False)\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs'])\n",
    "s2 = time.time()\n",
    "# ----- START OF METHOD 2\n",
    "# In this method, we compute the transform only post motion correction. We apply the motion correction and then the warp\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s3 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2')\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)])\n",
    "s4 = time.time()\n",
    "\n",
    "print('Method 1 runtime:', s2 - s0, '({} for combination, {} to apply)'.format(s1 - s0, s2 - s1))\n",
    "print('Method 2 runtime:', s4 - s2, '({} for combination, {} to apply)'.format(s3 - s2, s4 - s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9c4ef",
   "metadata": {},
   "source": [
    "To convince you that the two produced images are almost identical (you might notice differences on the order of the $10^{-3}$, but consider the relative error this entails and why such an error might happen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1'))\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67fe1d",
   "metadata": {},
   "source": [
    "Why does it matter? Well, just applying a back-of the envelope calculation, the first method takes 122s per volume, while the second method takes 87 seconds to combine **once** the transforms excluding motion correction, and 4 seconds per volume to apply the transforms including motion correction. If we plot the two with an increasing number of volumes, we can see why this quickly becomes relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 1000, 10)\n",
    "plt.plot(x, x*122, label='Method 1')\n",
    "plt.plot(x, 87 + x*4, label='Method 2')\n",
    "plt.xlabel('Number of volumes')\n",
    "plt.ylabel('Runtime (seconds) [LOG SCALE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb01b1f",
   "metadata": {},
   "source": [
    "Hopefully, you're convinced that:\n",
    "- We don't lose anything using method 2 imaging-wise\n",
    "- We have a benefit in using method 2, computation-wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13b398",
   "metadata": {},
   "source": [
    "### 1.2.4 Applying the transformation to the entire timeseries at last\n",
    "\n",
    "With all this in mind, let's now apply our transformation to all our volumes! The steps are:\n",
    "\n",
    "1. Split our EPI into all individual volumes (remember: applywarp only works on a single 3D image but our EPI is 4D).\n",
    "2. Combine all transformations from EPI after motion correction all the way to standard space **once**. \n",
    "3. Use applywarp for every volume, passing the motion correction transform of this volume and the EPI > standard space warp\n",
    "4. Combine back all volumes as a single 4D EPI in standard space\n",
    "\n",
    "Let's make sure you understand why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab3867-3c50-4c0a-b02b-956d43b434fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split our starting EPI volume across time \n",
    "split_target = original_epi\n",
    "split_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_split')\n",
    "\n",
    "subprocess.run(['fslsplit', split_target, split_name, '-t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a9109-c717-4036-8700-5c4078e5cf5f",
   "metadata": {},
   "source": [
    "Let's have a look at our folder structure now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da7bff-27b1-4879-bc99-69e524e10b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0b151-739c-4f43-9aa8-7c4553c173e9",
   "metadata": {},
   "source": [
    "As you can see, a lot of new volumes have appeared. These are the split, individual volumes of our EPI.\n",
    "\n",
    "Great, let's now combine the different transforms EXCEPT motion correction, with method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f4766-1b6f-4ec5-8077-2bb98176bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Let's combine the different transforms EXCEPT motion correction!\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_epi_moco_2_std_warp')\n",
    "\n",
    "print(\"Starting to combine transforms...\")\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "print(\"Done, moving on to application of transforms...\")\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "# Now apply transformation to all our volumes.\n",
    "# We will remember the volumes as well, to group them back afterwards.\n",
    "##########\n",
    "\n",
    "# Notice that we are sorting the volumes here! This is important, to make sure we don't get them in random order :)\n",
    "split_vols = sorted(glob.glob(op.join(preproc_root, 'sub-001', 'func', '*_bold_split*')))\n",
    "\n",
    "\n",
    "# Define a function that wraps subprocess.run()\n",
    "def run_subprocess(split_vol, vol_nbr):\n",
    "    \"\"\"\n",
    "    SAFETY GOGGLES ON\n",
    "    This function launches applywarp in parallel to reach complete result quicker\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    split_vol: str\n",
    "        Path to the volume on which to apply the transformation\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact, since parallelisation does not honour order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vol: str\n",
    "        Path to the transformed volume\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        split_nbr = split_vol.split('_')[-1].split('.')[0].split('split')[1]\n",
    "        epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "        out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "        result = subprocess.run(['applywarp', '-i', split_vol, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)], check=True)\n",
    "        return out_vol, vol_nbr\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"applywarp for volume '{split_vol}' failed with error: {e.stderr.decode('utf-8')}\"\n",
    "\n",
    "\n",
    "produced_vols = [None]*len(split_vols)\n",
    "# Initialize ThreadPoolExecutor and the progress bar\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use tqdm to wrap the futures\n",
    "    with tqdm(total=len(split_vols)) as progress:\n",
    "        # Launch subprocesses in parallel\n",
    "        futures = {executor.submit(run_subprocess, vol,i): vol for i,vol in enumerate(split_vols)}\n",
    "\n",
    "        # Process completed tasks\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            out_vol, vol_nbr = future.result()  # Get the result of the subprocess\n",
    "            produced_vols[vol_nbr] = out_vol\n",
    "            # Update the progress bar for each completed task\n",
    "            progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2672185-f2ba-43a0-b7f4-bd027fd0e7cb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Great, now we finally have our volumes!\n",
    "We can finally analyze them, as you will see next week!\n",
    "\n",
    "### 1.2.5 Grouping the volumes together (Optional)\n",
    "\n",
    "<b>This section is purely optional, you can readily skip it if you want.</b>\n",
    "\n",
    "Our volumes are now individually separated, but we started with a 4D volume...\n",
    "Perhaps we'd like to have them back as an individual volume, to visualize them better?\n",
    "The part below aims precisely at doing that for you.\n",
    "\n",
    "In theory we should be able to group everything back. The issue is, our fMRI is now interpolated at 1 x 1 x 1 mm3 resolution, so it won't fit in RAM on this virtual machine, where a high performance computing cluster would do this without any issue.\n",
    "\n",
    "There are two answers to this issue. The first, used by fMRIprep, is to play it smart and actually never modify the fMRI's resolution.\n",
    "\n",
    "The second is to use to group back the files a memory map, that is to say a file on disk from which we only read the parts we need (VERY useful when you do not have enough RAM). Because writing to disk is very slow, we do it in a batch approach below.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C04000; color: #112A46; border-left: solid #C04000 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>‚ö†Ô∏è  This part is TIME INTENSIVE and it is OPTIONAL ‚ö†Ô∏è </b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    This part will take TIME (about an hour, in fact)\n",
    "It is probably better if you jump ahead to another part of the notebook and start reading it while this is running, or just do not run it at all and remember this in case you absolutely need it later. This hour is probably better utilized somewhere else, right ? ;)</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef73514-3357-4012-99a5-a76c779878ad",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import progressbar\n",
    "\n",
    "first_vol = nib.load(produced_vols[0])\n",
    "v_shape = first_vol.get_fdata().shape\n",
    "\n",
    "preproc_root = '/home/jovyan/Data/dataset/ds004226/derivatives/preprocessed_data'\n",
    "filename = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std.dat')\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='w+', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "batch_size = len(produced_vols)//4\n",
    "\n",
    "A = np.zeros((v_shape[0],v_shape[1],v_shape[2], batch_size))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=len(produced_vols)) as bar:\n",
    "    for batch_i in range(4):\n",
    "        print('Starting for batch {}/4'.format(batch_i+1))\n",
    "        start_batch = batch_size * batch_i\n",
    "        end_batch = min(batch_size * (batch_i+1),len(produced_vols))\n",
    "        max_len = end_batch - start_batch + 1\n",
    "        for i in range(start_batch, end_batch):\n",
    "            vol = nib.load(produced_vols[i])\n",
    "            A[:,:,:,i-start_batch] = vol.get_fdata()\n",
    "            bar.update(i)\n",
    "        large_array[:,:,:, start_batch:end_batch] = A[:,:,:,:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76578c-c971-4690-bedc-1479862f4850",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, save to Nifti file using Nibabel\n",
    "\n",
    "# Step 1: Ensure all changes of the memmap are flushed to disk and then close it\n",
    "#large_array.flush()\n",
    "#del large_array\n",
    "print(\"Done flushing mmap\")\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='r', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "# Step 2: Modify the header to indicate that we have 4D data, and specify its TR.\n",
    "header = first_vol.header.copy()  # Copy the header of the first volume (to get right resolution, affine, Q-form etc)\n",
    "header['dim'][0] = 4  # Specifies that this is a 4D dataset\n",
    "header['dim'][1:5] = large_array.shape  # Update dimensions (x, y, z, t)\n",
    "header['pixdim'][4] = 1.5  # Set the TR in the 4th dimension. You can see the TR of the data by looking at your original EPI series with fslhd, remember ;)\n",
    "print(\"Done with header\")\n",
    "\n",
    "# Step 3: Create the Nifti1 image and save it to disk\n",
    "mni_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_mni.nii.gz')\n",
    "img = nib.Nifti1Image(large_array, first_vol.affine, first_vol.header)\n",
    "print(\"Done creating the image\")\n",
    "img.to_filename(mni_epi)\n",
    "print(\"Done writing it to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625691c-812f-4352-9a64-429b3f40393f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Finally, we can move on to removing all the temporary files we used, to end up with only one clean Nifti file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21f414-8b32-46c6-b216-d602c6616e41",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.system('rm -rf {}'.format(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_split*.nii.gz')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73531a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, let's check we did a proper job. If we did a proper mapping, we should definitely observe the EPI positioned on the anatomical in MNI space. How well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8abf15",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(produced_vols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9079c4-8ae7-4c34-b68c-1cfdba7909d7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.3 Where are we?\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Field unwarping</td><td style='text-align:justify;'>Correction distortions induced by inhomogeneities of the b0 field through maps acquired specifically to measure this field called fieldmaps.</td><td style='text-align:justify;'>FUGUE (but also FLIRT - see below)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Slice-timing correction</td><td style='text-align:justify;'>Accounting for the difference in acquisition between the slices that make up a volume to interpolate back voxels to a fixed time reference.</td><td style='text-align:justify;'>slicetimer</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec62f69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "NOTE:\n",
    "epi_reg is still using FLIRT under the hood! To be more specific, it is using flirt setup with its search cost as BBR. If you look through FLIRT's options, you'll notice that many more options are open to you:\n",
    "<img src=\"imgs/flirt_options.png\"/>\n",
    "\n",
    "Feel free to explore the effect of different search costs :) But remember: not all costs are born equal when registering images **across modalities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18139c8b-b600-4774-ab8f-c2741779e1fe",
   "metadata": {},
   "source": [
    "## 1.4 Slice time correction\n",
    "\n",
    "The importance of this step is still being investigated in fMRI literature. See <a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2019.00821/full\">here</a> for an in-depth analysis of its impact on the pipeline. One of the take-aways from this reference is that slice-time correction together with motion correction does improve results of fMRI analysis and does not hurt.\n",
    "Doing it before or after MC is not clear, as you can see in the reference above, so we're *choosing* here to showcase it after motion correction, but only time and further investigations will tell if there's a good order :)\n",
    "\n",
    "\n",
    "#### 1.4.1 A toy example\n",
    "\n",
    "To help you understand the underlying theory of slice-time correction, we will start from a rather unreasonable case on synthetic data, which will help you better visualize how slice-time correction affects the patterns.\n",
    "\n",
    "As you'll see, this step can only be performed if you have knowledge of the way in which slices were acquired. Most of the time, fortunately, this is easy to recover. If it is not present in your data, you can ask the scanner operator to find out which sequence was used for your data acquisition.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce97298-6217-43de-836a-d3f88b543fc3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run generate_smileys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb346d-3818-448a-bb04-5579adc83cec",
   "metadata": {},
   "source": [
    "First load the file named \"ground_truth_modulation.nii\" in FSLeyes to visualize it. The data is 4D, go ahead a play a bit around to see exactly what happens during the sequence! (don't forget in case of flickering to untick the \"Synchronize movie update\" option of FSLeyes).\n",
    "\n",
    "What you see is the ground truth, ie: the real phenomenon as it plays out!\n",
    "\n",
    "We now have an acquisition sequence. It performs in slices, but not in a linear order.\n",
    "Our sequence is even weirder: at a given time, not one but 9 slices are made at the same time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2a4d-b8be-4709-abb8-727c67adb32b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import InterpolatedUnivariateSpline as Interp\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_array_asnib(array, save_name):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a Nifti file.\n",
    "    The nifti is considered to have identity transformation matrix and to be unsigned int.\n",
    "    If you wish to use this function for float data, you should remove the .astype(np.uint8) ;)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        The array to save\n",
    "    save_name: str\n",
    "        Path to which the array will be saved\n",
    "    \"\"\"\n",
    "    img = nib.Nifti1Image(array.astype(np.uint8), np.eye(4))\n",
    "    nib.save(img, save_name)\n",
    "    \n",
    "def check_dims(axis_len, seq_len):\n",
    "    \"\"\"\n",
    "    Checks that axis length and seq length are equal. Otherwise, raise an Exception.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    axis_len: int\n",
    "        Length of the axis (number of elements along the axis of the array in your case)\n",
    "    seq_len: int\n",
    "        Length of the sequence\n",
    "    \"\"\"\n",
    "    if axis_len != seq_len:\n",
    "        raise Exception('The number of slices in the sequence is different from the number of slices available in the axis. Are you sure this is the right axis?')\n",
    "\n",
    "def reslice_with_timings(slice_dir, slice_sequence,input_data, original_times):\n",
    "    \"\"\"\n",
    "    Perform slice timing correction according to slice sequence timing and slice direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    slice_dir: char (can be 'x', 'y', or 'z')\n",
    "        Slice direction, the encoding direction of the sequence.\n",
    "    slice_sequence: np.ndarray\n",
    "        Matrix representing which slices are acquired in which order.\n",
    "    input_data: np.ndarray\n",
    "        The matrix that was acquired by the slice sequence and which we wish to correct\n",
    "    original_times: np.ndarray\n",
    "        The times at which the images are acquired and where we would like to reinterpolate back our slices.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    output_data: np.ndarray\n",
    "        The resliced data matrix, result of the slice-timing correction.\n",
    "    \"\"\"\n",
    "    assert(original_times.size == input_data.shape[3])\n",
    "    \n",
    "    n_acqs_per_tr = slice_seq.shape[0]\n",
    "    n_multibands = slice_seq.shape[1]\n",
    "    n_slices = slice_seq.size\n",
    "    \n",
    "    r= -1\n",
    "    if slice_dir=='x':\n",
    "        # For each slice in x, interpolate!\n",
    "        n = input_data.shape[0]\n",
    "        check_dims(n, n_slices)\n",
    "        r=0\n",
    "    elif slice_dir == 'y':\n",
    "        # For each slice in y, interpolate!\n",
    "        n = input_data.shape[1]\n",
    "        check_dims(n, n_slices)\n",
    "        r=1\n",
    "    elif slice_dir == 'z':\n",
    "        # For each slice in z, interpolate!\n",
    "        n = input_data.shape[2]\n",
    "        check_dims(n, n_slices)\n",
    "        r=2    \n",
    "    else:\n",
    "        # Undefined yo\n",
    "        raise Exception('Invalid dimension! Should be x, y or z')\n",
    "    # Reshape the input data to have r as first dimension!\n",
    "    input_data = np.swapaxes(input_data, 0, r)\n",
    "    \n",
    "    output_data = np.zeros(input_data.shape)\n",
    "    print(output_data.shape)\n",
    "\n",
    "    y_s = input_data.shape[1]\n",
    "    z_s = input_data.shape[2]\n",
    "    \n",
    "    # Now, on the first axis, we will iterate over slices :)\n",
    "    for b in range(0, n_acqs_per_tr):\n",
    "        time_slice = original_times + b*1./n_acqs_per_tr\n",
    "        # For all slices acquired together in the multiband\n",
    "        slices = slice_seq[b]\n",
    "        print('---------')\n",
    "        print(time_slice)\n",
    "        print(slices)\n",
    "        for s in range(0, n_multibands):\n",
    "            sl = slices[s]\n",
    "            for y in range(0, y_s):\n",
    "                for z in range(0, z_s):\n",
    "                    lin_interper = Interp(time_slice, input_data[sl, y, z, :], k=1)\n",
    "                    output_data[sl, y, z, :] = lin_interper(original_times)\n",
    "    # Now that this very inefficient method is done, we should remember to swap back the axis :)\n",
    "    input_data =np.swapaxes(input_data, r, 0)\n",
    "    output_data=np.swapaxes(output_data, r, 0)\n",
    "\n",
    "    output_data[output_data < 0] = 0\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d0b5d-66c4-4f7b-ab31-615e67a8d08f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_seq = np.arange(0, 99).reshape((11, - 1))\n",
    "slice_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75842c17-dd90-418f-91e9-23d07b021d04",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As you can see, these slices are acquired in a sequential order that can be called either ascending or descending depending on your convention!\n",
    "It means that the slices are obtained successively.\n",
    "\n",
    "There are different types of slicing, which depends on your sequence. Notice that in our example we acquire several slices simultaneously (for example, slices 0 up to 9 are all acquired together!). This could be some multiband acquisition, for instance, but really it is mostly to help you visualize the effect of slice timing for the exercise. In practice the slices are defined to sample the signal in the most appropriate way, so our toy sequence will likely be too crude :)\n",
    "\n",
    "In any case, we had some ground truth signal, that you can visualize in ground_truth_modulation.nii (don't forget to play the movie with the box unticked!)\n",
    "\n",
    "This signal represents the true signal that we want to acquire.\n",
    "The participant steps in an MRI, and the scanner operator uses the sequence we've defined above:\n",
    "\n",
    "```\n",
    "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "       [ 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
    "       [18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
    "       [27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    "       [36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
    "       [45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
    "       [54, 55, 56, 57, 58, 59, 60, 61, 62],\n",
    "       [63, 64, 65, 66, 67, 68, 69, 70, 71],\n",
    "       [72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
    "       [81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
    "       [90, 91, 92, 93, 94, 95, 96, 97, 98]])\n",
    "```\n",
    "\n",
    "What this really means is that we acquire 9 slices at the same time and then move on to the next slices to acquire, we acquire 9 of them and so on.\n",
    "\n",
    "Your task will be two-folds:\n",
    "1. Understand which axis the sequence was applied on, ie along which direction (X, Y or Z) was slicing performed\n",
    "2. With this simple knowledge, apply a slice-timing correction algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d929f-c505-4329-bd11-fe135eac193b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This cute smiley will represent our neurological data! Every time point, the behaviour is simple: the smiley's intensity increases across time!\n",
    "Here is what it looks like in FSLeyes (NOTE: to see the smileys you have to change volumes in the FSLeyes visualization!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fea7cd-aeec-47a2-ade4-561c0faa4c2a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('ground_truth_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea02c8d-cc35-4c9a-aa6f-af1bde00643a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('ground_truth_upsampled_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a4fd0-59ef-4fa7-a934-51d7212186eb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Using the following slice sequence, we have acquired our smiley signal.\n",
    "The resulting timeseries is represented in the acquired_modulation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ffe4c-33ee-40c0-9cc1-83c93a6aeac4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('acquired_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735339a-c692-4663-a56f-b2f4f361c0ae",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Oh no! What went wrong?\n",
    "\n",
    "Well, if you think about it, nothing. It is simply that acquiring every slice takes time. During this time, the signal is evolving, so we're a little late in our acquisition, which causes the drift you're seeing.\n",
    "\n",
    "This is what you observe in the acquired modulation file. The slice that is at the top - which is the last slice - also turns out to be the one with highest value. At timestep 0, it is in fact almost equal to 10 - the value of the ground truth at timestep 1 !\n",
    "\n",
    "### 1.4.2 Correcting the delay\n",
    "\n",
    "The heart of slice-timing correction is an interpolation in time.\n",
    "Because the timing of the slices is wrong, we account for it by interpolating back to some reference time. This is how we obtain the resliced data.\n",
    "For this, we need some informations. The first is the sequence in which slices are acquired, to know the lag of each slice. We also need the axis along which slices are acquired.\n",
    "\n",
    "Your task, in this simplified example, based **only on the abnormal smiley visualization in FSLeyes** and the knowledge of the sequence (that is: a bottom-up sequence with 9 simultaneous bands in every slice), is to determine the direction of the phase encoding, ie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ad944-0cc3-4ce4-897f-511141c95c1e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a35e79-89c5-4882-8f2a-ab5030301a66",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Based on your answer, in the following cell, fill in the phase_encode variable, with either 'x', 'y' or 'z' (with the quotation marks!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22149708-1e7a-4129-bdcc-398f3cce6a9a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phase_encode = 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d93ca-9de7-4b49-ba15-f08f3fcf21cc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will now conduct slice-timing correction: the idea is simply to interpolate back the slices in time along the slice direction. Easy right? Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9035e2-8f90-4ee7-91f6-c687eb3187f1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "smile_ts = nib.load('ground_truth_modulation.nii').get_fdata()\n",
    "smile_resampled = nib.load('acquired_modulation.nii').get_fdata()\n",
    "resliced_data = reslice_with_timings(phase_encode, slice_seq, smile_resampled, np.arange(0,9))\n",
    "save_array_asnib(resliced_data.astype(np.uint8), 'resliced_data.nii')\n",
    "\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('resliced_data.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41757311-9e52-4659-ab2e-67c81acb06e7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Are you convinced on this toy example we did a not-too bad job?\n",
    "\n",
    "### 1.4.3  Application to real data\n",
    "\n",
    "We have shown you the basic principle, but the application to real data requires some specific informations.\n",
    "You need the following ingredients:\n",
    "- When was each slice acquired in the sequence: **(Slice timing)**\n",
    "- Along which axis were the slices acquired: **Phase direction**\n",
    "- How much time we take to acquire all slices: **TR**\n",
    "\n",
    "Let's go back to our practical dataset to extract these informations. Can you find them, when looking through the JSON sidecar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a4c56-4d26-4213-a197-2edefc620f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_json_from_file(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188188-bc2c-478d-b747-263a105d04e9",
   "metadata": {},
   "source": [
    "This data is actually a dictionary. We can thus extract the slice timing as an array directly from it. For example, to extract TaskName, we would use:\n",
    "```python\n",
    "data['TaskName']\n",
    "```\n",
    "\n",
    "Go ahead and extract the slice timing array, and store it in the slice_timing variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fbdbb-4565-441e-ac7e-56d21b83c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_timing = data['SliceTiming'] # Replace with the appropriate key (have a look above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713d62a-fdf3-4547-9cb8-fc51cc91c5db",
   "metadata": {},
   "source": [
    "Now, we might want to know where our slices are, ie along which axis, right? Typically it is along the z-direction, but we're better off if we check! Using FSLeyes, determine how many slices each axis has **for the functional data of interest**. You should thus open the relevant functional file in FSLeyes to answer this question.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>Using FSL command line</b></p>\n",
    "<p style='text-indent: 10px;'>To figure out the dimensions of an MRI image, a faster option - if you have FSL installed directly - is to run the command line command:\n",
    "    <blockquote>fslhd [your_volume]</blockquote>\n",
    "This will give you all informations contained within the header of the NIfti file. For example, running the command for our volume will easily allow us to access the slice informations:\n",
    "    <img src=\"imgs/fslhd_capture.png\"></p>\n",
    "</span>\n",
    "</div>\n",
    "Let's compare now with the amount of slices we have in our acquisition. We can consider simply the number of timings for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa06533-f91b-45c6-be31-e60160f984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slice_timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659a402",
   "metadata": {},
   "source": [
    "So we have 52 slices in our slice timings, and you likely found 77 slices on the X axis, 77 axis on the Y axis and 51 slices on the Z axis. As a consequence, Z is the axis where the slices were acquired!\n",
    "Great, so we know which axis we want, we know the slice timings, but we still need to know the TR. This information is also in the JSON sidecar! Extract it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14624586",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data['RepetitionTime'] # Extract the TR from the sidecar's appropriate field\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c1429",
   "metadata": {},
   "source": [
    "To now perform the correction, we need to apply FSL's slicetimer command. For this, we need to save the timings first to their own separate file! Instead of giving the slice timings, we will provide instead the slice **order** (ie which slice was done in which order) and let FSL figure out how to best correct based on this information.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59910eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_order = np.argsort(slice_timing) + 1\n",
    "\n",
    "# Write to a file the corresponding sorted timings :)\n",
    "timing_path = op.join(preproc_root,  'sub-001', 'func', 'sub-001_task-sitrep_run-01_slice-timings.txt')\n",
    "file = open(timing_path, mode='w')\n",
    "for t in slice_order:\n",
    "    file.write(str(t) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a80368",
   "metadata": {},
   "source": [
    "Finally we can call slicetimer from a terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d929366",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_to_realign = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "output_target = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_slice-corr')\n",
    "\n",
    "subprocess.run(['slicetimer', '-i', file_to_realign, '-o', output_target, '-r', str(tr), '-d', str(3), '--ocustom={}'.format(timing_path)])\n",
    "#cmd = 'slicetimer -i ' + file_to_realign + ' -o ' + output_target + ' -r ' + str(tr) + ' -d 3 --ocustom=' + timing_path\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b381dbc-4673-4ee9-8b0a-fb4e8df51028",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(file_to_realign)\n",
    "fsleyesDisplay.load(output_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c93a3c-4d12-4a9c-ad9c-dce9af067586",
   "metadata": {},
   "source": [
    "As you would notice, visual differences due to slice-timing correction are not too obvious in general.\n",
    "\n",
    "There are unfortunately cases where it can go very wrong, due to the interpolation nature of the approach.\n",
    "Consider the four images below.\n",
    "\n",
    "<div class=\"fit\">\n",
    "<img src=\"imgs/slice_uncorr.png\" max-width=\"1400px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188288d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Had we launched it on the unscrubbed data, we would really notice the impact on the first volume. <span style=\"color:red;\">Notice that you should in general **never** do this, as you would introduce lots of noise and garbage in your data.</span> Prefer to first clean all that is weird and then perform steps that might not bring a visible improvement rather than starting with data that is so bad that you can visually see changes when running above steps.\n",
    " <div class=\"row\">\n",
    "    <img src=\"imgs/slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>First volume without slice correction</center>\n",
    "    <img src=\"imgs/slice_corr.png\" style=\"width:100%\">\n",
    "       <center>First volume with slice correction: the staircase has been more or less mitigated but the result is still imperfect...</center>\n",
    "    <center>**And because of the linear interpolation, the garbage of volume 0 was spilled to volume 1!!!**</center>\n",
    "    <img src=\"imgs/vol1_slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>Second volume without slice correction</center>\n",
    "    <img src=\"imgs/vol1_slice_corr.png\" style=\"width:100%\">\n",
    "       <center>Second volume with slice correction: the result is worse than before...</center>\n",
    "</div> \n",
    "\n",
    "\n",
    "The message here is: \n",
    "- **always** perform QC between your steps\n",
    "- no algorithm can turn trash to gold. Remove the faulty volumes or you'll likely have a garbage-in garbage-out scenario. \n",
    "\n",
    "<u>Remember the 1 - 10 -100 dollar rule! It is much easier to avoid errors than compensate for them.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de77c5-c9ff-48b7-9438-27974562068c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# MRI and fMRI preprocessing: conclusion\n",
    "\n",
    "You have reached the end the preprocessing for MRI and fMRI. \n",
    "\n",
    "As you can see, there are many fairly involved steps that need to be conducted.\n",
    "\n",
    "To normalize choices and results, software solutions exist, such as the excellent <a href=\"https://fmriprep.org/en/stable/\">fmriprep</a>, to conduct automatically all steps for you while providing you with quality checks to verify that all went well, as no algorithm can replace your expert eye to inspect potential artefacts and remove them.\n",
    "\n",
    "These tools are nonetheless precious to help different groups follow same systematic choices of preprocessing, both in parameters and order of application for each method, leading to more reproducible science in the long run. (But they require a big RAM and take hours to run, which is why we've avoided them for the purpose of this tutorial :) )\n",
    "\n",
    "Let's wrap up what you have learnt.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM in one week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Fieldmap preparation</td><td style='text-align:justify;'>Field maps can be used to create a distortion field to correct...Distortions. </td><td style='text-align:justify;'>topup, FUGUE</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration</td><td style='text-align:justify;'>Realignment of fMRI volumes to anatomical space - the subject's own MRI is typically used. We can include susceptibility-distortion correction with fieldmaps. We compute this transformation only for the volume we used as reference in MCFLIRT. Then, we apply it to all other volumes in the EPI.</td><td style='text-align:justify;'>epi_reg (FLIRT with boundary-based registration)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Normalization</td><td style='text-align:justify;'>Putting the EPI in a template space, such as MNI. This is done by applying the transformation of anatomical space to MNI space, which was computed in the anatomical preprocessing. Note that we typically like to combine this transform with coregistration to do everything in one go.</td><td style='text-align:justify;'>applywarp to apply combined warps, otherwise (if going transform by transform), FLIRT with applyxfm option</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If you have any more questions, both on these tools and on preprocessing, do not hesitate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114371c-8565-4b97-a88e-1a6d36daec47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style=\"color:red;font-size:20px;\">To release all fsleyes resources and to have interactivity below, you will now need to STOP the kernel. Start running the cells from Part 2 onwards after that.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac02d6a-d896-48e0-a36c-e280f83b37df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p><b>üéâ You've reached the end of this week's notebook! Congratulations! üéâ </b></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
